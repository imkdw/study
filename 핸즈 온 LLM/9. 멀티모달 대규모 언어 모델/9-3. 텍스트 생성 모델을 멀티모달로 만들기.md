# 텍스트 생성 모델을 멀티모달로 만들기
- 전통적으로 텍스트 생성 모델은 텍스트 표현을 해석하는 모델임
- 훈련된 모달리티인 텍스트에 대해서는 추론이 뛰어나지만 이는 특정 학습에 의해 국한되어있음
- 텍스트 생성 모델의 경우 특정 입력 이미지에 대해 추론이 가능함
- 두 도메인 사이의 간극을 메꾸기 위해서 BLIP-2라는게 있는데, 이는 기존 언어 모델에 비전 능력을 손쉽게 추가할 수 있는 쉬운 모듈식 기법을 사용함

<br>

# BLIP-2 : 모달리티 간극 매꾸기
- 밑바닥부터 멀티모달 언어를 만드는건 매우 어려우므로 이를 위해서 Q-포머를 통해 비전과 언어 사이의 간극을 매꿈
- Q-포머는 사전 훈련된 이미지 인코더와 사전 훈련된 LLM 사이에 다리를 연결함

<br>

### Q-포머의 구성
- 어텐션 층을 공유하는 두 개의 모듈로 구성됨
  - 특성 추출을 위해 동결된 비전 트랜스포머와 상호작용하는 이미지 트랜스포머
  - LLM과 상호작용이 가능한 텍스트 트랜스포머

<br>

### Q-포머의 훈련과정
- Q-포머는 모달리티마다 하나씩 2개의 단계로 훈련됨
- 동결된 ViT에 이미지를 주입해서 비전 임베딩을 얻고, 이 임베딩을 Q-포머의 비전 트랜스포머를 위한 입력으로 사용함
- 이후 캡션은 Q-포머의 텍스트 트랜스포머에 입력됨
- 1단계에선 표현 학습이 적용되어서 비전과 언어 표현을 동시에 학습하고, 2단계에선 이 표현이 소프트 시각 플홈프트로 변환되어서 LLM에 주입됨
- 이미지-텍스트 대조 학습, 이미지-텍스트 매칭, 이미지 기반 텍스트 작업에서 훈련됨
- 최종적으로 비전과 언어(표현학습), 비전 투 언어(생성적 학습) 단계를 모두 거치면 LLM에게 문맥을 제공하는 것과 비슷한 방식으로 이미지에 대한 정보 제공이 가능함

<br>

# 멀티모달 입력 전처리
- BLIP-2 모델을 사용하는 사례는 이미지 캡셔닝, 시각 자료가 포함된 질문에 대한 답변 등 다양한 사례가 많이 존재함

<br>

### 모델과 전처리기 가져오기
- 전처리기는 임지와 텍스트 같은 비정형 입력을 모델이 일반적으로 기대하는 표현으로 변환하는 역할을 함
```python
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch

# 전처리기 및 메인 모델 로딩
blip_processor = AutoProcessor.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24"
)

model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24",
    torch_dtype=torch.float16
)

# 속도를 높이기 위해서 모델을 GPU로 전송함, 없다면 CPU로 사용함
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
```

<br>

### 이미지 전처리
```python
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch
from PIL import Image
from urllib.request import urlopen

# 전처리기 및 메인 모델 로딩
blip_processor = AutoProcessor.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24"
)

model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24",
    torch_dtype=torch.float16
)

# 속도를 높이기 위해서 모델을 GPU로 전송함, 없다면 CPU로 사용함
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# 예제인 슈퍼카 이미지 로딩
car_path = "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png"
image = Image.open(urlopen(car_path)).convert("RGB")

# 이미지 전처리 진행
inputs = blip_processor(image, return_tensors="pt").to(device, torch.float16)

# torch.Size([1, 3, 224, 224]), 전처리기에 맞는 사이즈로 이미지를 변환함
print(inputs["pixel_values"].shape)
```

<br>

### 이미지 전처리
- 중간에 보면 `Ġ` 같은 이상한 토큰이 들어가는데 이는 사실 공백임
- 프린트를 위해서 특정 코드 포인트에 문자 + 256 만큼 더한 문자를 사용함
```python
# 텍스트 전처리 진행
text = "Her vocalization was remarkably melodic"
token_ids = blip_processor(image, text=text, return_tensors="pt")
token_ids = token_ids.to(device, torch.float16)["input_ids"][0]

# 토크나이저로 변환된 토큰들의 아이디 목록
# tensor([    2, 13584,  7578,  1938,    21, 20635, 15352, 30054])
print(token_ids)

# 토큰 아이디를 다시 토큰으로 되돌리기
tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids)

# ['</s>', 'Her', 'Ġvocal', 'ization', 'Ġwas', 'Ġremarkably', 'Ġmel', 'odic']
print(tokens)
```

<br>

# 사용 사례 1 : 이미지 캡셔닝
- BLIP-2 같은 모델을 사용한 가장 간단한 예제는 이미지 캡션을 만드는것임
- 이는 전처리 과정과 비슷한데 이미지를 모델이 해석할 수 있게 픽셀 값으로 변환부터 시작함
- 이후에 픽셀값을 모델에 전달해서 소프트 시각 프롬프트로 변환하고, LLM은 이를 사용해서 적절한 캡션을 생성함

<br>

### 캡셔닝 예제
- 모델은 대량의 공개 데이터에서 훈련되었기 때문에 특정 만화 캐릭터나 가상의 창조물과 같은 특정 도메인의 이미지는 캡션을 잘 만들지 못할수도 있음
- 
```python
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch
from PIL import Image
from urllib.request import urlopen

# 전처리기 및 메인 모델 로딩
blip_processor = AutoProcessor.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24"
)

model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24",
    torch_dtype=torch.float16
)

# 속도를 높이기 위해서 모델을 GPU로 전송함, 없다면 CPU로 사용함
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# 슈퍼카 이미지
car_path = "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png"
image = Image.open(urlopen(car_path)).convert("RGB")

# 이미지를 전처리해서 입력을 준비
inputs = blip_processor(image, return_tensors="pt").to(device, torch.float16)

# 이미지 임베딩을 생성하고 Q-포머의 출력을 디코더(LLM)에 전달해서 토큰 아이디를 만듬
generated_ids = model.generate(**inputs, max_new_tokens=20)

# 생성된 토큰 ID를 바탕으로 텍스트를 생성함
generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)
generated_text = generated_text[0].strip()

# an orange supercar driving on the road at sunset
print(generated_text)
```

<br>

### 로르샤흐 검사 이미지로 캡션 생성하기
- 잉크 얼룩에 대한 개인 인식을 테스트하는 오래된 심리학 실험임
- 잉크 무늬를 무엇으로 보는지가 그 사람의 성격 특정에 대해 무언가를 말해주는데 상당히 주관적인 테스트라 더 재밌음
```python
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch
from PIL import Image
from urllib.request import urlopen

# 전처리기 및 메인 모델 로딩
blip_processor = AutoProcessor.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24"
)

model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24",
    torch_dtype=torch.float16
)

# 속도를 높이기 위해서 모델을 GPU로 전송함, 없다면 CPU로 사용함
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

url = "https://upload.wikimedia.org/wikipedia/commons/7/70/Rorschach_blot_01.jpg"
image = Image.open(urlopen(url)).convert("RGB")
inputs = blip_processor(image, return_tensors="pt").to(device, torch.float16)
generated_ids = model.generate(**inputs, max_new_tokens=20)
generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)
generated_text = generated_text[0].strip()

# a black and white ink drawing of a bat
# 검은색과 하얀색 잉크로 그려진 박쥐로 해석
print(generated_text)
```
 
<br>

# 사용 사례 2 : 채팅 기반 멀티모달 프롬프트
- 이전의 예제처럼 선형적인 흐름 대신에 동시에 두 모달리티를 제시해서 `시각 질문 답변` 작업 수행이 가능함
- 모델에게 이미지와 이미지에 대한 질문을 전달하는데, 모델은 이미지와 질문을 동시에 전처리함

<br>

### 프롬프트를 통한 캡션 생성하기
```python
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch
from PIL import Image
from urllib.request import urlopen

# 전처리기 및 메인 모델 로딩
blip_processor = AutoProcessor.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24"
)

model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24",
    torch_dtype=torch.float16
)

# 속도를 높이기 위해서 모델을 GPU로 전송함, 없다면 CPU로 사용함
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# 슈퍼카 이미지
car_path = "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png"
image = Image.open(urlopen(car_path)).convert("RGB")

# 시각 질문 답변, 아래에서는 우선 이미지에 대한 설명을 적으라고 지시함
# 만약 질문이 없다면 모델은 이전처럼 동일하게 이미지에 대한 캡션을 생성함
prompt = "Question: Write down what you see in this picture. Answer:"

# 이미지 및 프롬프트에 대해서 동시에 전처리를 진행
inputs = blip_processor(image, text=prompt, return_tensors="pt").to(device, torch.float16)

# 텍스트를 생성함
generated_ids = model.generate(**inputs, max_new_tokens=30)
generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)
generated_text = generated_text[0].strip()

# A sports car driving on the road at sunset
print(generated_text)
```

<br>

### 채팅 스타일로 대화하기
```python
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch
from PIL import Image
from urllib.request import urlopen

# 전처리기 및 메인 모델 로딩
blip_processor = AutoProcessor.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24"
)

model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    revision="51572668da0eb669e01a189dc22abe6088589a24",
    torch_dtype=torch.float16
)

# 속도를 높이기 위해서 모델을 GPU로 전송함, 없다면 CPU로 사용함
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# 슈퍼카 이미지
car_path = "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png"
image = Image.open(urlopen(car_path)).convert("RGB")

# 시각 질문 답변, 프롬프트 형식으로 사진에 대한 질문 -> 해당 질문을 프롬프트화 -> 다시 차의 가격이 얼마일꺼같은지 질문
# 만약 질문이 없다면 모델은 이전처럼 동일하게 이미지에 대한 캡션을 생성함
prompt = "Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:"

# 이미지 및 프롬프트에 대해서 동시에 전처리를 진행
inputs = blip_processor(image, text=prompt, return_tensors="pt").to(device, torch.float16)

inputs = blip_processor(image, text=prompt, return_tensors="pt").to(device, torch.float16)
generated_ids = model.generate(**inputs, max_new_tokens=30)
generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)
generated_text = generated_text[0].strip()

# $1,000,000
print(generated_text)
```