# 멀티모달 임베딩 모델
- 임베딩은 대규모 정보를 포착하고 정보의 모래사장에서 바늘을 찾는 효율적인 방법임
- 멀티모달 임베딩 모델을 사용하면 입력 텍스트를 기반으로 이미지를 찾을 수 있음
- 다양한 임베딩 모델이 많지만 가장 많이 사용한건 `CLIP` 모델임

<br>

# CLIP : 텍스트와 이미지 연결
- CLIP은 이미지와 텍스트의 임베딩을 모두 계산할 수 있는 모델임
- 만들어진 임베딩은 동일한 벡터 공간에 놓이는데, 이는 이미지 임베딩을 텍스트 임베딩과 비교할 수 있다는 의미임
- 이런 특성때문에 제로샷 분류, 클러스터링, 검색, 생성 작업에 매우 유용함

<br>

# CLIP이 멀티모달 임베딩을 생성하는 방법
- CLIP 모델은 이미지와 캡션이 있는 데이터셋에서 이미지와 캡션에 대해 두 개의 표현을 만들 수 있음
- 텍스트 인코더를 통해서 텍스트를 임베딩하고, 이미지 인코더를 사용해서 이미지를 임베딩함
- 처음 훈련을 시작할때는 이미지 및 텍스트 임베딩 사이의 유사도는 매우 낮은데 훈련하는 동안 두 임베딩 사이의 유사도를 최적화 하게됨
- 또한 가능한 한 정확한 표현을 만들기 위해서 관련 없는 이미지와 캡션을 사용하는 `네거티브 샘플`이 훈련 과정에 포함되어야함

<br>

# OpenCLIP
```python
from urllib.request import urlopen
from PIL import Image
from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel
import torch
import numpy as np
import matplotlib.pyplot as plt

puppy_path = "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png"
image = Image.open(urlopen(puppy_path)).convert("RGB")

caption = "a puppy playing in the snow"

model_id = "openai/clip-vit-base-patch32"

# 텍스트 전처리를 위한 토크나이저
clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)

# 이미지 전처리를 위한 전처리기
clip_processor = CLIPProcessor.from_pretrained(model_id)

# 텍스트 및 이미지 임베딩 생성을 위한 모델
model = CLIPModel.from_pretrained(model_id)

# 캡션을 토큰화함
inputs = clip_tokenizer(caption, return_tensors="pt")

# 텍스트 임베딩 생성
text_embedding = model.get_text_features(**inputs)

# 이미지 전처리 진행
processed_image = clip_processor(
    text=None, images=image, return_tensors='pt'
)['pixel_values']

# 이미지 임베딩 생성
image_embedding = model.get_image_features(processed_image)

# 임베딩 정규화
text_embedding /= text_embedding.norm(dim=-1, keepdim=True)
image_embedding /= image_embedding.norm(dim=-1, keepdim=True)

# 유사도 계산
text_embedding = text_embedding.detach().cpu().numpy()
image_embedding = image_embedding.detach().cpu().numpy()
score = text_embedding @ image_embedding.T

# [[0.33149624]]
# 0.33이라는 유사도가 도출됨
print(score)
```
