# 텍스트 생성 모델 사용하기
- `<|user|>`: 사용자 입력
- `<|endoftext|>`: 토큰 끝
- `<|end|>`: 메시지 끝

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cpu",
    torch_dtype="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=500,
    do_sample=False
)

messages = [
    {
        "role": "user",
        "content": "Create a funny joke about chickens"
    }
]

prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)

"""
<|user|>
Create a funny joke about chickens<|end|>
<|endoftext|>
"""
print(prompt)
```

<br>

# 모델 출력 제어하기
### temperature
- 텍스트 생성의 무작위성 또는 창의성을 조절하는 매개변수
- 0이면 항상 확률이 가장 높은 단어가 선택되서 언제나 동일한 응답이 생성됨
- 결과적으로 높으면 다양한 출력을 만들고, 낮으면 좀 더 결정론적인 출력을 만듬
- 아래 예시처럼 1로 설정하게되면 실행할 때마다 출력이 바뀌는데 이는 모델이 랜덤하게 토큰을 선택하게 되어서 확률적으로 동작하게됨
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cpu",
    torch_dtype="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=500,
    do_sample=False
)

messages = [
    {
        "role": "user",
        "content": "Create a funny joke about chickens"
    }
]

"""
  Why do chickens make terrible internet users? Because they always peck at the 'poultry' problem!
  Why don't chickens use computers? Because they're afraid of the crows pecking at their eggs while they're typing!
  "Why don't chickens believe in social media? Because they already follow the flock model!"
  Why don't chickens use computers? Because they're afraid of "poultry" web!
  Why did the chicken join the orchestra? Because it had perfect timing!
"""
for i in range(1, 5):
    output = pipe(messages, do_sample=True, temperature=1)
    print(output[0]["generated_text"])
```

<br>

### top_p
- 뉴클리어스 샘플링이라고도 부르는 top_p는 LLM이 고려할 토큰 일부를 제어하는 샘플링 기법
- 0.1로 지정하면 누적 확률이 이 값에 도달할 때까지 확률 크기 순으로 토큰을 선택함
- 1.0으로 지정하면 모든 토큰을 고려하게됨
- 비슷하게 top_k도 있는데 이는 LLM이 고려할 수 있는 토큰의 수를 제어함
- 100으로 바꾸면 LLM은 확률이 가장 높은 토큰의 상위 100개만 고려하게됨
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cpu",
    torch_dtype="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=500,
    do_sample=False
)

messages = [
    {
        "role": "user",
        "content": "Create a funny joke about chickens"
    }
]

"""
  Why did the chicken join the band?
  Because it had drumsticks!
  Did you hear about the chicken who wrote a cookbook? It wasn't very appetizing because it couldn't stop clucking about the coop...
  Why don’t chickens use computers? Because they’re afraid of too many screen-eggs!
  Why do chickens like to play hide and seek? Because when they think nobody's looking, they're on egg-sue!
"""
for i in range(1, 5):
    output = pipe(messages, do_sample=True, top_p=1)
    print(output[0]["generated_text"])
```


<br>

### temperature 및 top_p 값 선택 예시
#### 브레인스토밍 세션
- temperature: 높음
- top_p : 높음
- 무작위성이 높고 잠재적인 토큰풀이 많은데 이러면 매우 다양한 결과를 만들며 창의적이로 기대하지 못한 출력을 얻을 수 있음

<br>

#### 이메일 작성
- temperature: 낮음
- top_p: 낮음
- 확률이 가장 높은 토큰이 선택되ㅑ는 결정론적인 출력, 결과를 예상할 수 있고 보수적인 출력을 얻음

<br>

#### 창의적인 글쓰기
- temperature: 높음
- top_p: 낮음
- 무작위성이 높고 잠재적인 토큰 풀이 적음, 일관성을 유지하면서 창의적인 출력을 얻을 수 있음

<br>

#### 번역
- temperature: 낮음
- top_p: 높음
- 확률이 가장 높은 토큰이 선택되는 결정론적인 출력. 다양한 어휘를 사용한 일관성있는 결과를 생성하므로언어 다양성이 있는 출력을 만듦
