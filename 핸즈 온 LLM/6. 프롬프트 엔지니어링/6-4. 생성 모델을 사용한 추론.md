# 생성 모델을 사용한 추론
- 프롬프트 엔지니어링을 통해서 LLM과 협업해서 추론 과정을 모방하고 LLM의 출력을 개선할 수 있음
- 자기 성찰적인 동작없이 자동적이고 직관적인 과정도 있고 브레인스토핑이나 자기 성찰과 비슷한 의식적이고, 느리며, 논리적인 과정도 있음
- 만약 생성 모델에게 자기 성찰의 형태를 흉내낼 수 있는 능력을 부여한다면 두번째 방법을 모방하는 셈이됨

<br>

# CoT(Chain of Thought) : 답변하기 전에 생각하기
- 생성 모델이 추론 과정 없이 바로 질문에 대답하지 않고 그 전에 생각하게 만드는 것이 목표임
- 이런 추론 과정을 `사고`라고 부르며 수학 문제와 같이 복잡도가 매우 높은 작업에 매우 이로움

<br>

### CoT 예시

- 예시를 보면 정답뿐만 아니라 답을 내기 전에 설명을 먼저 제시하고 있음. 이렇게 해서 모델이 생성한 지식을 활용해서 최종 답변 계산이 가능함
- CoT는 생성 모델의 출력을 향상할 수 있는 훌룡한 방법이지만 프롬프트에 한 개 이상의 추론 예시를 포함해야함
- 하지만 이런 예시는 유저가 만들지 못할수도 있는데 대신 `제로샷 CoT`도 가능함
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cpu",
    torch_dtype="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=500,
    do_sample=False
)

cot_prompt = [
    {"role": "system", "content": "한글로 대답해주세요"},
    {"role": "user",
     "content": "로저는 테니스공 5개를 가지고 있습니다. 그는 테니스공 2캔을 더 삽니다. 각 캔에는 테니스공이 3개씩 들어 있습니다. 그는 지금 테니스공을 몇 개 가지고 있나요?"},
    {"role": "assistant", "content": "로저는 처음에 공 5개를 가지고 있었습니다. 테니스공이 3개씩 든 캔 2개는 공 6개입니다. 5 + 6 = 11. 정답은 11입니다."},
    {"role": "user", "content": "식당에는 사과가 23개 있었습니다. 점심을 만드는 데 20개를 사용하고 6개를 더 샀다면, 지금 사과가 몇 개 있을까요?"}
]

outputs = pipe(cot_prompt)

"""
식당에서 점심을 만들기 전 사과가 23개였습니다.
점심을 만들 때 사용했던 사과는 20개였습니다.
사과가 사용되었으므로 남은 사과는 23 - 20 = 3개입니다.
그런데 식당에서 6개를 더 샀습니다.
따라서 지금 사과는 3 + 6 = 9개입니다. 정답은 9개입니다.
"""
print(outputs[0]["generated_text"])
```

<br>

### 제로샷 CoT
- CoT 방식에서 유저가 프롬프트에 예시를 제공할 수 없다면 사용 가능한 방법임
- 예시를 제공하는 대신 생성 모델에 추론을 수행하도록 요구함
- 다양한 방식이 있지만 일반적으로 효과있는 방식은 `Let's think step-by-step`임

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cpu",
    torch_dtype="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=500,
    do_sample=False
)

zeroshot_cot_prompt = [
    {"role": "user",
     "content": "식당에는 사과가 23개 있었습니다. 점심을 만드는 데 20개를 사용하고 6개를 더 샀다면, 지금 사과가 몇 개 있을까요? Let's think step by step!."}
]

# 점심을 만들었을 때 사과는 20개 사용했습니다. 그런 다음 6개를 더 샀기 때문에 생겼습니다. 따라서 지금 사과는 20 + 6 = 26개 입니다. 따라서 지금 사과가 26개 있습니다.
outputs = pipe(zeroshot_cot_prompt)
print(outputs[0]["generated_text"])
```

<br>

# 자기 일관성 : 출력 샘플링
- temperature, top_p 같은 매개변수로 창의성을 허용하는 경우 동일한 프롬프트여도 다른 결과를 얻을 수 있음
- 결국 운에 따라야 했는데 무작위성에 대응하고 생성 모델의 품질을 향상시키기 위해서 `자기 일관성 방법`이 개발됨
- 생성 모델에게 동일한 프롬프트를 여러 번 요청하고 다수를 차지하는 결과를 최종 답변으로 내놓음
- 성능을 향상시키긴 하지만 하나의 질문에 대해서 여러번 요청해야 하므로 출력 샘플 개수가 n개면 n배만큼 느려짐

<br>

# ToT(Tree of Thoughts) : 중간 단계 탐색
- ToT는 여러가지 아이디어를 깊게 탐색할 수 있는 방법임
- 여러 단계의 추론이 필요한 문제를 만났을 때 이 문제를 여러 단계로 나누느 것이 종종 유용함
- 생성 모델을 여러 번 호출하는 대신 모델이 이런 동작을 모방하도록 해서 전문가 여럿이 주고 받는 대화를 흉내내도록 요청함

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cpu",
    torch_dtype="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=500,
    do_sample=False
)

zeroshot_tot_prompt = [
    {"role": "user",
     "content": "Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realises they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results."}
]
outputs = pipe(zeroshot_tot_prompt)

"""
 Expert 1:
Step 1: Start with the initial number of apples, which is 23.

Expert 2:
Step 1: Subtract the number of apples used for lunch, which is 20.
Step 2: Add the number of apples bought, which is 6.

Expert 3:
Step 1: Start with the initial number of apples, which is 23.
Step 2: Subtract the number of apples used for lunch, which is 20.
Step 3: Add the number of apples bought, which is 6.

Results:
All three experts arrived at the same answer:

Expert 1: 23 - 20 + 6 = 9 apples
Expert 2: (23 - 20) + 6 = 9 apples
Expert 3: (23 - 20) + 6 = 9 apples

All three experts agree that the cafeteria has 9 apples left.
"""
print(outputs[0]["generated_text"])
```