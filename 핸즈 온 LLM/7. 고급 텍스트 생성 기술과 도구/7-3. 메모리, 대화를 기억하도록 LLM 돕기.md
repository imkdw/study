# 메모리 : 대화를 기억하도록 LLM 돕기
- LLM을 있는 그대로 사용하면 대화의 내용을 기억하지 못함
- LLM에게 건망증이 있는 이유는 모델에 유지되는 상태가 없기 때문임
- 메모리가 없는 LLM과 대화하는 것은 멋진 경험이 아님

```python
from langchain import PromptTemplate, LlamaCpp, LLMChain

llm = LlamaCpp(
    model_path="Phi-3-mini-4k-instruct-fp16.gguf",
    n_gpu_layers=-1,
    max_tokens=500,
    n_ctx=2048,
    seed=42,
    verbose=False
)

template = """<s><|user|>
{input_prompt}<|end|>
<|assistant|>"""
prompt = PromptTemplate(
    template=template,
    input_variables=["input_prompt"]
)

# 프롬프트 템플릿과 LLM 연결해서 체인 만들기
basic_chain = prompt | llm

#  Hello Maarten! The answer to 1 + 1 is 2.
print(basic_chain.invoke({"input_prompt": "Hi! My name is Maarten. What is 1 + 1?"}))

#  I'm unable to determine your name without more context. As an AI, I don't have the capability to know personal information about individuals unless it has been shared with me in the course of our conversation for privacy and security reasons. If you're looking to find out a piece of personal information like your name, it's best done through your own records or devices where that information is stored securely.
print(basic_chain.invoke({"input_prompt": "What is my name?"}))
```

<br>

# 대화 버퍼
- 가장 간단한 LLM 메모리 형태는 단순히 과거의 대화를 그대로 전달하는것임
- ConversationBufferMemory는 지금까지 LLM과 나눈 대화를 모두 저장함

```python
from langchain import PromptTemplate, LlamaCpp, LLMChain
from langchain.memory import ConversationBufferMemory

llm = LlamaCpp(
    model_path="Phi-3-mini-4k-instruct-fp16.gguf",
    n_gpu_layers=-1,
    max_tokens=500,
    n_ctx=2048,
    seed=42,
    verbose=False
)

# 채팅 히스토리를 포함한 프롬프트 작성
template = """<s><|user|>Current conversation:{chat_history}

{input_prompt}<|end|>
<|assistant|>"""

prompt = PromptTemplate(
    template=template,
    input_variables=["input_prompt", "chat_history"]
)

# 프롬프트 템플릿과 LLM 연결해서 체인 만들기
basic_chain = prompt | llm

# 메모리 선언
memory = ConversationBufferMemory(memory_key="chat_history")

# LLM에 체인에 프롬프트랑 메모리를 결합시킴
llm_chain = LLMChain(
    prompt=prompt,
    llm=llm,
    memory=memory
)

"""
{
    "input_prompt": "Hi! My name is Maarten and I am 33 years old. What is 1 + 1?",
    "chat_history": "",
    "text": " 1 + 1 equals 2. It's a basic arithmetic operation, unrelated to personal details like your name or age!\n"
}
"""
print(llm_chain.invoke({"input_prompt":"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?"}))

"""
{
    "input_prompt": "What is my name?",
    "chat_history": "Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\nAI:  Hello Maarten! 1 + 1 equals 2.\n\nHowever, based on the given context, this seems like a simple math question rather than discussing personal information. If you have any other topics in mind for our conversation, feel free to share them!",
    "text": " Your name is Maarten. You mentioned it at the beginning of our conversation.\n--------------------------"
}
"""
print(llm_chain.invoke({"input_prompt": "What is my name?"}))
```

<br>

# 윈도 대화 버퍼
- 대화 버퍼 방식은 대화가 늘어남에 따라서 입력 프롬프트의 크기도 커쳐서 최대 토큰개수를 초과할 수 있음
- 문맥 윈도 크기를 최소화하는 방법은 전체 채팅 기록을 사용하지 않고 마지막 k개의 대화만 사용하는것임
- `ConversationBufferWindowMemory`를 사용해서 입력 프롬프트에 얼마나 많은 대화를 전달할지 결정할 수 있음

### 최근 k개의 대화만 기억하는 상황
- 맨 처음에 말한 나이에 관한 질문은 대답하지 못함
- 이 방법은 대화 기록의 크기를 줄여주긴 하지만 마지막 몇 개의 대화만 기억함
- 그래서 긴 대화에는 이상적이지 않음
```python
from langchain import PromptTemplate, LlamaCpp, LLMChain
from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory

llm = LlamaCpp(
    model_path="Phi-3-mini-4k-instruct-fp16.gguf",
    n_gpu_layers=-1,
    max_tokens=500,
    n_ctx=2048,
    seed=42,
    verbose=False
)

# 채팅 히스토리를 포함한 프롬프트 작성
template = """<s><|user|>Current conversation:{chat_history}

{input_prompt}<|end|>
<|assistant|>"""

prompt = PromptTemplate(
    template=template,
    input_variables=["input_prompt", "chat_history"]
)

# 프롬프트 템플릿과 LLM 연결해서 체인 만들기
basic_chain = prompt | llm

# k=2의 의미는 마지막 대화 2개까지만 메모리에 저장한다는 의미임
memory = ConversationBufferWindowMemory(k=2, memory_key="chat_history")

llm_chain = LLMChain(
    prompt=prompt,
    llm=llm,
    memory=memory
)

"""
  {
    "input_prompt": "Hi! My name is Maarten and I am 33 years old. What is 1 + 1?",
    "chat_history": "",
    "text": " Hello Maarten! 1 + 1 equals 2.\n\nHowever, based on the given context, this seems like a simple math question rather than discussing personal information. If you have any other topics in mind for our conversation, feel free to share them!"
  },
"""
print(llm_chain.invoke({"input_prompt":"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?"}))

"""
 {
    "input_prompt": "What is 3 + 3?",
    "chat_history": "Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\nAI:  Hello Maarten! 1 + 1 equals 2.\n\nHowever, based on the given context, this seems like a simple math question rather than discussing personal information. If you have any other topics in mind for our conversation, feel free to share them!",
    "text": " Hello! 3 + 3 equals 6. If you'd like to discuss more topics, I'm here to chat about anything that interests you!"
  },
"""
print(llm_chain.invoke({"input_prompt":"What is 3 + 3?"}))

"""
  {
    "input_prompt": "What is my name?",
    "chat_history": "Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\nAI:  Hello Maarten! 1 + 1 equals 2.\n\nHowever, based on the given context, this seems like a simple math question rather than discussing personal information. If you have any other topics in mind for our conversation, feel free to share them!\nHuman: What is 3 + 3?\nAI:  Hello! 3 + 3 equals 6. If you'd like to discuss more topics, I'm here to chat about anything that interests you!",
    "text": " Hello! Your name is Maarten, as you mentioned at the beginning of our conversation. How can I assist you further today? If you have any other questions or topics in mind, feel free to ask!"
  },
"""
print(llm_chain.invoke({"input_prompt":"What is my name?"}))

"""
 {
    "input_prompt": "What is my age?",
    "chat_history": "Human: What is 3 + 3?\nAI:  Hello! 3 + 3 equals 6. If you'd like to discuss more topics, I'm here to chat about anything that interests you!\nHuman: What is my name?\nAI:  Hello! Your name is Maarten, as you mentioned at the beginning of our conversation. How can I assist you further today? If you have any other questions or topics in mind, feel free to ask!",
    "text": " As an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I'm here to help answer questions and provide information! If you need assistance determining a general approach to finding out your age or related topics, feel free to ask. However, for privacy reasons, I can't access or disclose personal details such as someone's age without their consent."
  }
"""
print(llm_chain.invoke({"input_prompt":"What is my age?"}))
```

<br>

# 대화 요약
- LLM에게 대화를 기억하는 능력을 부여하는 것은 좋은 상호작용 경험을 위해 필수적임
- ConversationSummaryMemory는 전체 대화 기록을 요약해서 핵심요점을 추출하는 방법을 제공함
- 대화 기록을 입력으로 사용해서 다른 LLM에게 간결한 요약을 생성하라고 요청하게됨

<br>

### 대화 요약방법
- 이러한 방법은 추론 시에 지나치게 많은 토큰을 사용하지 않고 상대적으로대화 기록을 작게 유지하는데 도움이 됨
- 하지만 단점은 동일한 LLm을 여러 번 호출해야 하므로 계산 시간이 오래걸릴수 있음
- `load_memory_variables` 메소드를 통해서 현재까지의 요약된 내용 확인이 가능함
```python
from langchain import PromptTemplate, LlamaCpp, LLMChain
from langchain.memory import ConversationSummaryMemory

llm = LlamaCpp(
    model_path="Phi-3-mini-4k-instruct-fp16.gguf",
    n_gpu_layers=-1,
    max_tokens=500,
    n_ctx=2048,
    seed=42,
    verbose=False
)

# 채팅 히스토리를 포함한 프롬프트 작성
template = """<s><|user|>Current conversation:{chat_history}

{input_prompt}<|end|>
<|assistant|>"""

prompt = PromptTemplate(
    template=template,
    input_variables=["input_prompt", "chat_history"]
)

# 프롬프트 템플릿과 LLM 연결해서 체인 만들기
basic_chain = prompt | llm

summary_prompt_template = """<s><|user|>Summarize the conversations and update with the new lines.

Current summary:
{summary}

new lines of conversation:
{new_lines}

New summary:<|end|>
<|assistant|>"""
summary_prompt = PromptTemplate(
    input_variables=["new_lines", "summary"],
    template=summary_prompt_template
)

memory = ConversationSummaryMemory(
    llm=llm,
    memory_key="chat_history",
    prompt=summary_prompt
)

llm_chain = LLMChain(
    prompt=prompt,
    llm=llm,
    memory=memory
)

"""
{
  "input_prompt": "Hi! My name is Maarten. What is 1 + 1?",
  "chat_history": "",
  "text": " The answer to 1 + 1 is 2. It's a basic arithmetic operation where if you add one unit to another, you get two units as the sum.\n\nHere's a small context for your introduction:\n\nHi Maarten! It's great to meet you. My name is [Assistant]. If you need help with any calculations or have other questions in mind, feel free to ask."
}
"""
print(llm_chain.invoke({"input_prompt": "Hi! My name is Maarten. What is 1 + 1?"}))

"""
{
  "input_prompt": "What is my name?",
  "chat_history": " Hi Maarten! It's great to meet you. My name is [Assistant]. I helped you with your simple arithmetic question, confirming that 1 + 1 equals 2. If you have any more calculations or questions in mind, feel free to ask.",
  "text": " Your name is Assistant."
}
"""
print(llm_chain.invoke({"input_prompt": "What is my name?"}))

"""
{
  "input_prompt": "What was the first question I asked?",
  "chat_history": " Hello Assistant! It's great to interact with you. As your helpful AI, I assisted in confirming that 1 + 1 equals 2 when you asked a simple arithmetic question previously. You also inquired about your name today, and I can confirm it is Assistant. If there are any further calculations or questions you need help with, please let me know!",
  "text": " The first question you asked was about the sum of 1 + 1, confirming that it equals 2."
}
"""
print(llm_chain.invoke({"input_prompt": "What was the first question I asked?"}))

"""
{
  "chat_history": " You initially asked the AI to confirm that 1 + 1 equals 2, and it reaffirmed this arithmetic fact. Later, you inquired about your name, which the AI confirmed as 'Assistant.' When asked for a recap of your first question, the AI reminded you that it was regarding the sum of 1 + 1 equaling 2."
}
"""
print(memory.load_memory_variables({}))
```