# 랭체인으로 양자화된 모델 로드하기
- GGUF 모델은 LLM 파라미터를 표현하는 데 필요한 비트 수를 축소하는 `양자화` 기법을 사용해서 원본 모델을 압축한 버전임
- 양자화는 대부분 원본 정보를 유지하면서 LLM의 파라미터를 표현하기 위해 필요한 비트 수를 줄여줌
- 핵심적인 정보를 유지하면서 값의 정밀도를 줄이기 위한 비슷한 과정이라고 볼 수 있음

<br>

# 모델 로드
### 양자화된 모델 다운로드
```
wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf 
```

<br>

### 모델 로드 및 출력확인
- 이 때 아무런 출력도 되지 않음
- Phi-3 모델은 프롬프트 템플릿이 필요하므로 명시적으로 구성해야함
```python
from langchain import LlamaCpp

llm = LlamaCpp(
    model_path="Phi-3-mini-4k-instruct-fp16.gguf",
    n_gpu_layers=-1,
    max_tokens=500,
    n_ctx=4096,
    seed=42,
    verbose=False
)

output = llm.invoke("Hi! My name is Maarten. What is 1 + 1?")

print(output)  # 출력 X
```
