# 텍스트 클러스터링을 위한 파이프라인
- 텍스트 클러스터링을 사용하면 익숙하거나 익숙하지 않은 데이터에서 패턴을 찾을 수 있음
- 작업뿐만 아니라 작업의 복잡도를 직관적으로 이해하도록 도와줌
- 결과적으로 텍스트 클러스터링은 탐색적 데이터 분석을 신속하게 수행하는 방법 그 이상이 되었음

<br>

### 일반적인 파이프라인
- 그래프 기반 신경망부터 다양한 기법이 존재함
- 하지만 일반적으로 사용되는 파이프라인은 3개의 단계와 알고리즘으로 구성됨
- `임베딩 모델`을 사용해서 입력 문서를 임베딩으로 변환
- `차원 축소 모델`을 사용해 임베딩의 차원을 줄임
- `클러스터링 모델`을 사용해서 의미가 있는 비슷한 문서의 그룹을 찾음

<br>

# 문서 임베딩
- 의미적으로 비슷한 문서를 찾아야하므로 의미 유사도 작업에 최적화된 임베딩 모델을 선택하는게 특히 중요함
- 다행히 요즘은 대부분 임베딩 모델은 의미 유사도에 초점을 맞추고 있음
```python
from datasets import load_dataset
from sentence_transformers import SentenceTransformer

dataset = load_dataset("maartengr/arxiv_nlp")["train"]

abstracts = dataset["Abstracts"]
titles = dataset["Titles"]

embedding_model = SentenceTransformer("thenlper/gte-small")
embeddings = embedding_model.encode(abstracts, show_progress_bar=True)

"""
임베딩 문서마다 문서의 의미 표현을 표현하는 384개의 값을 가진다는 뜻
(44949, 384)
"""
print(embeddings.shape)
```

<br>

# 임베딩 차원 축소하기
- 차원수가 증가함에 따라서 가능한 값의 개수가 지수적으로 증가하므로 임베딩 공간 안에 있는 모든 부분공간을 찾는 일이 점점 복잡해짐
- 결국 고차원 데이터는 해당 데이터에서 의미 있는 클러스터를 찾는 것이 어렵기 때문에 많은 클러스터링 방법에서 문제가 될 수 있음
- 그래서 차원 축소 방법을 사용하는데 이것의 목표는 고차원 데이터의 `전역 구조`를 보존하는 저차원 표현을 찾는것임
- 이는 압축 기술이며 알고리즘의 임의로 차원을 삭제하지 않음

<br>

### UMAP
- 유명한 차원 축소 방법은 `PCA(Principal Component Analysis)`와 `UMAP(Uniform Manifold Approximation and Projection)`가 있음
```python
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from umap import UMAP

dataset = load_dataset("maartengr/arxiv_nlp")["train"]

abstracts = dataset["Abstracts"]
titles = dataset["Titles"]

embedding_model = SentenceTransformer("thenlper/gte-small")
embeddings = embedding_model.encode(abstracts, show_progress_bar=True)

umap_model = UMAP(
    # 저차원 공간의 크기를 결정하는 파라미터
    # 일반적으로 5 ~ 10 사이의 값이 고차원 전역 구조를 포작하는데 잘 맞음
    n_components=5,

    # 임베딩된 포인트 사이의 최소 거리
    # 값을 0으로 조정하면 조밀한 클러스터가 만들어짐
    min_dist=0.0,

    # 유클리드 기반 지표는 고차원 데이터를 다루기 어려워서 metric 파라미터를 cosine으로 지정
    metric='cosine',

    # 해당 파라미터를 설정하면 실행할 때마다 같은 결과를 재현할 수 있지만 병렬 실행이 안되서 훈련 속도가 느려짐
    random_state=42
)
reduced_embeddings = umap_model.fit_transform(embeddings)
```

<br>

# 축소된 임베딩 클러스터링
- 일반적으로 k-평균과 같은 센트로이드 기반 알고리즘은 선택함
- 생성할 클러스터 개수를 지정해야 하지만 클러스터 개수를 사전에 알수없음
- 밀도 기반 알고리즘은 자유롭게 클러스터 개수 결정이 가능하고 모든 데이터 포인트를 클러스터에 할당하지는 않음

<br>

### 밀도 기반 모델
- HDBSCAN은 클러스터링 알고리즘 DBSCAN의 게층형 버전임
- 이 알고리즘은 명시적으로 클러스터 개수를 지정하지 않고 조밀한 클러스터를 찾을 수 있음
- 밀도 기반 방법이므로 데이터에 있는 `이상치`도 감지가 가능한데 이는 어떤 클러스터에도 속하지 않는 데이터 포인트를 뜻함

```python
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN

dataset = load_dataset("maartengr/arxiv_nlp")["train"]

abstracts = dataset["Abstracts"]
titles = dataset["Titles"]

embedding_model = SentenceTransformer("thenlper/gte-small")
embeddings = embedding_model.encode(abstracts, show_progress_bar=True)

umap_model = UMAP(
    n_components=5,
    min_dist=0.0,
    metric='cosine',
    random_state=42
)
reduced_embeddings = umap_model.fit_transform(embeddings)

# 모델 훈련 후 클러스터 추출
hdbscan_model = HDBSCAN(min_cluster_size=50).fit(reduced_embeddings)
cluster = hdbscan_model.labels_

# 클러스터 개수 조회, 156
print(len(set(cluster)))
```

<br>

# 클러스터 조사
