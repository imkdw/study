# 텍스트 클러스터링에서 토픽 모델링으로
- 텍스트 데이터 집합에서 주제나 잠재적인 토픽을 찾는 것을 `토픽 모델링`이라고 부름
- 토픽에 하나의 레이블을 할당하는게 아닌 일련의 키워드를 통해서 토픽의 의미를 이해해야함
- `잠재 디리클레 할당`같은 전통적인 방법은 각 토픽이 말뭉치의 어휘사전에 있는 단어의 확률 분포로 표현된다고 가정함
- 이런 방법들은 BoW 기법을 사용하는데, BoW는 단어와 구의 맥락이나 의미를 고려하지 않음

<br>

# BERTopic : 모듈화된 토픽 모델링 프레임워크
- BERTopic은 의미적으로 유사한 텍스트 크러스터를 활용해서 다양한 종류의 토픽 표현을 추출하는 토픽 모델링 기법
- 텍스트 클러스터링과 동일한 과정을 거치는데 알고리즘은 2단계로 생각이 가능함

<br>

### 토픽 모델링 알고리즘
- 텍스트 클러스터링처럼 문서를 임베딩, 차원을 축소하고 축소된 임베딩을 클러스터링해서 의미적으로 유사한 문서 그룹을 생성
- 그리고나서 고전적인 BoW 방식을 사용해서 말뭉치의 어휘사전에 있는 단어의 분포를 모델링함

<br>

### 위 방법의 단점
- 우리가 관심을 두는것은 클러스터 수준의 관점인데 BoW는 문서 수준의 관점임
- 이를 해결하기 위해서 단어의 빈도를 문서가 아닌 클러스터를 기준으로 계산함

<br>

### BERTopic의 가중치 할당 방법
- 클래스 기반 TF-IDF를 사용해서 클러스터에 의미 있는 단어에는 높은 가중치를 부여, 모든 클러스터에서 사용되는 단어에는 낮은 가중치를 할당함
- `CountVectorizer`를 사용해서 클래스 기반 BoW를 만들고 `c-TF-IDF`를 사용해서 단어에 가중치를 부여함

<br>

### BERTopic 파이프라인
- 토픽 클러스터링과 토픽 표현 두 단계를 합치면 BERTopic 파이프라인이 구성됨
- 이 파이프라인을 사용해서 의미적으로 비슷한 문서를 클러스터로 모으고, 이 클러스터에서 몇 개의 키워드로 표현되는 토픽 생성이 가능함
- 이 파이프라인의 주요 장점은 클러스터링과 토픽 표현 두 단계가 서로 독립적이라는 것임
- 문서 클러스터링과 의존적이지 않고 모듈화가 가능한데 파이프라인의 각 요소는 비슷한 다른 알고리즘으로 완전히 대체가 가능하고 모듈화 덕분에 새로운 모델을 BERTopic의 아키텍처 내부에 통합이 가능함

<br>

### BERTopic 생성하기
```python
from bertopic import BERTopic
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
import numpy as np

dataset = load_dataset("maartengr/arxiv_nlp")["train"]

abstracts = dataset["Abstracts"]
titles = dataset["Titles"]

embedding_model = SentenceTransformer("thenlper/gte-small")
embeddings = embedding_model.encode(abstracts, show_progress_bar=True)

umap_model = UMAP(
    n_components=5,
    min_dist=0.0,
    metric='cosine',
    random_state=42
)
reduced_embeddings = umap_model.fit_transform(embeddings)

# 모델 훈련 후 클러스터 추출
hdbscan_model = HDBSCAN(min_cluster_size=50).fit(reduced_embeddings)
clusters = hdbscan_model.labels_

# 클러스터 개수 조회
print(len(set(clusters)))

cluster = 0
for index in np.where(clusters == cluster)[0][:3]:
    print(abstracts[index][:300] + "... \n")

# 384차원 -> 2차원으로 줄임
reduced_embeddings = UMAP(
    n_components=2, min_dist=0.0, metric='cosine', random_state=42
).fit_transform(embeddings)

# BERTopic 모델 훈련
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    verbose=True
).fit(abstracts, embeddings)

"""
get_topic_info : 발견한 토픽에 대해서 간단한 정보를 제공함
"""
print(topic_model.get_topic_info())

"""
get_topic(index) : 개별 토픽 조사가 가능하고, 가장 잘 표현하는 키워드 확인이 가능
"""
print(topic_model.get_topic(0))

"""
get_topic(str) : 검색어를 기반으로 토픽 조회하기
"""
print(topic_model.find_topics(0))
```

<br>

# 특수 레고 블록 추가하기