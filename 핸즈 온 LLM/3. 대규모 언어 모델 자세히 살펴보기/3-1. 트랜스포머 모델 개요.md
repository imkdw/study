# 훈련된 트랜스포머 LLM의 입력과 출력
- 트랜스포머 LLM은 텍스트를 받아서 응답으로 텍스트를 생성할 수 있음
- 이러한 모델은 한번에 모든 텍스트를 생성하지 않고 실제로 한 번에 하나의 토큰씩 생성함
- 각각의 토큰 생성 과정은 모델을 통과하는 한 번의 정방향 게산에 해당됨
- 하나의 토큰을 생성하고 출력 토큰을 입력 프롬프트의 끝에 추가해서 다음 생성 단계를 위한 프롬프트로 사용함

<br>

### 반복해서 토큰을 생성하는 방법
- 기본적으로 루프 안에서 이 과정을 실행해서 완료할 때까지 생성된 텍스트로 순차적으로 확장함
- 모델이 입력 프롬프트를 기반으로 다음 토큰을 예측하는 작업 과정을 정확하게 이해할 수 있음
- 이러한 특성을 `자기회귀 모델`이라고 부르는데 텍스트 생성 LLM은 이러한 특성이 있어거 자기회귀 모델이라고 부름

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cuda",
    torch_dtype="auto",
    trust_remote_code=False,
)

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=50,
    do_sample=False
)

prompt = "write an email apologizing to Sarach for the tragic gadening mishap. Exaplain how it happended"
output = generator(prompt)

"""
Subject: Sincere Apologies for the Unfortunate Incident

Dear Sarach,

출력 토큰을 50으로 설정해서 50자 까지만 출력됨
"""
print(output[0]['generated_text'])
```

<br>

# 정방향 계산의 구성요소
- 자기회귀 모델 외 다른 두 개의 핵심 구성 요소는 `토크나이저`와 `언어 모델링 헤드(LM헤드)`임
- 토크나이저 다음에는 모든 처리 과정을 담당하는 `트랜스포머 블록`을 쌓은 신경망이 이어짐
- 그 다음에 `LM 헤드`가 나오는데 이는 트랜스포머 스택의 출력을 가장 가능성 있는 다음 토큰에 대한 확률로 변환함
- 토크나이저는 토큰의 테이블인 `어휘사전`을 가지는데 트랜스포머 모델은 어휘사전에 존재하는 각 토큰에 연관된 벡터 표현(임베딩)을 가짐
- 다양한 종류의 시스템을 만들기 위해서는 트랜스포머 블록의 스택에 여러 종류의 헤드를 붙일 수 있는데 LM 헤드도 그 중 하나임

<br>

# 확률 분포로부터 하나의 토큰 선택(샘플링/디코딩)
- 토큰 생성 과정의 마지막 모델이 출력하는 건 어휘사전에 있는 각 토큰에 대한 확률 점수임
- 이 확률 분포에서 하나의 토큰을 선택하는 방법을 `디코딩 전략`이라고 부름

<br>

### 디코딩 전략(Decoding Strategy)
- 가장 간단한 디코딩 전략은 확률 점수가 가장 높은 토큰을 고르는 것임
- 하지만 실제로는 대부분의 경우에 위 방식이 좋은 출력을 내지 못하는 경향이 있음
- 약간의 무작위성을 가미하거나 두, 세번째로 확률이 높은 토큰을 고르는 것이 더 나음

<br>

### 탐욕적 검색(Greedy Search)
- 항상 가장 높은 점수의 토큰을 선택하는 방법을 뜻함
- LLM에서 `온도(temperature)` 파라미터를 0으로 설정하면 이 방식을 사용하게됨

<br>

### 작동 과정
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cuda",
    torch_dtype="auto",
    trust_remote_code=False,
)

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=50,
    do_sample=False
)

prompt = "The capital of France is"

# 입력 프롬프트를 토큰으로 변환
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# 입력 토큰을 GPU에 배치
input_ids = input_ids.to("cuda")

# lm_head 앞에 있는 model의 출력을 가져옴
model_output = model.model(input_ids)

# lm_head의 출력을 가져옴
lm_head_output = model.lm_head(model_output[0])

# 가장 높은 확률의 토큰을 선택
token_id = lm_head_output[0, -1].argmax(-1)

# Paris
print(tokenizer.decode(token_id))
```

<br>

# 병렬 토큰 처리와 문맥 크기
- 트랜스포머는 언어 처리 분야의 기존 신경망 구조보다 병렬 처리가 뛰어남
- 이 때 각 토큰은 독자적인 계산 스트림을 통해서 처리됨
- 트랜스포머 모델은 동시에 처리할 수 있는 토큰 수가 제한되는데 이런 제한을 `문맥 길이`라고 부름
  - 예를 들어 문맥 크기가 4K인 모델은 4K의 토큰만 처리가 가능함
- 각 입토큰 스트림은 입력 벡토러 시작하는데 스트림의 끝에서 모델의 처리 결과로 또 다른 벡터가 만들어지게됨

<br>

### 처리 스트림
- 텍스트 생성의 경우 마지막 스트림의 출력 결과만 사용해서 다음 토큰을 예측하게됨
- 이 출력 벡터만 LM 헤드에 전달되어서 다음 토큰의 확률을 계산함
- 사실 마지막 토큰을 제외한 모든 토큰의 결과는 버려지는데 왜 모든 모큰 스트림을 계산하기 위해서 궁금할 수 있음
- 트랜스포머 블록의 `어텐션 매커니즘`은 이전 스트림의 출력을 사용하기 때문임

<br>

# 키와 값을 캐싱해서 생성 속도 높이기
- 두 번쨰 토큰을 생성할 때 단순히 첫 번째 출력 토큰을 입력에 덧붙여서 또 한번 모델의 정방향 계산을 수행하게됨
- 만약 모델이 이전의 계산 결과를 캐싱할 수 있다면 이전 스트림의 계산을 반복할 필요가 없음
- 이런 최적화 기법은 `키와 값 캐시`라고 부르고 텍스트 생성 과정의 속도를 크게 높여줌
- `transformers`는 기본적으로 캐싱을 사용하며 `use_cache` 파라미터를 False로 지정해서 비활성화도 가능함
- 사실상 캐싱을 사용해도 N초가 걸리는데 이는 사용자한테는 오래 걸리는 요청임
  - UX를 위해 대부분의 LLM API는 전체 문장이 완성되기 전에 모델이 생성하는 토큰을 스트리밍함

<br>

# 트랜스포머 블록 내부
- LLM은 일련의 트랜스포머 블록으로 구성되는데 대규모 LLM은 100개 이상을 사용하기도함
- 각 블록은 입력을 처리한 다음 이 결과를 다음 블록으로 전달함
- 트랜스포머 블록에는 `어텐션 층`, `피드포워드 층` 총 2개의 구성 요소가 있음

<br>

### 어텐션 층(Attention Layer)
- 모델이 특정 토큰을 처리할 떄 문맥을 통합하도록 도와주는 메커니즘임
- 예를 들어 The dog chased the squirrel because it 이라는 문단에서 it은 뭘 뜻할지 고민해야함
- 여기서 어텐션 매커니즘에 it에 올 단어를 결정하게 되는데 이는 문맥 정보를 it 토큰의 표현에 추가함
- 현재 토큰을 처리하는 데 도움이 되는 이전 위치의 관련 정보를 통합하는 역할을함

<br>

### 피드포워드 층(Feedforward Layer)
- 모델의 처리 용량의 대부분을 담당하는 구성요소
- 대규모 텍스트 데이터에서 성공적으로 훈련될 떄 이 작업을 성공시키기 위한 정보 및 동작을 학습하고 저장함
- 즉 모델의 기억과 보간(interpolation)을 담당하게됨
- LLM이 성공적으로 훈련되려면 많은 정보를 암기해야 하는데 인상적인 텍스트생성 레시피에서 필요한 하나의 재료일 뿐임
- 모델은 이러한 메커니즘을 통해서 데이터 포인트 사이와 복잡한 패턴 사이를 보간해서 일반화가 가능함

<br>

### 어텐션이 전부입니다
- 어텐션 매커니즘에 있는 주요한 두 개의 단계는 아래와 같음
  - 이전 입력 토큰이 현재 처리 대상 코튼에 얼마나 관련이 있는지 점수를 매김
  - 이 점수를 사용해서 다양한 위치의 토큰에서 얻은 정보를 하나의 출력 벡터에 통합하게됨
- 트랜스포머에 더 광범위한 어텐션 능력을 부여하기 위해서 어텐션 매커니즘을 여러 벌 만들어서 병렬로 실행함
- 이 때 병렬로 실행되는 각각의 어텐션 메커니즘을 `어텐션 헤드`라고 부름

<br>

### 어텐션 계산 과정
- 어텐션 층은 한 위치의 토큰에 대한 어텐션을 처리하게됨
  - 해당 층에는 `현재 위치 또는 토큰에 대한 벡터표현`, `이전 토큰에 대한 벡터 표현`이 입력됨
- 이전 토큰에서 관련 정보를 통합해서 현재 위치에 대한 새로운 표현을 생성하는것이 목표
- 훈련 과정을 통해서 이 계산에 활용되는 3개의 투영 행렬이 생성됨
  - 쿼리 투영 행렬, 키 투영 행렬, 값 투영 행렬

<br>


### 셀프 어텐션: 관련성 점수 계산
- 생성형 트랜스포머에선 한 번의 하나의 토큰을 생성함
- 어텐션 매커니즘은 한 위치에만 관심을 두며 다른 위치에서 정보를 가져와서 어떻게 이 위치에 반영할 수 있는지에 초점을 맞춤
- 관련성 점수 계산 단게는 현재 위치의 쿼리 벡터와 키 행렬을 곱해서 수행되는데 이를 통해 이전 토큰이 얼마나 관련 있는지를 나타내는 점수가 만들어짐
- `소프트맥스` 연산을 통해서 이 점수의 합이 1이 되도록 정규화를 진행함

<br>

### 셀프 어텐션 : 정보 통합
- 관련성 점수가 준비되는 각 토큰에 연관된 값 벡터와 이 점수를 곱함
- 계산 결과 벡터를 모두 더하면 어텐션 단게의 출력이 됨