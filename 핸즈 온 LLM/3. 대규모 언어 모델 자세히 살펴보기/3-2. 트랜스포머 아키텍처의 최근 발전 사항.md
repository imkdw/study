# 효율적인 어텐션
- 어텐션 연산은 계산 비용이 가장 비싼 부분임
- 연구 커뮤니티에서가장 많은 관심을 받는 부분은 트랜스포머 어텐션 층임

<br>

### 로컬/희소 어텐션
- 희소 어텐션은 모델이 주의를 기울일 수 있는 이전 토큰의 문맥을 제한함
- 성능을 포이기 위함으로 적은 수의 이전 위치에만 주의를 기울이게됨
- 하지만 모든 트랜스포머 블록에서 사용하는건 아니고 GPT-3의 경우 `완전 어텐션`과 섞어서 사용함
- 이는 디코더 트랜스포머 블록의 자기회귀적인 특징도 보여주는데, 이 때 양쪽으로 주의를 기울일 수 있는 BERT와는 다름

<br>

### 멀티 쿼리 어텐션, 그룹 쿼리 어텐션
- 최근에 개발된 더 효율적인 트랜스포머 어텐션은 llama2, 3 같은 모델에 적용된 `그룹 쿼리 어텐션`임
- 그룹 쿼리 어텐션은 멀티 쿼리 어텐션을 기반으로 하며 행렬 크기를 감소시켜서 대규모 모델의 추론능력을 향상시킴

<br>

### 어텐션 최적화 : 멀티 헤드, 멀티 쿼리, 그룹 쿼리
- 멀티 쿼리 어텐션의 최적화 방법은 키와 값 행렬을 모든 헤드에서 공유하는 것임
- 따라서 각 헤드에서 고유한 행렬은 쿼리 행렬뿐임
- 어텐션은 쿼리, 키, 값 행렬을 사용해서수행되는데 멀티 쿼리 어텐션은 모든 어텐션 헤드가 키와 값 행렬을 공유해서 더 효율적인 어텐션 매커니즘이 됨
- 하지만 멀티 쿼리 어텐션의 경우 모델 크기가 커지면서 이런 최적화는 매우 과할 수 있고 모델의 품질을 향상시키기 위해서 메모리를 더 사용할 수 있음
- 그룹 쿼리 어텐션은 키와 값 행렬을 하나만 사용하는게 아닌 여러개를 사용할 수 있음. 단 헤드의 개수보다는 적음

<br>

### 플래시 어텐션
- 트랜스포머 LLM이 GPU에서 훈련하거나 추론할 떄 속도를 크게 높일 수 있는 인기 있는 방법이자 구현임
- GPU의 `공유 메모리(SRAM)`와 `고대역폭 메모리(HBM)` 사이에 이동값을 최적화해서 어텐션 계산의 속도를 높임

<br>

# 트랜스포머 블록
- 트랜스포머 블록의 주요 구성요소는 어텐션 층과 피드포워드 신경망임
- 블록 내부에는 `잔차 연결(residual connection)`과 `층 정규화(Layer normalization)`이 있음
- 새로운 버전의 트랜스포머 블록에서는 정규화가 어텐션과 피드포워드 층 이전에 등장한다는 것임
- 이는 훈련 시간을 단축시켜준다고 보고되었음
- 또한 정규화에서 층 정규화보다 더 간단하고 효율적인 `RMS 정규화`를 사용함

<br>

# 위치 임베딩(RoPE)
- 위치 임베딩은 원본 트랜스포머 이후로 핵심 요소가 되었는데 이를 통해 모델이 시퀀스/문장 안에서 토큰/단어 순서 추적이 가능해짐
- 이는 언어에서 필수적인 정보이며 지난 몇년간 위치 인코딩 방법이 많이 등장했는데 그중에 `로터리 위치 임버딩`이 주목할만함
- 원본 트랜스포머 및 초기 변종 모델은 절대 위치 임베딩을 사용함
  - ex) 첫번째 토큰은 1, 두번째 토큰은 2 이런식으로 사용
  - 하지만 모델의 규모를 늘리면서 이런 방법에서 어려움이 많이생겨났음
- 로터리 위치 임베딩은 모델 훈련시 문서를 연결해서 훈련 배치의 문맥을 채우게됨
- 문맥 크기를 `문서 + 패딩`으로 다 채우는게 아닌 `문서1 + 구분자 + 문서2 + 패딩` 방식으로 문맥을 채움
- 정방향 계산 시작 부분에 더해지는 정적이고 절대적인 임베딩 대신에 로터리 임베딩은 상대적인 토큰 위치 정보를 인코딩하는 방법임