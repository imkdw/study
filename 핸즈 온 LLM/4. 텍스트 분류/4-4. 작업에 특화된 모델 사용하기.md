# 작업에 특화된 모델 사용하기
### 모델을 로딩하고 추론 및 분류 리포트 생성
- 리포트를 생성하면 `혼동 행렬(confusion martrix)`이 생성됨
- 올바른 예측 대비 잘못된 예측, 올바른 클래스 대기 잘못된 클래스에 따라 4개의 조합이 생성됨
```python
from transformers import pipeline
import numpy as np
from tqdm import tqdm
from transformers.pipelines.pt_utils import KeyDataset
from sklearn.metrics import classification_report
from datasets import load_dataset

# 영화 리뷰 데이터셋 로딩
data = load_dataset("rotten_tomatoes")


# 분류 리포트를 만드는 함수
def evaluate_performance(y_true, y_pred):
    performance = classification_report(
        y_true, y_pred,
        target_names=["Negative Review", "Positive Review"]
    )
    print(performance)


# 트위터 데이터셋으로 학습된 감성 평가 모델 로딩
model_path = "cardiffnlp/twitter-roberta-base-sentiment-latest"
pipe = pipeline(
    model=model_path,
    tokenizer=model_path,
    return_all_scores=True,
    device="cpu"
)

# 추론 진행
y_pred = []
for output in tqdm(pipe(KeyDataset(data["test"], "text")), total=len(data["test"])):
    negative_score = output[0]["score"]
    positive_score = output[2]["score"]
    assignment = np.argmax([negative_score, positive_score])
    y_pred.append(assignment)

#
"""
                 precision    recall  f1-score   support

Negative Review       0.76      0.88      0.81       533
Positive Review       0.86      0.72      0.78       533

       accuracy                           0.80      1066
      macro avg       0.81      0.80      0.80      1066
   weighted avg       0.81      0.80      0.80      1066
"""
evaluate_performance(data["test"]["label"], y_pred)
```

<br>

### 분류 리포트 분석방법
- 총 4가지로 `정밀도(precision)`, `재현율(recall)`, `정확도(accuracy`, `F1 점수`가 있음
- `정밀도`는 찾은 것 중에 얼마나 많은 항목이 관련된 것인지를 측정함, 관련되었다고 찾은 결과의 정확도를 나타냄
- `재현율`은 관련된 클래스를 얼마나 많이 찾았는지 즉 관련 결과를 모두 찾는 능력을 나타냄
- `정확도`는 모델의 전체 예측 중에 올바른 예측이 얼마나 많은지를 나타냄, 모델의 전반적인 올바름 정도임
- `F1 점수`는 정밀도와 재현율 사이에 균형을 맞추어서 모델의 전체적인 성능을 평가함
- 각 클래스를 동등하게 취급하기 위해 F1 점수의 가중 평균을 사용시 `0.8`로 영화 데이터로 훈련한게 아닌거 치고는 높은 점수임
- 더 높은 점수를 얻기 위해서 영화 리뷰 데이터에서 학습된 다른 모델을 사용하는것도 좋은 선택임