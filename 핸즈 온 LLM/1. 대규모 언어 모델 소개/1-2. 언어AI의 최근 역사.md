# 언어 AI의 최근 역사

- 언어를 표현하고 생성하기 위한 많은 알고리즘과 모델이 출현했음
- 하지만 언어는 컴퓨터에게는 어려운 개념임
- 텍스트는 태생적으로 비구조적이고 0과 1로 표현될 때 의미를 잃음
- 결과적으로 언어 AI의 역사에는 컴퓨터가 쉽게 사용할 수 있도록 언어를 구조적인 방식으로 표현하는데 많은 관심이 집중됨
- 텍스트(비구조적인 데이터) 입력 -> `언어 모델` -> 텍스트 출력 / 임베딩 변환 / 분류가 언어모델의 역할임

<br>

# BoW(bag-of-words)로 언어 표현하기

- 언어 모델의 역사는 비구조적인 텍스트를 표현하는 한 방법인 BoW라는 기법으로 시작됨
- BoW 모델의 첫번째 단계는 문장을 개별 단어나 부분 단어로 분할하는 `토큰화`임
- 가장 널리 사용되는 토큰화 방법은 공백을 기준으로 개별 단어로 분할하는 것임
- 토큰화 다음에는 각 문장의 고유한 단어를 모두 합쳐서 `어휘사전`을 만듬
- 어휘사전을 사용해서 각 문장에 단어가 얼마나 많이 등장하는지 헤아리는데 그래서 이 방법을 BoW 라고 부름

<br>

# 밀집 벡터 임베딩으로 더 나은 표현 만들기

- work2vec은 임베딩으로 텍스트의 의미를 포작하는데 성공한 첫 번째 시도였음
- 임베딩은 데이터의 의미를 포작하기 위한 벡터 표현임
- `신경망`을 통해서 표현을 생성하는데, 이는 여러층이 서로 연결되어 구성되며 각 층은 정보를 처리하는 노드로 이루어짐
- 한 층의 노드와 다음 층의 노드를 잇는 연결에는 입력값에 따라서 특정한 가충치가 부여되는ㄷ 이를 모델의 `파라미터`라고 부름
- word2vec은 신경망을 사용해서 주어진 문장에서 다음에 어떤 단어가 등장하느지를 살펴봄으로써 단어 임베딩을 생성함

<br>

### word2vec의 상세 과정

- 먼저 어휘사전에 있는 모든 단어에 대해서 랜덤하게 초기화된 일련의 값을 단어 임베딩으로 할당함
- 훈련 스텝마다 훈련 데이터에서 단어 쌍을 가져와서 모델이 문장 안에서 단어 쌍이 이웃에 나타날 가능성이 있는지 예측함
- 만약 두 단어가 이웃에 나타날 가능성이 높다면 두 단어의 임베딩은 서로 매우 가까워짐

<br>

### word2vec을 통해 생성된 임베딩

- 이렇게 생성된 임베딩은 단어의 의미를 포착하는데 임베딩은 단어의 속성을 표현함으로써 의미를 포착하게됨
- `newborn`, `human` 두 가지 속성에서는 높은 점수를 가지지만 `apple`은 이런 속성에서 낮은 값을 가짐
- 이런 속성은 매우 모호하여 사람이 식별할 수 있는 개념과는 연관이 되지는 않음
- 하지만 이런 속성을 합치면 컴퓨터에게 의미가 있고 사람의 언어를 컴퓨터 언어로 바꾸는 좋은 방법이됨
- 임베딩은 단어 사이에 있는 의미 유사성을 측정할 수 있기 때문에 매우 유용함

<br>

# 임베딩의 종류

- 임베딩의 종류는 많은데 `단어 임베딩`과 `문장 임베딩`은 다른 수준의 추상화에 사용됨
- `BoW`는 전체 문서를 표현하므로 문서 수준에서 임베딩 만들고, `word2vec`은 단어 수준에서 임베딩을 만듬

<br>

# 어텐션을 사용한 문맥 인코딩 & 디코딩

- `bank`라는 단어는 어떤 문장이냐에 따라 의미가 달라질 수 있는데, 즉 임베딩은 문맥에 따라서 달라져야함
- 이런 텍스트를 인코딩하는 단계는 `RNN(Recurrent Neural Network)`으로 달성되었음

<br>

### RNN

- RNN은 연속적인 입력으로 시퀀스를 모델링할 수 있는 신경망의 한 종류임
- RNN은 두 개의 작업에 사용하는데 입력 문장을 `인코딩`하거나 표현하고, 출력 문장을 `디코딩`하거나 생성하는작업임
- `i love llamas` -> `신경망 기계번역(인코더 / 디코더)` -> `ik hou van lama's`로 변환
- 또한 RNN은 `자기회귀적`으로 이전에 생성한 모든 단어를 사용해서 다음 단어를 생성하게됨
- 인코딩 단계의 목표는 입력을 가능한 한 잘 표현해서 디코더의 입력으로 사용되는 임베딩의 형태로 문맥을 생성하는것임

<br>

### 어텐션(Attention)

- 문맥 임베딩은 하나의 임베딩으로 전체 입력을 표현하기 때문에 긴 문장을 처리하기 어려웠음
- 그래서 새롭게 나온게 `어텐션`이라는 방법이 소개됨
- 어텐션은 주어진 문장에서 어떤 단어가 가장 중요한지 선택적으로 결정하게됨
- 이러한 메커니즘을 디코더 단계에 추가해서 RNN이 입력 시퀀스의 각 단어에 대해서 출력 가능성에 관련된 신호를 생성할 수 있음
- RNN과 비교하면 이러한 구조는 텍스트의 순차적 특징과 전체 문장에 주의를 기울여서 텍스트의 문맥을 표현할 수 있음
- 하지만 이러한 순차적인 특징은 모델 훈련을 병렬화하는데 방해가됨

<br>

# Attention Is All You Need

- 어텐션 메커니즘만 사용하고 RNN을 제거한 `트랜스포머(Transformer)`라는 신경망 구조가 제안됨
- 병렬로 모델 훈련이 가능하므로 훈련 속도를 크게 높일 수 있음
- 트랜스포더는 `트랜스포머 인코더`와 `트랜스포머 디코더`를 쌓아서 구성되는데 입력은 각각의 인코더/디코더를 통과함
- 여기서는 `셀프 어텐션`과 `피드포워드 신경망` 두 부분으로 구성됨

<br>

### 셀프 어텐션 / 인코더

- 인코더 블록은 셀프 어텐션을 중심으로 작동해서 중간 표현을 생성하게됨
- 어텐션과 비교하면셀프 어텐션은 한 시퀀스 안의 다른 위치에 주의를 기울임
- 한번에 하나의 토큰을 처리하는게 아닌 전체 시퀀스를 한 번에 처리할 수 있음

<br>

### 셀프 어텐션 / 디코더

- 디코더는 인코더와 다르게 인코더의 출력에 주의를 기울이는 별도의 층이 추가됨
- 미래의 위치를 마스킹하는데 출력을 생성할 때 정보 누출 방지를 위해서 앞선 위치에만 주위를 기울일 수 있음
- 즉 미래를 참조하지 못하도록 이전 토큰에만 주의를 기울임

<br>

# 표현 모델 : 인코더 기반 모델

- 원본 트랜스포머 모델은 인코더-디코더 구조라서 번역 작업에는 좋지만 텍스트 분류같은 작업에는 쉽게 사용할 수 없음
- BERT(Bidirectional Encoder Representations from Transformers)라는 새로운 구조가 소개되었는데 이는 언어를 표현하는데 초점을 맞춘 `인코더 기반 구조`임
- 인코더 기반 구조는 인코더만 사용하고 디코더는 사용하지 않는다는 의미임

<br>

### BERT 베이스 모델의 구조

- 이전의 인코더 블록과 동일하게 셀프 어텐션 이후에 피드포워드 신경망이 나옴
- 입력에는 추가적으로 `[CLS]` 토큰이나 분류 토큰이 포함되는데 이 토큰을 전체 입력에 대한 표현으로 사용함
- 뷴류와 같은 특정 작업에서 모델을 파인튜닝 하기 위해서 `[CLS]` 토큰을 입력의 임베딩으로 사용하기도함

<br>

### 마스크드 언어 모델링(Masked Language Modeling)

- 이 방법은 모델이 예측할 입력의 일부분을 마스킹처리함
- 이런 예측 작업은 어렵지만 BERT 가 더 정확한 입력에 대한 중간 표현을 만들 수 있도록 도와줌
- 이런 구조와 훈련 방법덕에 BERT 및 이와 유사한 구조가 언어의 문맥을 표현하는데 놀라운 성능을 보여줌

<br>

### BERT의 사용처

- BERT와 같은 모델은 일반적으로 `전이 학습`에 사용됨
- 먼저 언어 모델링을 위해서 모델을 `사련 훈련`하고 그 다음 구체적인 작업을 위해서 파인튜닝을 실시함
- 사전에 훈련된 모델의 장점은 대부분의 훈련이 이미 완료된것인데 파인튜닝의 경우는 리소스를 적게 사용함
- BERT 모델은 거의 모든 단계에서 임베딩을 생성하는데 특정 작업에서 미세 튜닝할 필요없이 특성 추출기로 사용도 가능함

<br>

# 생성 모델 : 디코더 기반 모델

- 디코더 기반 모델은 주로 텍스트를 생성하는데 초점을 맞추고 임베딩을 생성하도록 훈련되지 않음
- BERT와 비슷한 구조로 `GPT(Generative Pre-trained Transformer)`라는 모델이 제안됨
- 해당 모델은 1억1700개의 파라미터(간선)을 갖는데 모든것이 동일하다면 파라미터가 많을수록 모델의 능력과 성능에 영향을 미칠꺼라고 기대해서 점점 더 큰 모델이 출시되고있음
- 디코더 기반 생성 모델, 특히 대규모 모델을 LLM(Large Language Model)이라고 부름

<br>

### 시퀀스 투 시퀀스 모델

- 시퀀스 투 시퀀스 모델로써 생성 LLM은 어떤 텍스트를 받아 이를 자동으로 완성함
- 이러한 모델의 진정한 힘은 챗봇으로 훈련되었을 때 빛을 발함
- 완성 모델에서 가장 중요한건 `문맥 길이`와 `문맥 윈도`임
- 문맥의 길이는 모델이 처리할 수 있는 최대 토큰 수를 나타내는데 문맥 길이가 크면 LLM에 문서를 통으로 전달할 수 있음

<br>

# 생성 AI의 해

- 2023년에는 다양한 AI 모델이 등장하면서 AI의 해라고 불렸음
- 그와 동시에 오픈소스 베이스 모델(파운데이션 모델)와 상용 모델도 많이 등장했음
- 파운데이션 모델은 명령 수행과 같은 특정 작업을 위해서 파인튜닝이 가능함
