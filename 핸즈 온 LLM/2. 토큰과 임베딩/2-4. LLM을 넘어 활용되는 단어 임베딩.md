# LLM을 넘어 활용되는 단어 임베딩
- 임베딩 또는 객체에 의미 있는 벡터 표현을 할당하는건 추천 엔진과 로봇공학을 포함해 많은 분야에서 사용함
- 

<br>

# 사전 훈련된 단어 임베딩 다운로드
```python
import gensim.downloader as api

model = api.load('glove-wiki-gigaword-50')

"""
[
    ('brother', 0.7492412328720093),
    ('emperor', 0.7736247777938843),
    ('ii', 0.7746230363845825),
    ('king', 1.0000001192092896),
    ('kingdom', 0.7542160749435425),
    ('prince', 0.8236179351806641),
    ('queen', 0.7839044332504272),
    ('ruler', 0.7434254288673401),
    ('son', 0.766719400882721),
    ('throne', 0.7539914846420288),
    ('uncle', 0.7627150416374207)
]

king과 같이 특정 단어와 가장 가까운 이웃 단어 조회
"""
print(model.most_similar([model['king']], topn=11))
```

<br>

# word2vec 알고리즘과 대조 훈련
- LLM과 비슷하게 word2vec은 텍스트로부터 만든 샘플에서 훈련됨
- 이 알고리즘은 `슬라이딩 윈도`를 사용해서 훈련 샘플을 사용함
- 윈도 크기가 2라면 중심 단어의 앞뒤로 이어질 2개의 단어를 고려함
- 임베딩은 분류 작업을 통해서 생성되는데 단어가 일반적으로 동일한 문맥에서 나타나는지 에측하도록 신경망을 훈련함
- 최종적으로 훈련된 모델은 이웃 관계를 감지해서 두 단어가 이웃이면 1을 출력할거라고 기대할 수 있음
- 이는 텍스트와 이미지 같은 데이터 유형을 연결하는데도 사용됨

<br>

### 데이터셋 보완하기
- 데이터셋에 타깃 값이 1만 있다면 모델이 속임수를 배워서 항상 1만 출력할 수 있음
- 그래서 `이웃이 아닌 단어로 만든 샘플(음성 샘플)`을 사용해서 훈련 데이터셋 보완이 필요함
- 텍스트로부터 샘플을 수백만, 수십억개도 만들 수 있음