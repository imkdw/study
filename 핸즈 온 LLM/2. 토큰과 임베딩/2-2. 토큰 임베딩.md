# 토큰 임베딩
- 언어는 토큰의 시퀀스임
- 좋은 모델을 충분히 큰 토큰 집합에서 훈련하면 훈련 데이터셋에 있는 복잡한 패턴을 포착하기 시작함
- 훈련 데이터에 영어 텍스트가 많다면 영어를 표현하고 생성할 수 있는 모델로 패턴이 드러남
- 훈련 데이터가 사실정보(위키백과 등)를 참조한다면 사실적 정보를 생성하는 능력을 갖추게 될것임

<br>

# 토크나이저의 어휘사전에 대한 임베딩을 내장한 언어모델
- 토크나이저가 초기화되고 훈련되고 나면 이를 사용해서 언어 모델을 훈련함
- 이는 사전에 훈련된 언어 모델이 해당 토크나이저와 연결되는 이유인데 재학습 없이는 토크나이저 교체가 불가능함
- 언어 모델은 토크나이저의 어휘사전에 있는 각 토큰에 대한 임베딩 벡터를 가지고 있음

<br>

# 언어 모델로 문맥을 고려한 단어 임베딩 만들기
- 토큰 임베딩을 생성하는 것은 언어 모델을 텍스트 표현에 사용하는 주요 방법의 하나임
- 언어 모델은 각 토큰이나 단어를 정적인 벡터로 나타내는 대신 문맥을 고려해서 토큰 임베딩으로 만듬
- 이 경우 단어는 문맥에 따른 다른 임베딩으로 표현되는데 이런 벡터는 다양한 작업에 사용이 가능함

<br>

### 문맥을 고려한 단어 임베딩 사용법
```python
from transformers import AutoModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base")
model = AutoModel.from_pretrained("microsoft/deberta-v3-xsmall")
tokens = tokenizer('Hello World', return_tensors='pt')
output = model(**tokens)[0]

"""
tensor([[[-3.3508, -0.0485, -0.0830,  ..., -0.1111, -0.2236,  0.5579],
         [ 0.5474,  0.3631,  0.0802,  ...,  0.0377,  1.0769,  0.2436],
         [-0.1720,  0.4741, -0.3350,  ..., -0.1897, -0.1222,  0.2062],
         [-3.2574, -0.1377,  0.0257,  ..., -0.0524, -0.2275,  0.4682]]],
       grad_fn=<NativeLayerNormBackward0>)
       
모델의 원시 출력
"""
print(output)

"""
torch.Size([1, 4, 384])

- 4개의 토큰이 존재한다
- 각 토큰은 384개 값을 가진 벡터로 임베딩 되어있다 
"""
print(output.shape)

"""
[CLS]
Hello
World
[SEP]
"""
for token in tokens['input_ids'][0]:
    print(tokenizer.decode(token))
```