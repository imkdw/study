# 토크나이저 언어가 모델의 입력을 준비하는 방법
- 겉으로 봤을땐 생성 LLM은 입력 프롬프트를 받아서 응답을 생성함
- 하지만 실제로는 프롬프트를 언어 모델에 전달하기 전에 토크나이저에 통과시켜서 더 작은 단위로 쪼개야함
- 모델이 텍스트를 처리하기 전에 토크나이저가 텍스트를 단어나 부분단어로 나는데 이 과정은 토큰화 방법과 훈련 방식에 따라서 다름

<br>

# LLM 다운로드 / 실행
- 아래 내용처럼 모델이 직접 텍스트 프롬프트를 입력받지 않고 토크나이저가 입력 프롬프트를 처리해서 모델에게 필요한 정보를 input_ids에 저장
- 해당 내용에는 일련의 정수가 들어있고 이는 특정 토큰에 대한 고유한 ID를 뜻함
- 이는 테이블에 대한 참조값이며 이 테이블에는 토크나이저가 인식할 수 있는 모든 토큰이 담겨져있음
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

prompt = "write an email apologizing to Sarah for the gragic gardening mishap. Exaplain how it happended.<|assistant|>"

input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")

"""
tensor([[ 2436,   385,  4876, 27746,  5281,   304, 19235,   363,   278,   330,
          1431,   293, 16423,   292,   286,   728,   481, 29889,  1222,   481,
          7420,   920,   372,  2250,  2760, 29889, 32001]], device='cuda:0')
"""
print(input_ids)

generation_output = model.generate(
    input_ids=input_ids,
    max_new_tokens=20
)

"""
write an email apologizing to Sarah for the gragic gardening mishap. Exaplain how it happended.<|assistant|> 

실제 생성한 토큰 : Subject: Sincere Apologies for the Gardening Mishap Dear Sarah
"""
print(tokenizer.decode(generation_output[0]))
```

<br>

### 토큰 분해하기
- 일부 토큰은 완전한 단어이며 일부 토큰은 부분단어임
- 구두점 또한 하나의 토큰임
```python
"""
write an email apolog izing to Sarah for the g rag ic garden ing m ish ap . Ex ap lain how it happ ended . <|assistant|> 
"""
for id in input_ids[0]:
  print(tokenizer.decode(id), end=" ")
```

<br>

# 토크나이저가 텍스트를 분할하는 방법
- 모델 설계시 모델 작성자가 토큰화 방법을 선택함
  - GPT에서 사용하는 BPE(byte pair encoding)
  - BERT에서 사용하는 WordPiece
- 토큰화 방법 선택 이후에 어휘사전 크기와 특수 토큰 같은 토크나이저 설계상의 여러가지 선택을 해야함
- 토크나이저는 특정 데이터셋에서 훈련하여 해당 데이터셋을 표현하는 최상의 어휘사전을 구축해야함
  - 예를 들면 영어 텍스트 데이터셋에서 훈련된 토크나이저는 코드, 다국어 등과는 다른 구조를 가짐
- 또한 토크나이저는 언어 모델을 위해서 입력 테스트 처리 외에도 언어 모델의 출력 결과에 생성된 토큰 ID를 해당하는 단어나 토큰으로 변환하기도함

<br>

# 단어 토큰, 부분단어 토큰 문자 토큰, 바이트 토큰
### 단어 토큰
- 초기 word2vec 같은 토큰화에 보편적으로 사용되던 방식이지만 NLP에선 점점 덜 사용됨
- NLP 외 추천 시스템과 같은 곳에서 사용함
- 토크나이저가 훈련된 후에 데이터셋에 새롭게 추가된 단어를 처리할 수 없다는 단점이 있음
- 그래서 차이가 크지 않은 비슷한 토큰을 어휘사전에 많이 가지고 있어야함
  - apology, apologize라고 한다면 공통접두사인 y, ize, etic 등으로 나눠서 해결은 가능함

<br>

### 부분단어 토큰
- 완전한 단어와 부분 단어를 포함하는 방식임
- 새로운 단어를 더 작은 단위로 나누기 때문에 새로운 단어도 표현할 수 있음

<br>

### 문자 토큰
- 대체할 원시 문자가 있기 때문에 새로운 단어를 잘 처리할 수 있는 또 다른 방법
- 토큰화 표현이 쉽지만 모델링은 어려운 방식
- 부분단어 토큰화 모델이 play를 하나의 토큰으로 표현하지만 문자 토큰을 사용하는 모델은 나머지 시퀀스를 모델링하는 것 외에도 `p-l-a-y`를 조합하는 정보까지 모델링해야함
- 트랜스포머 아키텍쳐 기반의 모델은 문맥 길이가 제한되어 있어서 부분단어 토큰이 문자 토큰에 비해 더 많은 텍스트를 처리할 수 있는 이점이 존재함

<br>

### 바이트 토큰
- 유니코드 문자를 표현하는 바이트로 토큰을 분할하는 또 다른 토큰화 방법
- 일부 부분단어 코트나이저는 어휘사전에 바이트를 토큰으로 포함하기도함
  - 이는 표현할 수 없는 문자를 만났을 때 대체할 마지막 수단이 됨
- 또한 바이트를 사용해서 모든 것을 표현하지 않고 일부분만 나타냄

<br>

# 훈련된 LLM 토크나이저 비교하기
- 새로운 토크나이저가 모델 성능을 개선하기 위해서 동작 방식을 계속 바꾸고 있음
- 또한 특수한 모델은 종종 특수한 토크나이저가 필요한 경우도 있음

<br>

### 토크나이저 방법 비교 코드
```python
from transformers import AutoTokenizer

text = """
English and CAPITALIZATION
😀 👽
show_tokens False None elif == >= else: two tabs:"      " four spavces: "    "
12.0*50=600
"""

colors_list = [
    "102;194;165", "252;141;98", "141;160;203",
    "231;138;195", "166;216;84", "255;217;47"
]


def show_tokens(sentence, tokenizer_name):
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    token_ids = tokenizer(sentence).input_ids
    for idx, t in enumerate(token_ids):
        print(
            f'\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +
            tokenizer.decode(t) +
            '\x1b[0m',
            end=' '
        )
```


<br>

# 토크나이저 속성
- 토크나이저는 모델마다 차별화된 속성을 가지고 있음
- 토크나이저의 행동은 `토큰화 방법`, `초기화 파라미터`, `토크나이저가 목표로 하는 데이터의 도메인`에 따라 달라짐

<br>

### 토큰화 방법
- 여러 토큰화 방법 중에서 BPE가 제일 인기가 높음
- 각 방법은 데이터셋을 표현하기 위해서 적절한 토큰 집할을 선택하는 알고리즘에 대한것임

<br>

### 토크나이저 파라미터
- 토큰화 방법을 선택했다면 LLM 설계자는 아래 파라미터를 선택해야함
- `어휘사전 크기` : 토크나이저 어휘사전에 얼마나 많은 토큰을 포함할 것인지
- `특수 토큰` : 모델이 추적해야 할 특수토큰을 결정, 예를 들어 Galactice에선 `<work>`, `[START_REF]`가 존재
- `대소문자` : 대소문자를 다루는 방법

<br>

### 데이터와 도메인
- 토크나이저의 동작은 훈련한 데이터셋에 따라서 달라짐
- 코드의 경우는 텍스트에 초점을 맞춘 토크나이저는 들여쓰기 공백을 모두 토큰화 할 수 있음