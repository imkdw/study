## 물리 복제
- PG는 물리 복제와 논리 복제 기능을 제공함
- 주요 업무 환경은 대부분 물리 복제를 활용해서 읽기 작업을 분산하게된다
- 물리 복제의 주요 목적은 읽기 부하를 분산해서 전체 시스템의 성능을 향상시키는 데 있으며 DR에서도 활용 가능하다
- 하지만 물리 복제만으로는 마스터 서버의 가용성 확보가 불가능하며 디스크 이중화나 SW 방식의 이중화 솔루션이 필요하다

<br>

## 물리 복제 구성하기
### 리플리카 서버 준비
- 마스터 서버와 동일한 스펙의 서버를 준비함
- 마스터 서버 - 리플리카 서버 간 방화벽을 오픈하고 동일한 버전의 PG를 설치한다

<br>

### 복제 전용 유저 생성
- 슈퍼유저(postgres)로 복제도 가능하지만 복제 전용 유저를 별도로 생성하는게 권장사항이다
- 복제 전용 유저에게는 `REPLICATION` 권한을 부여한다

```sql
-- 복제 전용 유저 생성
postgres@localhost:postgres> create user dbarep with password 'reppass' replication;

-- 복제 전용 유저 권한 확인
postgres@localhost:postgres> \du dbarep
+---------+----------+------------+---------------+-------------+-------------+--------------+---------------+----------+----------------+
| rolname | rolsuper | rolinherit | rolcreaterole | rolcreatedb | rolcanlogin | rolconnlimit | rolvaliduntil | memberof | rolreplication |
|---------+----------+------------+---------------+-------------+-------------+--------------+---------------+----------+----------------|
| dbarep  | False    | True       | False         | False       | True        | -1           | <null>        | []       | True           |
+---------+----------+------------+---------------+-------------+-------------+--------------+---------------+----------+----------------+
```

<br>

### 마스터 서버의 pg_hba.conf 파일 설정
- 물리 복제는 DB 클러스터 전체를 복제함. 그래서 `DATABASE` 항목에는 특정 디비가 아닌 replication을 입력함
- `USER`에는 위에서 생성한 복제용 유저를 입력하고, `ADDRESS`에는 리플리카 서버의 IP를 입력함

```bash
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    replication     dbarep          192.168.117.3/32        trust
```

<br>

### pg_basebackup을 이용한 복제
- 리플리카 서버에 postgres 유저로 ㅈ버속해서 pg_basebackup을 통해 디스크 복제를 수행함
- 복제를 수행하기 전에 DB 클러스터, WAL, 테이블스페이스용 디스크 볼륨의 소유자를 postgres 유저로 변경함

```bash
root@7f4ec8b342af:/# pg_basebackup -h 192.168.147.2 -p 5432 -D /data/svc01 -U dbarep -c fast -X stream -v -R -P --waldir=/pg_wal/svc01 -W
Password: 
pg_basebackup: initiating base backup, waiting for checkpoint to complete
pg_basebackup: checkpoint completed
pg_basebackup: write-ahead log start point: 0/2000028 on timeline 1
pg_basebackup: starting background WAL receiver
pg_basebackup: created temporary replication slot "pg_basebackup_80"
23232/23232 kB (100%), 1/1 tablespace                                         
pg_basebackup: write-ahead log end point: 0/2000120
pg_basebackup: waiting for background process to finish streaming ...
pg_basebackup: syncing data to disk ...
pg_basebackup: renaming backup_manifest.tmp to backup_manifest
pg_basebackup: base backup completed
root@7f4ec8b342af:/# 
```

<br>

### 인스턴스 시작
- 데이터 백업이 완료되었다면 리플리카 인스턴스를 재부팅함
- 이후에 `pg_is_in_recovery` 함수를 사용해서 리플리카로 동작중인지 확인이 가능함
```sql
postgres=# select pg_is_in_recovery();
 pg_is_in_recovery 
-------------------
 t
(1 row)
```

<br>

## 물리 복제 아키텍처
- WAL 파일(트랜잭션 변경사항을 담고있는)의 내용을 스트리밍 방식으로 리플리카 서버에 전송하는 복제 방식임

<br>

### 물리 복제 처리 흐름과 프로세스
- 물리 복제와 관련된 주요 프로세스는 총 3개가 존재함

<br>

#### walsender 프로세스
- WAL 데이터를 walreceiver 프로세스에게 전송함
- walsender 프로세스는 walreceiver 프로세스가 마스터 서버에 접속할 때 postgres 데몬 프로세스에 의해 생성됨
- 리플리카 서버당 walsender 프로세스는 1개씩 할당됨

<br>

#### walreceiver 프로세스
- walsender 프로세스로부터 전송받은 WAL 데이터를 리플리카 서버의 WAL 파일에 기록하는 역할

<br>

#### startup recovering
- 리플리카 서버의 WAL 파일을 읽어서 변경 내용을 공유 버퍼에 적용시킴

<br>

### PG 물리 복제의 우수성
- PG의 물리 복제는 매우 우수한 성능을 제공하는데 이는 복제 지연 시간이 매우 짧다는 이유임
- 트랜잭션 로그를 송/수신하는 프로세스가 별도로 존재하고며 커밋 이전에도 변경된 데이터를 리플리카로 전송함
- MySQL은 커밋된 데이터만 전송하지만 PG는 커밋 전에도 전송하므로 복제 지연이 최소화됨
- 하지만 아무리 좋은 아키텍처라도 디스크 I/O, 네트워크가 안좋으면 지연이 생길수밖에 없음
- WAL 파일이 저장되는 디스크는 성능이 우수한 SSD를 사용하고 마스터/리플리카간 네트워크 대역폭도 충분한 확보가 필요함

<br>

## 동기화 모드 유형
### 비동기 모드(기본값)
- 트랜잭션 성능 저하가 발생하지 않으나 복제 지연 현상이 발생할 수 있음
- 마스터 서버에서 변경된 내용이 리플리카 서버에 반영될 떄 까지 대기하지 않는다
- 이는 마스터 서버에 변경 사항이 반영되면 즉시 트랜잭션을 완료한다는 의미로 복제 환경에서도 트랜잭션 성능 저하가 발생하지 않음
- 하지만 복제 지연으로 인해서 리플리카 서버의 데이터는 일정 시간 뒤처질 수 있음
- 실시간 데이터 조회가 불가능할수도 있으므로 실시간 데이터 조회가 필요한 업무는 반드시 마스터 서버에서 수행해야함

<br>

### 동기 모드
- 가용성 향상을 복적으로 소프트웨어 이중화 방식을 적용할 떄 고려되는 구성 방식임
- 트랜잭션 성능은 저하되지만 failover 상황에서 트랜잭션이 유실되지는 않음
- 하지만 리플리카 서버가 다운되면 마스터 서버의 트랜잭션이 `대기(hang)` 상태에 빠지므로 최소 2개의 리플리카 서버가 필요함
- 마스터 서버에 장애가 발생하는 경우 리플리카 서버가 마스터로 승격하는 구조임
- 즉 트랜잭션 유실을 방지하려면 반드시 동기 모드로 구성해야함

<br>

## 동기 모드 복제 구성 절차
- 마스터 서버 - 리플리카#1 은 Sync(동기) 모드로 작동한다
- 마스터 서버 - 리플리카#2 는 Async(비동기) 모드로 작동하며 이 때 Virtual IP를 사용함
- VIP를 사용하면 마스터 서버 장애로 인해서 리플리카#1이 승격하더라도 자동으로 다시 마스터로 연결함
- 복제도 자동으로 재개되며 설정 변경이 필요하지 않음

<br>

### 마스터의 pg_hba.conf 설정
- 기존 물리복제 구성시 추가되었던 리플리카#1외에 리플리카#2의 IP 주소도 추가해줌
```bash
# TYPE  DATABASE        USER            ADDRESS                 METHOD

# 리플리카#1
host    replication     dbarep          192.168.147.3/32        trust

# 리플리카#2
host    replication     dbarep          192.168.147.2/32        trust
```

<br>

### 리플리카 서버의 application_name 등록
- 리플리카를 동기모드로 설정하기 위해서는 마스터 서버의 `synchronous_standby_names` 파라미터에 리플리카 서버 이름을 등록해야함
- `primary_conninfo` 파라미터나 `cluster_name` 파라미터를 사용해서 설정이 가능함
- `postgresql.auto.conf` 내부에 `primary_conninfo`로 설정하면 인스턴스 재시작이 필요하지 않으니 이걸로 설정하고 변경 사항만 반영하면됨

<br>

#### 리플리카 내부에 설정
```bash
# postgresql.auto.conf 파일을 수정해서 인스턴스 재시작 없이 적용하기
root@33bdc0e61f31:/data/svc01# cat postgresql.auto.conf 
primary_conninfo = 'user=dbarep password=reppass channel_binding=prefer host=192.168.147.4 port=5432 sslmode=prefer sslnegotiation=postgres sslcompression=0 sslcertmode=allow sslsni=1 ssl_min_protocol_version=TLSv1.2 gssencmode=prefer krbsrvname=postgres gssdelegation=0 target_session_attrs=any load_balance_hosts=disable application_name=replica1'

pg_ctl reload -D data/svc01
```

<br>

#### 마스터 서버에 설정
```bash
# postgresql.auto.conf 파일을 수정해서 인스턴스 재시작 없이 적용하기
postgres@e7e44a09c854:~$ cat $PGDATA/postgresql.auto.conf 
synchronous_standby_names = 'replica1'
```

```sql
-- replica1 서버가 동기(sync) 모드로 동작중인 모습
postgres=# select pid, application_name, sync_state from pg_stat_replication;
 pid | application_name | sync_state 
-----+------------------+------------
  84 | replica1         | sync
(1 row)
```

<br>

## 동기화 레벨
- DB에서 I/O 성능 향상을 위해서 공유 버퍼를 사용하는것 처럼 OS 레벨에서도 I/O 성능 향상을 위해서 OS 캐시를 사용함
- WAL 버퍼의 내용을 파일에 기록할 때는 직접 디스크에 쓰는게 아닌 OS 캐시를 거쳐서 디스크로 기록됨
- OS 캐시에 기록시 `write()` 시스템 콜, 실제 디스크에 기록할 대는 `fsync()` 시스템 콜이 사용됨
- 물리 복제에서 동기 모드로 설정하는 경우 `synchronous_commit` 파라미터를 통해서 레벨 지정이 가능함

<br>

### 레벨별 특징
- off : 마스터 서버의 OS 캐시까지 WAL이 기록되면 트랜잭션이 완료된 것으로 간주
- local : 마스터 서버의 디스크에 WAL이 기록되면 트랜잭션이 완료된 것으로 간주
- remote_write : 리플리카 서버의 OS 캐시까지 WAL이 기록되면 트랜잭션이 완료된 것으로 간주
- on(기본값) : 리플리카 서버의 디스크에 WAL이 기록되면 트랜잭션이 완료된 것으로 간주
- remote_apply : 리플리카 서버의 recovery 프로세스가 WAL 정보를 읽어서 공유 버퍼에 적용해야 트랜잭션이 완료된 것으로 간주

<br>

### 레벨 설정 팁
- off, local, remote_write의 경우 리플리카 서버의 디스크에 WAL 정보가 기록되는걸 보장하지 않음
  - 트랜잭션 유실 가능성이 존재함
- 트랜잭션 유실을 방지하려면 기본값인 on 이상으로 설정이 필요함
- 만약 리플리카 서버에서도 마스터 서버와 동일한 실시간 데이터 조회가 필요하다면 remote_apply 레벨로 설정해야함
- 동기화 레벨이 높을수록 리플리카 서버의 응답을 기다리는 시간이 길어지며 복제 지연 시간은 증가할 수 밖에 없음
- 만약 단순하게 트랜잭션 유실을 방지하는 목적이라면 기본값은 on 설정만으로 충분함

<br>

## Cascade 복제 구성 절차
- 마스터 서버가 아닌 리플리카 서버를 통해서 또 다른 리플리카 서버로 복제를 수행하는 방식임
- `마스터` -> `리플리카#1` -> `리플리카#2` 같은 구성이다
- 새로운 리플리카 서버 추가 시 마스터 서버에 부하가 없다는 장점이 있다
- 하지만 홉이 많아지므로 복제 지연 시간이 더 늘어날 수 있다는 단점도 존재한다
- DR 처럼 어느정도 복제 지연시간을 감수할 수 있는 환경이라면 고려해도 좋다