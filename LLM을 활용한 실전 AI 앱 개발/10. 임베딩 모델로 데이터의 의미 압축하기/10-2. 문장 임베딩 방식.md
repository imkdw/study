# 문장 임베딩 형식
- 워드투벡은 단어를 임베딩 벡터로 변환함으로써 단어와 단어 사이의 관계를 판단하는데 밀집 임베딩 활용이 가능하도록 길을 열었음
- 하지만 텍스트를 활용할때는 단어가 아닌 문장이나 문단 같은 더 큰 단위를 사용함
- 따라서 여러 단어가 합쳐진 문장을 임베딩 벡터로 변환하는 방법이 필요함

<br>

# 문장 사이의 관계를 계산하는 두 가지 방법
- 트랜스포머 인코더ㅏ 구조를 활용한 BERT 모델은 입력 문장을 임베딩으로 변환하는데 있어 뛰어난 성능을 보였음

<br>

### 교차 인코더
- 하나의 BERT 모델에 검색 쿼리 문장과 검색 대상 문장을 넣고 텍스트 사이의 유사도 점수를 계산함
- 두 텍스트 사이의 관계를 모두 계산하기 때문에 정확한 계산이 가능하다는 장점이 있음
- 하지만 다른 문장과 검색 쿼리의 유사도를 알고 싶으면 다시 동일한 연산을 반복해야한다는 단점이 있음
- 1000개의 문장을 저장하고 있을때 문장 A와 가장 유사한 문장을 찾고싶다면 1~1000까지 모두 계산해야함
- BERT 모델이 사용하는 어텐션 연산은 계산랸이 많은 무거운 연산이기 때문에 가급적 적게 수행하는게 좋음
- 또한 모든 문장 조합에 대해 유사도를 계산해야 가장 유사한 문장을 검색할 수 있어서 확장성이 떨어짐

<br>

### 바이 인코더
- 각 문장은 동일한 모델을 통과해서 각 문장에 대한 임베딩으로 변환됨
- 각 문장 임베딩을 코사인 유사도와 같은 유사도 계산 방식을 통해서 최종적인 유사도 저수를 산출함
- 각 문장의 독립적인 임베딩을 결과로 반환하기 때문에 유사도를 계산하고 싶은 문장이 바뀌더라도 추가적인 BERT 연산이 필요하지 않음
- 바이 인코더의 장점은 새로운 문장 B와 가까운 문장을 검색할 때 드러나는데 검색하는 문장에 대해서만 BERT 연산을 수행하기 때문에 계산량이 줄고 확장성이 높아짐

<br>

# 바이 인코더 모델 구조
- 총 3가지의 풀링 모드를 지원한다
```python
from sentence_transformers import SentenceTransformer, models

# BERT 모델
word_embedding_model = models.Transformer('klue/roberta-base')

# 풀링 층 차원 입력
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())

# 모듈 결합
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

"""
SentenceTransformer(
    (0): Transformer({
        'max_seq_length': 512, 
        'do_lower_case': False
    }) with Transformer model: RobertaModel,
    (1): Pooling({
        'word_embedding_dimension': 768,
        'pooling_mode_cls_token': False,
        'pooling_mode_mean_tokens': True,
        'pooling_mode_max_tokens': False, 
        'pooling_mode_mean_sqrt_len_tokens': False, 
        'pooling_mode_weightedmean_tokens': False, 
        'pooling_mode_lasttoken': False, 
        'include_prompt': True
    })
)
"""
print(model)
```

<br>

### 클래스 모드(pooling_mode_cls_token)
- BERT 모델의 첫 번째 토큰인 [CLS] 토큰의 출력 임베딩을 문장 임베딩으로 사용함

<br>

### 평균 모드(pooling_mode_mean_tokens)
- 문장 내 모든 토큰의 임베딩을 평균하여 문장 임베딩으로 사용함
- 일반적으로 많이 활용함

<br>

### 최대 모드(pooling_mode_max_tokens)
- 모든 입력 토큰의 출력 임베딩에서 문장 길이 방향에서 최댓값을 찾아 문장 임베딩으로 사용

<br>

# Sentence-Transformers로 텍스트, 이미지 임베딩 생성
### 텍스트 임베딩 생성
```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

# 임베딩 벡터 생성
embeddings = model.encode(['잠이 안옵니다', '졸음이 옵니다', '기차가 옵니다'])

# 유사도 계산
cos_scores = util.cos_sim(embeddings, embeddings)

"""
tensor([[1.0000, 0.6039, 0.1658],
        [0.6039, 1.0000, 0.2730],
        [0.1658, 0.2730, 1.0000]])
    
- 졸음, 잠 2가지 문장의 경우 0.6이라는 수치로 유사함
- 졸음, 기차 2가지 문장의 경우 0.1이라는 수치로 유사하지 않음
"""
print(cos_scores)
```

<br>

### 이미지 임베딩 생성
```python
from PIL import Image
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('clip-ViT-B-32')

img_embs = model.encode([Image.open('dog.jpg'), Image.open('cat.jpg')])
text_embs = model.encode(['A dog on grass', 'Brown cat on yellow background'])

"""
tensor([[0.2965, 0.1280],
        [0.1924, 0.2422]])
        
강아지 이미지와 강아지 텍스트는 0.29, 강아지 이미지와 고양이 이미지는 0.19 -> 강아지가 유사도가 더 높음
고양이 이미지와 강아지 텍스트는 0.12, 고양이 이미지와 고양이 텍스트는 0.24 -> 고양이가 유사도가 더 높음
"""
print(util.cos_sim(img_embs, text_embs))
```

<br>

# 오픈소스와 상업용 임베딩 모델 비교
### 상업용 임베딩 모델
- 상업용 모델은 대량의 데이터로 학습된 만큼 성능이 뛰어나다는 장점이 있음
- LLM 텍스트 생성에 비해서 훨씬 낮은 비용으로 사용이 가능함
- 일부 텍스트 생성 모델에 대해서는 파인튜닝이 가능하지만, 임베딩 모델에 대한 파인튜닝은 지원하지 않음

<br>

### 오픈소스 임베딩 모델
- 자신의 데이터에 맞춰 미세 조정을 수행할 수 있는 장점이 있음
