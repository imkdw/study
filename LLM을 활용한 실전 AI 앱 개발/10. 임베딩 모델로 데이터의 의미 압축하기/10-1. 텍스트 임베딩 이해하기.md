# 텍스트 임베딩 이해하기
- 여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식을 `텍스트 임베딩` 또는 `문장 임베딩` 이라고 부름
- 여기서 `임베딩`이란 데이터의 의미를 압축한 숫자 배열을 말함

<br>

# 문장 임베딩 방식의 장점
- 데이터의 의미를 숫자로 표현할 수 있다면 데이터가 서로 유사한지, 관련이 있는지 등 중요한 정보 활용이 가능하다
- 아래처럼 마치 사람이 이해하는 것처럼 서로 유사한지, 관련이 있는지 판단이 가능함
```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

smodel = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
dense_embeddings = smodel.encode(['학교', '공부', '운동'])

"""
[
    [0.9999999  0.5950744  0.32537562]
    [0.5950744  1.         0.5459569 ]
    [0.32537562 0.5459569  0.9999998 ]
]

학교, 공부는 0.595 만큼 유사함
공부, 운동은 0.325 만큼 유사함
"""
print(cosine_similarity(dense_embeddings))
```

<br>

# 원핫 인코딩
- 범주형 데이터 사이에 의도하지 않은 관계가 담기는걸 방지한다는 장점이 존재
- 하지만 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 치명적인 단점이 존재함
- 아래 예시에서는 학교, 공부 사이에는 `무언가 배운다`라는 공통점이 있다면 원핫 인코딩에선 그러한 의미를 전혀 살릴수없음

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

word_dict = {
    "school": np.array([[1, 0, 0]]),
    "study": np.array([[0, 1, 0]]),
    "workout": np.array([[0, 0, 1]])
}

# [[0.]]
cosine_school_study = cosine_similarity(word_dict["school"], word_dict["study"])

# [[0.]]
consine_school_workout = cosine_similarity(word_dict["workout"], word_dict["study"])
```

<br>

# 백오브워즈
- 비슷한 단어가 많이 나오면 비슷한 문장 또는 문서라는 가정을 활용해서 문서를 숫자로 변환함
- 단어의 순서에 관계없이 해당 문서에 등장한 단어와 그 등장 횟수를 집계함
- 백오브워즈는 아이디어가 직관적이고 구현이 간단함에도 훌룡하게 작동해서 문장과 문서의 의미를 표현하는 방버으로 오랫동안 사용했음
- 하지만 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는데 그게 도움이 되지 않는 경우가 존재함
  - 대부분의 한국 기사에는 은/는/이/가/을/를 같은 단어가 매우 많이 등장한다

<br>

# TF-IDF
- 어느 문서에나 나오는 단어 문제를 보완하기 위해서 수식을 활용해 많은 문서에 등장하는 단어의 중요도를 작게 만듬
- TF(w)에서 w는 특정 단어 w가 등장한 횟수를 나타낸다
- DF(w)에서 w는 특정 단어 w가 등장한 문서의 수를 의미한다
- TF-IDF(w) = TF(w) * log(N/DF(w))
  - log(N/DF(w))는 특정 단어가 여러 문서에 등장할수록 값이 작아지는데 N개의 문서에 모두 존재한다면 0이됨
- 백오브워즈의 문제점을 해결했으며 BM25같은 번형방식은 현재까지도 가장 보편적인 연관도 점수 계산 방식으로 사용됨

<br>

# 워드투벡
- 단어가 함께 등장하는 빈도 정보를 활용해서 단어의 의미를 압축하는 단어 임베딩 방법이다
- 주변 단어로 가운데 단어를 예측하는 방식(CBOW)과 중간 단어로 주변 단어를 예측하는 방식(스킵그램)으로 모델을 학습시킴



























