# 에이전트 평가하기

- 모든 AI 제품 개발에서 성능 평가는 가장 핵심적인 부분 중 하나다
- 두 가지 방법으로 분류되는데 `결과를 사람이 직접 평가`하는 방식과 `사람의 결과를 비교해서 에이전트가 생성한것과 사람이 생성한 것을 구분할 수 있는지` 확인하는 방식이 있다
- 두 에이전트가 낸 결과를 사람이 평가해서 각각 점수를 매기는 방법은 일반적으로 사람의 평가 방식으로 부른다

<br>

### 튜링테스트

- 에이전트와 사람의 결과물을 구분할 수 있는지 확인하는 평가 방식을 `튜링 테스트`라고 부른다
- 에이전트의 결과물을 사람의 결과물이라고 판단하거나 구분할 수 없다면 에이전트가 사람과 동등한 지능적 행동이 가능하다고 말할 수 있다

<br>

# 벤치마크 데이터셋을 활용한 객관적인 평가방식

- 아래 평가 기준중에서는 아직까지는 목표한 작업을 얼마나 성공적으로 수행하는지가 평가하는 유요성을 중심으로 연구가 활발히 진행되고있음

### 에이전트를 평가하는 4가지 기준

- 유용성 : 작업을 자동으로 수행하기 위해서 에이전트를 사용할 때 작업 성공률 같은 기준으로 평가
- 사회성 : 언어를 얼마나 숙련되게 사용하는지, 협력/협상이 얼마나 뛰어난지, 역할이 부여되면 얼마나 부합하게 행동하는지 등으로 평가
- 가치관 : 거짓말을 지어내지 않고 신뢰할 수 있는 정확한 정보를 전달하는지, 차별적인 편향이 있는지 등 평가
- 진화 능력: 위 3가지 조건을 만족하면 충분히 훌룡하지만 상황에 맞춰 스스로 발전이 가능하면 더 좋다
  - 즉 지속적인 학습, 스스로 목표를 설정하고 달성하는 학습능력 등으로 평가

<br>

### AgentBench

- 칭화대에서 만든 LLM 에이전트를 평가할 수 있는 데이터셋
- 하나의 작업으로 평가하는 것이 아닌 8개의 작업으로 평가하기 때문에 일반화 성능을 확인할 수 있다는 장점이 있음
- 현재까지의 오픈소스 및 상용 모델을 평가시에는 오픈소스 LLM의 일반화 능력이 상용 LLM에 미치지 못하고있다는 결과가 있음

<br>

### 높은 점수와 낮은 점수의 차이

- 에이전트가 실패하는 대표적인 경우는 대화가 여러 턴에 걸쳐서 진행되면서 추론 및 의사결정 능력이 떨어지는 경우, 요청한 출력 형식으로 응답하지 않은 경우를 꼽을 수 있음
- LLM 에이전트는 텍스트 입출력으로 작업을 진행하므로 출력이 원하는 형식으로 나오지 않는다면 실패하게됨
