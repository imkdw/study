# 기각 샘플링
- 지도 미세 조정을 마친 LLM을 통해서 여러 응답을 생성 후 그중에서 리워드 모델이 가장 높은 점수를 준 응답을 모아서 다시 지도 미세 조정을 수행함
- 강화 학습을 사용하지 않으므로 학습이 비교적 안정적이고 간단하고 직관적인 방법임에도 효과가 좋아서 많이 활용함

<br>

### 기각 샘플링 순서
- 대용량의 사전 학습 데이터를 사용해서 `자기 지도 학습`을 수행함. 이는 다음 단어를 예측하는 언어 모델링을 사용했다고 보면됨
- 사전 학습을 통해서 기본 모델을 만들었다면 사람의 피드백을 반영한 선호 데이터셋을 구축하고 마지막으로 RHLF를 통해 사람들이 선호하는 답변을 하도록 학습함
- 이처럼 메타는 바로 강화 학습을 사용하는 PPO를 통해서 모델을 학습하지 않고 먼저 기각 샘플링을 통해서 언어 모델이 더 빠르고 안정적으로 사람의 선호를 학습한 후 PPO를 사용함

<br>

# DPO(Direct Preference Optimization)
- RHLF는 선호 데이터셋으로 리워드 모델을 만들고 언어 모델의 출력을 평가하면서 강화 학습을 진행함
- 하지만 DPO는 선호 데이터셋으로 직접 언어 모델을 학습시킴
- DPO는 학습이 잘되고 있는지 확인하기 위해서 학습하지 않는 학습 전 모델인 참고 모델과 비교를 하게된다
- 선호 학습을 할 때는 일반적으로 참고 모델로 지도 미세 조정을 마친 모델을 사용한다
- 결국 DPO 학습을 할때는 별도로 리워드 모델이 필요하지 않고 강화 학습을 사용하지도 않게된다