# 선호 데이터셋
- 코드 A, B를 비교해서 더 가독성이 높은 코드를 `선호 데이터`, 선택하지 않은 코드를 `비선호 데이터`로 분류한다
- 이 떄 두 데이터 중 사람이 더 선호하는 데이터를 선택한 데이터셋을 `선호 데이터셋`이라고 부른다
- 이 때 특정 코드만 보고 가독성을 분석하고 선택하지 않는 이유는 코드에서 직접 점수를 매긴 데이터셋은 구축하기가 어렵기 때문이다
- 코드 자체에 대한 점수를 부여하는건 어렵지만 두 코드를 비교해서 가독성이 높은 코드를 선택하는 것은 비교적 쉽기 때문에 선호 데이터셋을 구축해 채점 모델을 만들게된다

<br>

### 코드 예시
- 이러한 코드를 통해서 채점 모델이 선호 데이터에 비선호 데이터보다 높은 점수를 주도록 채점 모델을 학습시킨다
- 하지만 LLM은 사용자에 요청에 맞게 주기때문에 차별적인 답변도 생성하는 등 문제가 있다
- GPT는 생성한 답변의 점수를 평가하는 리워드 모델을 만들었다
```python
# 1) 가독성이 낮은 예시
def sum_even(nums):
    even_sum = sum(n for n in nums if n % 2 == 0)
    return even_sum


# 2) 가독성이 높은 예시
def sum_of_even_numbers(numbers_list):
    """
    Calculate the sum of all even numbers in a given list

    Params:
    number_list (list): A list of integers

    Returns:
    int: The sum of all even numbers in the list
    """
    even_numbers = [number for number in numbers_list if number % 2 == 0]
    total_sum = sum(even_numbers)
    return total_sum
```

<br>

### 리워드 모델
- 지도 미세 조정을 마친 LLm에 지시사항을 입력해서 미리 여러개의 응답(A, B, C)을 만든다
- 레이블러가 직접 이러한 답변을 평가해서 나온 결과(선호 데이터셋)를 기반으로 다시 학습시킨다

<br>

# 높은 코드 가독성 점수를 향해
### 강화학습
- 강화학습에서는 에이전트가 환경에서 행동을 하게되는데 이에 따라서 상태가 변경되고 행동에 대한 보상이 발생한다
- 에이전트는 가능하면 더 많은 보상을 받을수 있게 수정하면서 학습하는데 이러한 과정을 시나리오라고 부른다

<br>

### RHLF(Reinforcement Learning from Human Feedback)
- 사람의 피드백을 활용한 강화 학습이라는 의미다
- 리워드 모델이 생성한 텍스트를 평가하고 점수를 매기는데 강화학습 관점에서는 토큰 생성을 하나의 행동으로 볼 수 있음
- 강화학습에서는 생성한 답변에 대해서 모두 보상이 주어지는데 RHLF에서는 전체 생성 결과에 대해서 리워드 모델의 점수를 받게됨
- 하지만 문제점은 보상만 높게 받기 위한 `보상 해킹`이 발생할 수 있는데 OpenAI는 이를 해결하기 위해서 PPO라는 강화 학습 방법을 사용함
- 대표적으로 짧은 코드를 작성해서 가독성이 높다고 판단하게끔 만들고 보상만 높게 받는 행동임

<br>

# PPO: 보상 해킹 피하기
- 보상 해킹은 평가 모델의 높은 점수를 받는 과정에서 다른 능력이 감소하거나 평가 점수만 높게 받을 수 있는 우회로를 만들 찾는 현상을 뜻함
- PPO(Proximal Preference Optimization)는 `근접 정책 최적화`라는 의미로 보상 해킹을 피하기 위해서 사용함
- 지도 미세 조정 모델을 기준으로 학습하는 모델이 너무 멀지 않게 가까운 범위에서 리워드 모델의 높은 점수를 찾도록 한다는 의미다

<br>

# RHLF 멋지지만 피할 수 있다면...
- OpenAI는 RHLF를 통해서 AI 서비스에서 자주 문제가 되는 편향성, 공격성 등 여러 문제를 효과적으로 제어했음
- 하지만 결과물 만큼이나 사용하기 어려운데 리워드 모델을 학습시키는 과정도 매우 힘들고 비용도 많이듬
- 최근에는 강화 학습이나 리워드 모델을 사 용하지 않고도 사람의 선호를 학습할 수 있는 기술이 개발되었음