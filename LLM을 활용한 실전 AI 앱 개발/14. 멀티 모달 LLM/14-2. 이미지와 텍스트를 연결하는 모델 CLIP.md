# CLIP(Contrastive Language-Image Pre-Training) 모델이란

- 텍스트와 이미지 데이터의 관계를 계산할 수 있도록 텍스트 모델과 이미지 모델을 함께 학습시킨 모델
- 내부적으로 텍스트 모델과 이미지 모델 2개의 모델로 구성됨

<br>

# CLIP 모델의 학습방법

- 서로 관련이 있는 이미지와 텍스트 쌍을 활용함
- 이미지 - 텍스트 쌍 데이터는 이미지와 이미지에 대한 설명이 대응된 데이터를 말함
- CLIP 모델 연구팀은 직접 인터넷상에서 50만 개의 검색어로 4억 개의 쌍 데이터를 구축함
- 이후에는 `대조 학습`을 통해서 모델을 학습시킨다
  - 대조학습은 문장 임베딩 모델의 학습에서도 사용됐는데 유사한 데이터 쌍은 더 가까워지게, 유사하지 않은 데이터 쌍은 더 멀어지도록 학습시키는 방법이다

<br>

# CLIP 모델의 활용과 뛰어난 성능

- 학습을 마친 CLIP 모델은 `제로샷 추론`을 수행할 수 있는데 이는 사전 학습 데이터 외 특정 작업을 위한 데이터로 미세 조정을 하지 않은 상태에서 추론을 수행하는걸 뜻함
- 먼저 레이블을 `A photo of {Object}` 형태로 변경하는 프롬프트 엔지니어링을 수행함
- 이후에 입력된 텍스트는 학습된 텍스트 인코더를 통해서 텍스트 임베딩으로 변환, 이미지는 이미지 임베딩으로 변환함
- 또한 유사도 계산을 활용해서 이미지 검색에서 활용이 가능하다

<br>

# 직접 활용하기

```python
from transformers import CLIPProcessor, CLIPModel
import requests
from PIL import Image

# 허깅페이스를 통한 CLIP 모델 활용
model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14')
processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# a photo of {object} 방식으로 2가지의 인풋을 정의
inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image

# 추론 진행
probs = logits_per_image.softmax(dim=-1)

"""
tensor([[9.9925e-01, 7.5487e-04]], grad_fn=<SoftmaxBackward0>)

a photo of a cat이 9.9925e-01로 예시 이미지는 고양이란걸 알 수 있음
"""
print(probs)
```
