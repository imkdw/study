# LLaVA

- 이미지를 인식하는 CLIP 모델과 LLM을 결합해서 모델이 이미지를 인식하고 그 이미지에 대한 텍스트 생성이 가능함
- 기존에 DALL-E 모델은 이미지와 텍스트를 함께 다룰 수 있는 멀티 모달이지만 이미지에 대해 글을 생성하거나 대화를 나눌수는 없었음

<br>

# LLaVA의 학습 데이터

- 멀티 모달 대화 모델을 만들때 가장 어려운 부분은 대부분의 딥러닝 학습과 마찬가지로 데이터셋이 부족하다는 점임
- LLaVA 논문의 저자들은 ChatGPT, GPT-4를 통해서 학습 데이터셋을 구축함
- GPT-4는 이미지를 인식할 수 없는데 이미지에 대한 설명과 위치 정보를 통해서 GPT-4가 이미지를 인식하도록 했음

<br>

### 학습 데이터의 구조

- GPT에는 이미지를 설명하는 `캡션`과 이미지에 어떤 물체가 있는지 물체와 위치 정보를 나타내는 `박스` 데이터를 전달함
- 사람이 이미지에 대한 질문을 하면 어시스터느가 이미지를 보고 답변하는 형식의 데이터인 `대화`
- 이미지 설명을 읽고 이미지에 대해 자세히 설명한 `자세한 설명`
- 위 2개의 유형이 단순한 이미지에 대한 인식과 설명이라면 답변을 위해 단계별 추론이 필요한 어려문 질문 생성 및 답변인 `복잡한 추론`

<br>

# LLaVA 모델 구조

- 입력 이미지를 CLIP의 이미지 인코더를 통해서 이미지 임베딩으로 만듬
- 간단한 선형 층을 통과해서 LLM에 입력할 임베딩 토큰으로 만듬
- 텍스트 지시사항은 토큰 임베딩으로 변환해서 함께 입력으로 넣고 결과를 생성함

<br>

# LLaVA 1.5

- 이미지 인코더를 CLIP의 ViT-L/14에서 ViT-L/336px로 변경함
- 한 층의 선형 층으로 이미지 임베딩 -> 토큰 임베딩으로 변환하던 구조를 2층의 MLP로 변경
  - 이 과정에서 성능을 대폭 끌어올림

<br>

# LLaVA NeXT

- LLaVA 1.5를 더 발전시킨 모델로 성능이 더욱 개선됨
- 오픈소스 모델 중 최고 성능을 달성했고 상업용 모델보다 더 좋음
