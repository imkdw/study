# 모델 활용하기
- 허깅페이스는 모델을 `바디`, `헤드` 2개로 나눠서 구분함
- 같은 바디를 사용하면서 다른 작업에 사용이 가능함
- 허깅페이스 트랜스포머 라이브러리에서는 모델의 바디만 불러올수도 있고 헤드 불러올수도 있음

<br>



### 모델 불러오기

```python
# AutoModel은 바디를 불러오는 클래스
from transformers import AutoModel

# model_id에 맞춰서 적절한 클래스를 가져옴
model_id = 'klue/roberta-base'
model = AutoModel.from_pretrained(model_id)
print(model)
```

<br>

### AutoModel 클래스가 특정 저장소의 모델을 불러오는 방법

- 허깅페이스에는 모델이 저장될 때 config.json 파일과 함께 저장됨
- 해당 설정파일에는 모델 종류(model_type), 설정 파라미터 등이 함께 저장됨
- AutoModel, AutoTokenizer 클래스느 config.json을 참고해서 적절한 모델과 토크나이저를 불러옴

```json
{
  "architectures": ["RobertaForMaskedLM"],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 32000,
  "tokenizer_class": "BertTokenizer"
}
```

<br>


### 분류 헤드가 포함된 모델 불러오기
- 아래 모델은 입력된 문장이 어떤 감성을 나타내는지 분석하는 모델임
- config.json을 보면 `id2label` 부분이 있는데 여기서 각 숫자는 감성을 나타냄

```python
from transformers import AutoModelForSequenceClassification

model_id = 'SamLowe/reberta-base-go_emotions'
classfication_model = AutoModelForSequenceClassification.from_pretrained(model_id)
```

```json
{
  "id2label": {
    "0": "admiration",
    "1": "amusement",
    "2": "anger",
    "3": "annoyance",
    "4": "approval",
    "5": "caring",
    "6": "confusion",
    "7": "curiosity",
    "8": "desire",
    "9": "disappointment",
    "10": "disapproval",
    "11": "disgust",
    "12": "embarrassment",
    "13": "excitement",
    "14": "fear",
    "15": "gratitude",
    "16": "grief",
    "17": "joy",
    "18": "love",
    "19": "nervousness",
    "20": "optimism",
    "21": "pride",
    "22": "realization",
    "23": "relief",
    "24": "remorse",
    "25": "sadness",
    "26": "surprise",
    "27": "neutral"
  }
}
```

<br>

### 아키텍처 모델 바디만 불러오기
```python
from transformers import AutoModelForSequenceClassification

model_id = 'SamLowe/reberta-base'
classfication_model = AutoModelForSequenceClassification.from_pretrained(model_id)
```

<br>

# 토크나이저 활용하기
```python
from transformers import AutoTokenizer

model_id = 'klue/roberta-base'
tokenizer=AutoTokenizer.from_pretrained(model_id)
tokenized = tokenizer("토크나이저는 텍스트를 토큰 단위로 쪼갬")

'''
{
  # 토큰 아이디의 리스트
  # 0은 [CLS]에 대응됨
  # 9157은 "토큰"에 대응됨
  'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 3, 2],

  # 토큰이 속한 문장의 아이디, 이는 문장을 구분하는 역할을함
  # 0이면 일반적으로 첫번째 문장임을 의미함
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],

  # 토큰이 실제 텍스트인지 아니면 길이를 맞추기 위해서 추가된 패딩인지
  # 1이면 패딩이 아닌 실제 토큰이라는 의미
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
'''
print(tokenized) 

'''
  ['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '[UNK]', '[SEP]']
'''
print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))

'''
  [CLS] 토크나이저는 텍스트를 토큰 단위로 [UNK] [SEP]
'''
print(tokenizer.decode(tokenized['input_ids']))

'''
  # decode를 하게되면 [CLS], [UNK] 같은 특수문자가 붙는데 skip_special_tokens=True를 설정하면 특수문자 제거가 가능함
  토크나이저는 텍스트를 토큰 단위로
'''
print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))
```

<br>

### 여러문장 넣기
```python
from transformers import AutoTokenizer


model_id = 'klue/roberta-base'
tokenizer=AutoTokenizer.from_pretrained(model_id)
tokenized = tokenizer(['첫번째문장', '두번째문장'])

'''
  {
    'input_ids': [[0, 1656, 2517, 3135, 2346, 2121, 2], [0, 864, 2517, 3135, 2346, 2121, 2]],
    'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]],
    'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]
  }
'''
print(tokenized)
```

<br>

### 하나의 데이터에 여러 문장 넣기
```python
from transformers import AutoTokenizer


model_id = 'klue/roberta-base'
tokenizer=AutoTokenizer.from_pretrained(model_id)
tokenized = tokenizer([['첫번째문장', '두번째문장']])

'''
  {
    'input_ids': [[0, 1656, 2517, 3135, 2346, 2121, 2, 864, 2517, 3135, 2346, 2121, 2]],
    'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
    'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
  }
'''
print(tokenized)
```

<br>

### 토큰 아이디 문자열로 복원
```python
from transformers import AutoTokenizer


model_id = 'klue/roberta-base'
tokenizer=AutoTokenizer.from_pretrained(model_id)

first_tokenized = tokenizer(['첫번째문장', '두번째문장'])['input_ids']
decoded_first_tokenized = tokenizer.batch_decode(first_tokenized)
# ['[CLS] 첫번째문장 [SEP]', '[CLS] 두번째문장 [SEP]']
print(decoded_first_tokenized)

second_tokenized = tokenizer([['첫번째문장', '두번째문장']])['input_ids']
decoded_second_tokenized = tokenizer.batch_decode(second_tokenized)
# ['[CLS] 첫번째문장 [SEP] 두번째문장 [SEP]']
print(decoded_second_tokenized)
```

<br>

### NSP(Next Sentence Prediction)
- BERT 모델의 경우 2개의 문장이 서로 이어지는지 맞추는 NSP 작업을 활용함
- 이를 위해서 문장을 구분하는 토큰 타입 아이디를 만듬
- roberta 계열의 모델은 학습 과정에서 NSP 작업을 제외시켜버려서 문장 토큰 구분이 필요하지 않음
- roberta-base 모델을 통해서 영어 문장을 토큰화하면 결과에 token_type_ids 자체가 없음

```python
from transformers import AutoTokenizer

korean_sentence = ['첫번째 문장', '두번째 문장']
english_sentence = ['first sentence''second_sentence']

'''
    {
        'input_ids': [[2, 1656, 2517, 3135, 6265, 3], [2, 864, 2517, 3135, 6265, 3]],
        'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]],
        'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]
    }
'''
bert_tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
print(bert_tokenizer(korean_sentence))

'''
    {
        'input_ids': [[0, 1656, 2517, 3135, 6265, 2], [0, 864, 2517, 3135, 6265, 2]],
        'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]],
        'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]
    }
'''
roberta_tokenizer = AutoTokenizer.from_pretrained("klue/roberta-base")
print(roberta_tokenizer(korean_sentence))

'''
    # roberta 계열의 모델로 영어 문장 토큰화시에는 token_type_ids가 없음
    {
        'input_ids': [[0, 9502, 11305, 3204, 2832, 1215, 19530, 4086, 2]],
        'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]
    }
'''
en_roberta_tokenizer = AutoTokenizer.from_pretrained("roberta-base")
print(en_roberta_tokenizer(english_sentence))
``` 

<br>

### 패딩을 나타내는 attention_mask 필드
- 패딩은 모델에 입력하는 토큰 아이디의 길이를 맞추기 위해서 추가하는 특수한 토큰


```python
from transformers import AutoTokenizer

korean_sentence = ['첫번째 문장입니다', '첫번째 문장보다 더 긴 두번째 문장입니다']

bert_tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")

'''
    {
        'input_ids': [[2, 1656, 2517, 3135, 6265, 12190, 3], [2, 1656, 2517, 3135, 6265, 2178, 2062, 831, 646, 864, 2517, 3135, 6265, 12190, 3]],
        'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
        'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
    }
'''
print(bert_tokenizer(korean_sentence))

'''
    {
        'input_ids': [[2, 1656, 2517, 3135, 6265, 12190, 3, 0, 0, 0, 0, 0, 0, 0, 0], [2, 1656, 2517, 3135, 6265, 2178, 2062, 831, 646, 864, 2517, 3135, 6265, 12190, 3]],
        'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
        'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
    }
'''
print(bert_tokenizer(korean_sentence, padding='longest'))
```

<br>

### 패딩 관련 부가설명
```
# 패딩 사용을 권장하는 경우 (일반적)

학습/추론 시 배치 처리: GPU를 효율적으로 활용하려면 패딩이 필수적입니다
프로덕션 환경: 안정적이고 예측 가능한 성능이 중요할 때
대용량 데이터셋: 병렬 처리의 속도 향상이 메모리 낭비보다 훨씬 큰 이익을 가져다줄 때

# 패딩 없이 사용하는 경우

단일 샘플 처리: 실시간 챗봇이나 API 서비스에서 한 번에 하나씩 처리할 때
메모리가 극도로 제한적인 환경: 임베디드 시스템이나 모바일 기기
시퀀스 길이 차이가 극심한 경우: 10토큰 vs 1000토큰처럼 차이가 클 때

실무에서는 보통 패딩을 사용하되, attention_mask로 패딩 부분을 무시하도록 하고, 배치 크기나 최대 길이를 적절히 조절해서 메모리 효율성을 챙기는 방식을 씁니다.
```

```
# 장점

배치 처리 효율성: 서로 다른 길이의 시퀀스들을 동일한 크기로 맞춰 한 번에 병렬 처리할 수 있어 GPU/TPU 활용도가 크게 향상됩니다.
메모리 관리 최적화: 고정된 텐서 크기로 인해 메모리 할당이 예측 가능하고 안정적이며, 동적 크기 변경으로 인한 오버헤드가 없습니다.
모델 학습 안정성: 배치 내 모든 샘플이 동일한 차원을 가져 gradient 계산과 backpropagation이 일관되게 수행됩니다.

# 단점

메모리 낭비: 짧은 시퀀스에 불필요한 패딩 토큰이 추가되어 실제 정보가 없는 부분까지 메모리를 차지하게 됩니다.
연산 비효율성: 패딩 토큰에 대해서도 attention 계산 등이 수행되어 실질적으로 의미 없는 연산이 증가합니다.
시퀀스 길이 제약: 가장 긴 시퀀스에 맞춰 패딩하거나 최대 길이를 설정해야 하므로 매우 긴 시퀀스 처리 시 정보 손실이나 과도한 메모리 사용이 발생할 수 있습니다.
```

<br>

# 데이터셋 활용하기
- datasets 라이브러리를 통해서 허깅페이스 데이터셋을 코드로 불러올 수 있음
```python
from datasets import load_dataset

'''
    DatasetDict({
        train: Dataset({
            features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],
            num_rows: 17554
        })
        validation: Dataset({
            features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],
            num_rows: 5841
        })
    })
'''
klue_mrc_dataset = load_dataset('klue', 'mrc')
print(klue_mrc_dataset)

'''
    split 인자를 통해서 특정 항목만 불러올수도 있음
    
    Dataset({
        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],
        num_rows: 17554
    })
'''
klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train')
print(klue_mrc_dataset_only_train)
```

<br>

### 로컬 데이터셋 활용하기
```python
from datasets import load_dataset
from datasets import Dataset
import pandas as pd

# 로컬 csv 데이터 활용
dataset = load_dataset("csv", data_files="my_file.csv")

# 파이썬 Dictionary 활용
my_dict = {"a": [1, 2, 3]}

# 판다스 데이터프레임 활용
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)
```