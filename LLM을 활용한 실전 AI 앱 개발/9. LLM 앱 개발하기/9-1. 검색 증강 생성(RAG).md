# 필요한 패키지 설치
```
pip install datasets llama-index==0.10.34 langchain-openai==0.1.6 "nemoguardrails[openai]==0.8.0" openai==1.25.1 chromadb==0.5.0 wandb==0.16.6 llama-index-callbacks-wandb==0.1.2 -qqq
```

<br>

# 검색 증강 생성(RAG)
- 검색 증강 생성이란 LLM에게 단순히 질문이나 요청만 전달하고 생성하는게 아닌 답변에 필요한 충분한 정보와 맥락을 제공하고 답변하도록 하는 방법

<br>

# 데이터 저장
- 데이터 소스는 텍스트, 이미지와 같은 비정형 데이터가 저장된 데이터 저장소를 의미함
- 데이터 소스의 텍스트를 임베딩 모델로 변환해서 임베딩 백터로 변환한다, 이는 벡터 데이터베이스에 저장된다
- 벡터 데이터베이스는 임베딩 벡터의 저장소이고 입력한 벡터와 유사한 벡터를 찾는 기능을 제공해준다
- 검색 쿼리를 벡터로 변환하고 디비에서 위치를 찾고 쿼리 임베딩과 제일 가까운 벡터를 찾는데, 이 떄 `유클리드 거리`나 `코사인 유사도`를 활용해서 거리를 계산하게된다

<br>

# 프롬프트에 검색 결과 통합
- LLM은 결과를 생성할 때 프롬프트만 입력으로 받는다
- 그래서 원하는 결과를 얻기 위해서는 사용자 요청과 관련된 큰 크기의 문서를 벡터 디비에서 찾고 검색 결과를 프롬트트에 통합해야한다
- 매번 질문과 관련된 정보를 프롬프트에 수동으로 찾아서 넣을수는 없으므로 프로그래밍 방식으로 관련된 정보를 찾아서 프롬프트에 넣어줄 수 있어야한다

<br>

# 라마인덱스로 RAG 구현
### 데이터셋 다운로드 & API 키 설정
```python
import os
from datasets import load_dataset

os.environ["OPENAI_API_KEY"] = "API_KEY"
dataset = load_dataset('klue', 'mrc', split='train')
print(dataset[0])
```

<br>

### 테스트용 데이터를 100개만 추출 및 벡터로 변환해서 저장
```python
import os
from datasets import load_dataset
from llama_index.core import Document, VectorStoreIndex

os.environ["OPENAI_API_KEY"] = ""

dataset = load_dataset('klue', 'mrc', split='train')
text_list = dataset[:100]['context']
documents = [Document(text=t) for t in text_list]

index = VectorStoreIndex.from_documents(documents)

print(index)
```

<br>

### 100개의 기사 본문 데이터에서 질문과 가까운 기사 찾기
```python
import os
from datasets import load_dataset
from llama_index.core import Document, VectorStoreIndex

os.environ["OPENAI_API_KEY"] = ""

dataset = load_dataset('klue', 'mrc', split='train')
text_list = dataset[:100]['context']
documents = [Document(text=t) for t in text_list]

index = VectorStoreIndex.from_documents(documents)

question = dataset[0]['question'] # 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?

"""
    similarity_top_k=5 : 가장 가까운 5개의 기사를 반환하도록 설정 
"""
retrieval_engine = index.as_retriever(similarity_top_k=5, verbose=True)
response = retrieval_engine.retrieve(question)

print(len(response)) # 4

"""
    올여름 장마가 17일 제주도에서 시작됐다.
    서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.
    17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다.
    more...
...
"""
print(response[0].node.text)
```

<br>

### 라마인덱스를 통해 RAG 수행
```python
import os
from datasets import load_dataset
from llama_index.core import Document, VectorStoreIndex

os.environ["OPENAI_API_KEY"] = ""

# 필요한 데이터셋 로딩
dataset = load_dataset('klue', 'mrc', split='train')

# 데이터를 100개만 사용하고 이를 도큐먼트로 변환
text_list = dataset[:100]['context']
documents = [Document(text=t) for t in text_list]

# 도큐먼트를 임베딩 벡터로 변환
index = VectorStoreIndex.from_documents(documents)

# 질문 : 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?
question = dataset[0]['question']

# 임베딩 벡터를 쿼리 엔진으로 변환  
query_engine = index.as_query_engine(similarity_top_k=1)

# 쿼리 엔진에 질문 전달
response = query_engine.query(question)

# 결과 : 장마전선에서 내리는 비를 뜻하는 장마는 북태평양 기단과 오호츠크해 기단이 만나 형성되며 국내에 한 달가량 머무르게 됩니다.
print(response)
```

<br>

### LLM 오케스트레이션의 단점
- 적은 코드로 유사한 텍스트를 검색하고 답변을 생성ㅎ는 모든 과정 수행이 가능함
- 모든 과정이 내부에서 알아서 처리해서 편하긴 하지만 모든 과정을 내부에서 알아서 처리하기 힘든 단점도 존재

<br>

### 라마인덱스 내부에서 발생하는 행동들
- `VectorIndexRetriever`을 통해 벡터 디비에서 검색하는 `retriever` 생성
- `get_response_synthesizer`를 데이터를 사용한 유저의 프롬프트 요청과 통합
- `RetrieverQueryEngine`에 `retriever`, `response_synthesizer`를 전달해서 `query_engine` 생성
- 추가로 `SimilarityPostprocessor`를 사용해서 질문과 유사도가 낮은 경우 필터링도 가능