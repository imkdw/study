# 데이터 로깅

- LLM 앱의 경우 입력이 동일해도 출력이 다를 수 있기때문에 반드시 어떤 입력에서 어떤 출력을 반환했는지 로깅이 필요함
- 로깅은 서비스 운영 및 고도화 등에서 사용이 가능함
- 해당 코드는 w&b를 통해서 데이터 로깅을 진행함

<br>

### W&B 로그인 처리

```python
import os
from dotenv import load_dotenv
import wandb

load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

wandb.login(key=os.getenv("WANDB_API_KEY"))
wandb.init(project="llm")
```

<br>

# OpenAI API 로깅

```python
import os
from dotenv import load_dotenv
from openai import OpenAI
import wandb
from wandb.sdk.data_types.trace_tree import Trace

load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

wandb.login(key=os.getenv("WANDB_API_KEY"))
wandb.init(project="llm")

client = OpenAI()
system_messages = "You are a helpful assistant"
query = "대한민국의 수도는 어디야?"
temperature = 0.2
model_name = "gpt-3.5-turbo"

response = client.chat.completions.create(model=model_name, messages=[
    {
        "role": "system",
        "content": system_messages
    },
    {
        "role": "user",
        "content": query
    }
], temperature=temperature)

root_span = Trace(
    name="root_span",
    kind="llm",
    status_code="success",
    status_message=None,
    metadata={
        "temperature": temperature,
        "token_usage": dict(response.usage),
        "model_name": model_name,
    },
    inputs={"system_prompt": system_messages, "query": query},
    outputs={"response": response.choices[0].message.content}
)

root_span.log(name="openai_trace")
```

<br>

# 라마인덱스 로깅

```python
import os
from dotenv import load_dotenv
import wandb
from datasets import load_dataset
import llama_index
from llama_index.core import  Document, VectorStoreIndex, ServiceContext
from llama_index.llms.openai import OpenAI
from llama_index.core import set_global_handler

load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

wandb.login(key=os.getenv("WANDB_API_KEY"))
wandb.init(project="llm")

llm = OpenAI(model='gpt-3.5-turbo', temperature=0)
set_global_handler("wandb", run_args={"project": "llm"})
wandb_callback = llama_index.core.global_handler
service_context = ServiceContext.from_defaults(llm=llm)

dataset = load_dataset('klue' ,'mrc', split='train')
text_list = dataset[:100]['context']
documents = [Document(text=t) for t in text_list]

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

question = dataset[0]['question']
query_engine = index.as_query_engine(similarity_top_k=1, verbose=True)
response = query_engine.query(question)

print(response)
```
