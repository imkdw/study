# 필요한 패키지
```
pip install transformers==4.40.1 datasets==2.19.0 accelerate==0.30.0 peft==0.10.0 bitsandbytes==0.42.0 -qqq
```

<br>

# GPU에 올라가는 데이터 살펴보기
- 딥러닝 모델을 학습시키고 추론하기 위해서 GPU를 사용할 떄 가장 많이 발생하는 문제가 OOM이다
- 기본적으로 GPU에는 딥러닝 모델 자체가 올라가게된다
- 딥러닝 모델은 수많은 행렬 곱셈을 위한 파라미터의 집합이며 각각의 파라미터는 소수 또는 정수 형식의 숫자다

<br>

# 딥러닝 모델의 데이터 타입
- 파라미터가 70억개인 LLM에는 행렬 연산에 사용되는 70억 개의 수가 저장되어 있다. 따라서 LLM 모델의 용량은 모델을 몇 비트의 데이터 형식을 사용하는지에 따라 달라진다
- 과거에는 32비트(4바이트) 부동소수점 형식을 사용해서 저장했지만 성능을 위해서 점점 더 파라미터가 많은 모델을 사용했고 하나의 GPU에 올리지 못하거나 연산이 느린 문제가 발생했다
- 최근에는 주로 16비트로 수를 표현하는 fp16, bf16을 주로 사용한다
- 딥러닝 모델은 GPU에 올라가기 때문에 모델의 용량이 얼마인지가 GPU 메모리 사용량을 체크할 때 중요하다
- 최근에는 LLM 모델이 커지면서 7B 모델처럼 10억을 의미하는 B 라는 단어를 사용한다
- 만약 파라미터가 70억개인 7B 모델이 16비트(2바이트)를 사용한다면 모델의 용량은 7 * 2 = 14GB가 된다

<br>

# 양자화로 모델 용량 줄이기
- 모델의 용량이 커지면서 더 적은 비트로 모델을 표현하는 `양자화` 기술이 개발됨
- 양자화 기술에서는 더 적은 비트를 사용하면서도 원본 데이터의 정보를 최대한 소실 없이 유지하는 것이 핵심 과제다
- 또 다른 방법은 `퀀타일`이라는 방법도 있는데 이는 별도로 메모리를 사용해야된다는 단점이 있다

<br>

# GPU 메모리 분해하기
- GPU 메모리에는 아래와 같은 항목이 저장된다
  - 모델 파라미터
  - 그레이디언트
  - 옵티마이저 상태
  - 순전파 상태
- `순전파를 수행`하고 그때 계산한 손실로부터 `역전파`를 수행하고 마지막으로 `옵티마이저`를 통해서 모델을 업데이트함

<br>

### GPU 메모리 사용량 계산
```python
import torch

def print_gpu_utilization():
    if torch.cuda.is_available():
        used_memory = torch.cuda.memory_allocated() / 1024 ** 3
        print(f"GPU Memory Usage: {used_memory:.3f} GB")
    else:
        print("Change to runtime GPU")


print_gpu_utilization()
```

<br>

### 모델을 불러온 뒤 GPU 메모리와 데이터 타입 확인
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def print_gpu_utilization():
    if torch.cuda.is_available():
        used_memory = torch.cuda.memory_allocated() / 1024**3
        print(f"GPU 메모리 사용량: {used_memory:.3f} GB")
    else:
        print("런타임 유형을 GPU로 변경하세요")

def load_model_and_tokenizer(model_id, peft=None):
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if peft is None:
        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype="auto", device_map={"":0})

    print_gpu_utilization()
    return model, tokenizer

model_id = "EleutherAI/polyglot-ko-1.3b"
model, tokenizer = load_model_and_tokenizer(model_id)
print("모델 파라미터 데이터 타입: ", model.dtype)
```
