# 문장 임베딩 형식
- 워드투벡은 단어를 임베딩 벡터로 변환함으로써 단어와 단어 사이의 관계를 판단하는데 밀집 임베딩 활용이 가능하도록 길을 열었음
- 하지만 텍스트를 활용할때는 단어가 아닌 문장이나 문단 같은 더 큰 단위를 사용함
- 따라서 여러 단어가 합쳐진 문장을 임베딩 벡터로 변환하는 방법이 필요함

<br>

# 문장 사이의 관계를 계산하는 두 가지 방법
- 트랜스포머 인코더ㅏ 구조를 활용한 BERT 모델은 입력 문장을 임베딩으로 변환하는데 있어 뛰어난 성능을 보였음

<br>

### 교차 인코더
- 하나의 BERT 모델에 검색 쿼리 문장과 검색 대상 문장을 넣고 텍스트 사이의 유사도 점수를 계산함
- 두 텍스트 사이의 관계를 모두 계산하기 때문에 정확한 계산이 가능하다는 장점이 있음
- 하지만 다른 문장과 검색 쿼리의 유사도를 알고 싶으면 다시 동일한 연산을 반복해야한다는 단점이 있음
- 1000개의 문장을 저장하고 있을때 문장 A와 가장 유사한 문장을 찾고싶다면 1~1000까지 모두 계산해야함
- BERT 모델이 사용하는 어텐션 연산은 계산랸이 많은 무거운 연산이기 때문에 가급적 적게 수행하는게 좋음
- 또한 모든 문장 조합에 대해 유사도를 계산해야 가장 유사한 문장을 검색할 수 있어서 확장성이 떨어짐

<br>

### 바이 인코더
- 각 문장은 동일한 모델을 통과해서 각 문장에 대한 임베딩으로 변환됨
- 각 문장 임베딩을 코사인 유사도와 같은 유사도 계산 방식을 통해서 최종적인 유사도 저수를 산출함
- 각 문장의 독립적인 임베딩을 결과로 반환하기 때문에 유사도를 계산하고 싶은 문장이 바뀌더라도 추가적인 BERT 연산이 필요하지 않음
- 바이 인코더의 장점은 새로운 문장 B와 가까운 문장을 검색할 때 드러나는데 검색하는 문장에 대해서만 BERT 연산을 수행하기 때문에 계산량이 줄고 확장성이 높아짐

<br>

# 바이 인코더 모델 구조
