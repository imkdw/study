# 벡터 디비의 작동 원리
- 벡터 디비는 벡터 사이의 거리를 게산해서 유사한 벡터를 찾음
- 가장 기본적인 방법은 KNn 검색으로 저장된 모든 임베딩 벡터를 조회해서 가장 유사한 K개의 벡터를 반환하는 방법임

<br>

# KNN(K-Nearest Neighbor) 검색과 그 한계
- KNN은 직관적이고 모든 데이터를 조사하기 때문에 정확하는 장점이 있음
- 하지만 데이터가 많아질수록 연산량이 데이터 수에 비례해서 늘어나서 속도가 느려지고 확장성이 떨어짐

<br>

### 벡터 검색을 위한 과정
- 먼저 인덱스(RDBMS의 테이블)에 벡터를 저장하는데, 이 과정을 `색인한다`고 표현함
- 색인 단계에서는 인덱스의 메모리 사용량과 색인 시간이 중요함
- 검색 과정에서는 검색 시간과 `재현율`이 중요함
  - 재현율 : 실제로 가장 가까운 K개의 정답 데이터 중 몇개가 검색 결과로 반환됐는지 그 비율을 나타낸 값

<br>

### 데이터 준비
```
wget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz
tar -xf sift.tar.gz
mkdir data/sift1M -p
mv sift/* data/sift1M
```

<br>

### 지표 확인
- SHIF1M의 경우 128차원 벡터를 사용함
- BERT는 768차원, text-embedding-ada-002는 1536차원 벡터를 사용함
- 벡터 차원이 커지면 검색 속도가 계손해서 느려진다
- 이러한 KNN의 정확도를 일부 희생하더라도 속도를 높이기 위해서 다양한 기술이 개발됬는데 이러한 기술을 통칭 `ANN`이라고 부른다
```python
import time
import faiss
from faiss.contrib.datasets import DatasetSIFT1M
import psutil


# 현재 프로세스가 사용하는 메모리 단위를 MB 단위로 반환
def get_memory_usage_mb():
    process = psutil.Process()
    memory_info = process.memory_info()
    return memory_info.rss / (1024 * 1024)


# SIFT1M 데이터셋 로딩
ds = DatasetSIFT1M()

# 검색에 사용할 데이터, 저장된 벡터 데이터, 실제 정답 데이터 로딩
xq = ds.get_queries()
xb = ds.get_database()
gt = ds.get_groundtruth()

k = 1
d = xq.shape[1]

# 검색에 수행할 데이터수를 1000개로 제한
nq = 1000
xq = xq[:nq]

for i in range(1, 10, 2):
    start_memory = get_memory_usage_mb()
    start_indexing = time.time()

    # 임베딩 벡터 색인
    index = faiss.IndexFlatL2(d)
    index.add(xb[:(i + 1) * 100000])
    end_indexing = time.time()
    end_memory = get_memory_usage_mb()

    t0 = time.time()

    # 검색 수행
    D, I = index.search(xq, k)
    t1 = time.time()
    print(f"데이터 {(i + 1) * 100000}개:")
    print(
        f"색인: {(end_indexing - start_indexing) * 1000 :.3f} ms ({end_memory - start_memory:.3f} MB) 검색: {(t1 - t0) * 1000 / nq :.3f} ms")

"""
데이터 200000개:
색인: 21.820 ms (98.609 MB) 검색: 0.151 ms

데이터 400000개:
색인: 30.994 ms (195.328 MB) 검색: 0.294 ms

데이터 600000개:
색인: 43.433 ms (97.656 MB) 검색: 0.439 ms

데이터 800000개:
색인: 75.189 ms (97.656 MB) 검색: 0.583 ms

데이터 1000000개:
색인: 100.101 ms (97.406 MB) 검색: 0.726 ms

-> 검색량에 따라서 거의 선형적으로 증가하고 있음
"""
```

<br>

# ANN(Approximate Nearest Neighbor) 검색이란
- 대용량 데이터셋에서 주어진 쿼리 항목과 가장 유사한 항목을 효율적으로 찾는데 사용하는 기술
- 약간의 정확도를 희생하는 대신 훨씬 더 빠른 검색 속도를 제공함
- 실제 응용 사례에서는 데이터셋 크기가 크고 고차원의 데이터를 다루는 경우가 많아서 ANN 검색을 이해하고 응용이 가능해야함
- ANN 검색에는 대표ㅕ적으로 사용하는 IVF, HNSW 등이 존재함

<br>

### IVF(Inverted File Index)
- 검색 공간을 제한하기 위해 데이터셋 벡터들을 클러스터로 그룹화하는 근사 최근적 이웃 검색 알고리즘임
- 인덱싱 시점에 데이터를 가져와서 중심점(클러스터)을 형성함
- 쿼리 시점에는 가장 가까운 클러스터를 찾고, 클러스터 내에서 가장 가까운 데이터셋 포인트를 찾음
- 전체 데이터셋 대신에 몇 개의 클러스터만 검색하는게 IVF의 전략으로 일부 이웃을 놓칠 수 있다는 비용을 감수하면서도 검색 시간을 크게 향상시킬 수 있음

<br>

### HNSW(Hierarchical Navigable Small World)
- 효울적인 ANN 검색을 위한 그래프 기반 인덱싱 구조임
- 상위에는 연결이 적고 하위에는 연결이 밀집된 다층 그래프를 구축함
- 상위에는 연결된 계층이 적인데 한번에 먼 거리 이동이 가능하므로 빠르게 그래프 탐색이 가능함
- 검색의 경우 최상위 -> 하위 계층으로 이동하는데, 하위에는 더 많은 연결이 추가되어서 더 꼼꼼한 탐색이 가능함
- 그래프를 활용하기 때문에 속도가 빠르면서 검색 성능이 뛰어나서 가장 많이 활용되는 알고리즘임

<br>

# 탐색 가능한 작은 세계(NSW)
- HNSW는 그래프 기반의 ANN 검색 알고리즘인데, 그래프는 노드, 간선 2가지 요소로 이루어진다
- 여기서 서로 연결된 노드끼리만 탐색이 가능하기 때문에 노드와 노드 사이에 얼마나 많은 간선이 있는지에 따라 성능이 달라진다
- NSW란 규칙적으로 연결된 그래프와, 랜덤 그래프 사이에 `적당히 랜덤하게 연결된 그래프`를 뜻한다

<br>

### 규칙적으로 연결된 그래프
- 모든 노드가 간선으로 연결되어 있어서 정확한 탐색이 가능함
- 하지만 촘촘하게 연결되어 있어서 탐색 과정에 많은 단게를 거쳐야하므로 검색 시간이 길어짐

<br>

### 랜덤 그래프
- 노드의 연결 상태에 따라서 빠르게 탐색할 수 있는 경우도 있음
- 하지만 서로 연결되지 않은 노드가 존재하면 탐색이 불가능하며, 되려 더 오래걸리는 상황도 존재함

<br>

### 적당히 랜덤하게 연결된 그래프
- 규칙적인 연결에서 일부 연결만 랜덤하게 바꾼 형태
- 규칙적인 연결을 통해서 정확한 탐색이 가능하면서도 랜덤한 성질을 통해서 빠른 탐색도 가능함

<br>

### 작은 세계에서 랜덤 연결을 만들기
- 벡터의 순서를 랜덤으로 섞어서 저장하는 방식을 사용함
- 하지만 랜덤으로 저장하다보니 진입점에서 출발했을 때 찾으려는 검색 벡터와 가장 가까운점이 아닌 이상한 노드에서 탐색을 멈추는 `지역 최솟값` 문제가 있음
- 이런 문제를 햇결하기 위해서 `계층 구조`가 추가되었음

<br>

# 계층 구조
- 계층 구조는 자료구조인 `연결 리스트` 방식으로 구성된다
- 이 때 데이터가 크기순으로 정렬되어있어야 연결 리스트의 탐색 속도를 높일 수 있다
- 레벨 0에는 모든 데이터가 있도록 하고 레벨 1, 2로 갈수록 데이터를 듬성듬성 배치한다
- HNSW는 이러한 게층 구조를 NSW에 접목해서 여러 계층에 그래프를 배치하는 방식으로 벡터를 저장한다
- 확률을 통해서 높은 층으로 갈수록 데이터가 적어지는 구조를 만드는데 이러면 높은 층에 있는 벡터는 NSW에서 일부 랜덤 연결이 탐색 단계를 줄인 것처럼 탐색 단계를 줄여줌
