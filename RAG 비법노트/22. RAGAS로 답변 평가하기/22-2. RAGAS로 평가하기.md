# RAGAS로 평가하기
- ragas.metric은 평가 지표로 주로 4가지로 이루어짐
  - faithfulness : 사실적 일관성을 가지는지 측정, 환각이 있다면 이 점수가 차감됨
  - answer_relevancy : 답변이 주어진 프롬프트에 얼마나 관련이 있는지 평가
  - context_recall : 전체 문서에 답변이 필요한 정보를 얼마나 잘 가져왔는지 평가
  - context_precision : 검색된 문서가 질문과 얼마나 관련이 깊은지 평가

<br>

# 예제
```python
import pandas as pd
from datasets import Dataset
import ast
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    faithfulness,
    context_recall,
    context_precision,
)


def convert_to_list(example):
    contexts = ast.literal_eval(example["contexts"])
    return {"contexts": contexts}


df = pd.read_csv(
    "/Users/imkdw/study/RAG 비법노트/22. RAG 평가와 개선/ragas_synthetic_dataset.csv"
)

test_dataset = Dataset.from_pandas(df)
test_dataset = test_dataset.map(convert_to_list)


loader = PyMuPDFLoader(
    "/Users/imkdw/study/RAG 비법노트/22. RAG 평가와 개선/SPRI_AI_Brief_2023년12월호_F.pdf"
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
split_documents = text_splitter.split_documents(docs)

embeddings = OpenAIEmbeddings()

vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
retriever = vectorstore.as_retriever()

prompt = PromptTemplate.from_template(
    """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 

#Context: 
{context}

#Question:
{question}

#Answer:"""
)

llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

batch_dataset = [question for question in test_dataset["question"]]

answer = chain.batch(batch_dataset)

if "answer" in test_dataset.column_names:
    test_dataset = test_dataset.remove_columns(["answer"]).add_column("answer", answer)
else:
    test_dataset = test_dataset.add_column("answer", answer)

result = evaluate(
    dataset=test_dataset,
    metrics=[
        context_precision,
        faithfulness,
        answer_relevancy,
        context_recall,
    ],
)

# {
#     "context_precision": 0.6111,
#     "faithfulness": 0.6262,
#     "answer_relevancy": 0.9569,
#     "context_recall": 0.7407,
# }
print(result)
```