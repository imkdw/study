# 문자 단위로 분할하기
- CharacterTextSplitter는 기본적으로 `\n\n`을 기준으로 글자 단위로 텍스트를 분할함
- 텍스트에서 엔터키를 두 번 눌러서 문단 구분이 가능한데 이것을 기준으로 분할한다는 뜻임

```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    # 텍스트 분할시 분할 기준이 되는 문자 지정
    separator="\n\n",
    # 분할되는 최대 문자수 지정
    chunk_size=210,
    # 분할된 텍스트 간 중복되는 문자 수
    chunk_overlap=0,
    # 텍스트의 길이를 계산하는 함수를 지정
    length_function=len,
)

texts = text_splitter.create_documents([file])

# 분할된 문서의 개수, 197
print(len(texts[0].page_content))

# page_content='Semantic Search

# 분할된 문서 중 첫번째 문서
# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
# 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
# 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝
print(texts[0])
```

<br>

# 메타데이터 전달하기
```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    # 텍스트 분할시 분할 기준이 되는 문자 지정
    separator="\n\n",
    # 분할되는 최대 문자수 지정
    chunk_size=210,
    # 분할된 텍스트 간 중복되는 문자 수
    chunk_overlap=0,
    # 텍스트의 길이를 계산하는 함수를 지정
    length_function=len,
)

metadatas = [
    {"document": 1},
    {"document": 2},
]

documents = text_splitter.create_documents(
    [
        file,
        file,
    ],
    metadatas=metadatas,
)

# page_content='Semantic Search

# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
# 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
# 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

# Embedding' metadata={'document': 1}
print(documents[0])
```