# 토큰 단위로 분할하기
- `TokenSplitter`는 텍스트를 글자 수가 아닌 토큰 수를 기반으로 청크를 생성함
- 언어 모델에는 토큰 제한이 있는데 텍스트를 효율적으로 분할하기 위해서 토크나이저를 사용함
- 토크나이저는 텍스트를 토큰으로 변환하는데 사용하는 알고리즘임
- 토크나이저에 따라서 토큰 개수를 계산하는 방식이나 청크 분할 결과가 달라져서 모델에 입력하기 전에 적절한 토크나이저를 선택하는게 중요함

<br>

# Tictoken
- OpenAI에서 만든 빠른 BPE 토크나이저
```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    # 청크 크기를 300으로 설정
    chunk_size=300,
    # 청크 간 중복이 없도록 설정
    chunk_overlap=0,
)

texts = text_splitter.split_text(file)

# Semantic Search

# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
# 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
# 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

# Embedding

# 정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
# 예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
# 연관키워드: 자연어 처리, 벡터화, 딥러닝

# Token

# 정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
# 예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다.
# 연관키워드: 토큰화, 자연어 처리, 구문 분석
for text in texts:
    print(text, end="\n\n")
```

<br>

# TokenTextSplitter
- 텍스트를 토큰 단위로 분할이 가능함
- 토크나이저를 직접 사용해서 텍스트를 바로 토큰 단위로 분할함
```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

from langchain_text_splitters import TokenTextSplitter

text_splitter = TokenTextSplitter(
    chunk_size=300,
    chunk_overlap=0,
)

texts = text_splitter.split_text(file)

# Semantic Search

# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
# 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
# 연�
print(texts[0])
```

<br>

# spaCy
- spaCy는 고급 자연어 처리를 위한 오픈소스 라이브러리로 NLP 분야에서 널리 사용됨
- 내부적으로 spaCy 토크나이저를 사용하고 텍스트 분할 결과는 글자 수를 기준으로 크기를 측정함
- SpaCyTextSplitter는 텍스트 분할시 spaCy 토크나이저를 활용해서 단어와 문장을 이해함
```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

import warnings
from langchain_text_splitters import SpacyTextSplitter

warnings.filterwarnings("ignore")

text_splitter = SpacyTextSplitter(
    chunk_size=200,
    chunk_overlap=50,
)

texts = text_splitter.split_text(file)

# Semantic Search

# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된

# 결과를 반환하는 검색 방식입니다.


# 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
print(texts[0])
```

<br>

# SentenceTransformers
- SentenceTransformers 모델의 토큰 윈도우에 맞게 텍스트를 청크로 분할함
- 시작토큰과 종료토큰의 개수를 각각 한 개씩 포함해서 총 두 개로 설정함
- 총 토큰 개수를 계산할때 위 2개의 토큰은 제외하고 계산됨

```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

from langchain_text_splitters import SentenceTransformersTokenTextSplitter

splitter = SentenceTransformersTokenTextSplitter(chunk_size=200, chunk_overlap=0)

count_start_and_stop_tokens = 2

text_token_count = splitter.count_tokens(text=file) - count_start_and_stop_tokens

# 7686
print(text_token_count)

text_chunks = splitter.split_text(text=file)

# . 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 [UNK] 합니다. [UNK] : " 사과 " 라는 단어를 [ 0. 65, - 0. 23, 0. 17 ] 과 [UNK] 벡터로 표현합니다. 연관키워드 : 자연어 처리, 벡터화, 딥러닝 token 정의 : 토큰은 텍스트를 더 작은 [UNK] 분할하는 [UNK] 의미합니다. 이는 일반적으로 단어, 문장, [UNK] 구절일 수 [UNK]. [UNK] : 문장 " 나는 학교에 간다 " 를 " 나는 ", " 학교에 ", " 간다 " 로 분할합니다. 연관키워드 : 토큰화, 자연어 처리, 구문 분석 tokenizer 정의 : 토크
print(text_chunks[1])
```

<br>

# NLTK(Natural Language Toolkit)
- 파이썬으로 작성된 영어 자연어 처리를 위한 라이브러리와 프로그램 모음으로 다양한 NLP 작업이 가능함
- 내부적으로 NLTK 토크나이저를 사용하고 청크 크기는 글자 수를 기준으로함

```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

import nltk

nltk.download("punkt")
nltk.download("punkt_tab")

import chunk
from langchain_text_splitters import KonlpyTextSplitter

text_splitter = KonlpyTextSplitter(chunk_size=200, chunk_overlap=50)

texts = text_splitter.split_text(file)

with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

import nltk

nltk.download("punkt")
nltk.download("punkt_tab")

import chunk
from langchain_text_splitters import KonlpyTextSplitter

text_splitter = KonlpyTextSplitter(chunk_size=200, chunk_overlap=50)

texts = text_splitter.split_text(file)

# Semantic Search 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.

# 예시: 사용자가 " 태양계 행성" 이라고 검색하면, " 목성", " 화 성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
print(texts[0])
```

<br>

# Hugging Face 토크나이저
- 다양한 토크나이저를 제공하고 GPT2TokenizerFast를 통해서 텍스트의 토큰ㅇ 길이 계산이 가능함
- 분할은 문자열을 기준으로 이루어지고 청크는 허깅페이스 토크나이저를 통해 게산된 토큰 수를 기준으로 측정함

```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

from langchain_text_splitters import CharacterTextSplitter
from transformers import GPT2TokenizerFast

hf_tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(
    hf_tokenizer,
    chunk_size=300,
    chunk_overlap=50,
)

texts = text_splitter.split_text(file)

# Semantic Search

# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
# 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
# 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

# Embedding

# 정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
# 예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
# 연관키워드: 자연어 처리, 벡터화, 딥러닝

# Token

# 정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
# 예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다.
# 연관키워드: 토큰화, 자연어 처리, 구문 분석
for text in texts:
    print(text, end="\n\n")
```