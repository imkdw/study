# SemanticChunker
- 일반적으로 텍스트 분할시 글자나 토큰 수를 기준으로 나누는 방법을 많이 사용하지만 `SemanticChunker`는 의미적으로 유사한 문장끼리 묶음
- 실제 문서에서 문단의 길이는 상황에 따라 달라지므로 이러한 접근 방식이 더 상식에 가까울 수 있음

```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

# OpenAI 임베딩 모델을 활용함
# 의미를 기준으로 분할하므로 청크 크기나 오버랩을 지정하지 않음
text_splitter = SemanticChunker(OpenAIEmbeddings())

chunks = text_splitter.split_text(file)

docs = text_splitter.create_documents([file])
```

<br>

# SemanticChunker의 문장 분할 기준점 설정
- SemanticChunker는 의미가 유사한 문장들을 묶는 역할을 함
- 문장 간 유사도를 계산하고 각 문장 쌍 사이의 거리로 나타내고 계산된 거리 값들을 그래프 형태로 표현해서 거리가 가까운 것, 먼 것을 표현함
- 문장 간 거리를 기준으로 텍스트를 나누는 지점인 `분할 기준점`이 지정되는데 이 임계값을 넘는 지점에서 문장이 분리되어 하나의 청크를 이루게됨

<br>

### 백분위수 분리
- 문장 간 모든 차이를 계산하고 지정한 백분위수를 기준으로 분리
- 아래 예제는 70 + percentile로 설정하는데 이는 70% 이상의 유사도를 가진 문장 간에는 청크를 나누지 않고 함께 묶는다는 의미

```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

text_splitter = SemanticChunker(
    OpenAIEmbeddings(),
    # 분할 기준점 유형을 백분위수로 설정합니다.
    breakpoint_threshold_type="percentile",
    # 70% 로 설정
    breakpoint_threshold_amount=70,
)

docs = text_splitter.create_documents([file])

# Semantic Search

# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

# Embedding

# 정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
# ============================================================
# [Chunk 1]

# 예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝

# Token

# 정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
# ============================================================
# [Chunk 2]

# 예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

# Tokenizer

# 정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
# ============================================================
for i, doc in enumerate(docs[:5]):
    print(f"[Chunk {i}]", end="\n\n")
    print(doc.page_content)
    print("===" * 20)
```

<br>

### 표준편가 기준 나누기
- 표준편차란 값들이 평균으로부터 얼마나 떨어져 있는지를 측정하는 값을 말함
- 유사도 값들의 표준편차를 활용해서 문장 간 거리가 평균에서 유의미하게 크게 벗어난 지점을 기준으로 텍스트를 분리함

```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

text_splitter = SemanticChunker(
    OpenAIEmbeddings(),
    # 분할 기준으로 표준 편차를 사용합니다.
    breakpoint_threshold_type="standard_deviation",
    # 문장 간 거리가 평균에서 1.25보다 큰 지점을 청크의 분할 기준점으로 잡음
    breakpoint_threshold_amount=1.25,
)

docs = text_splitter.create_documents([file])


# Semantic Search

# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

# Embedding

# 정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝

# Token

# 정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

# Tokenizer

# 정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: "I love programming."이라는 문장을 ["I", "love", "programming", "."]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

# VectorStore

# 정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
# ============================================================
# [Chunk 1]

# 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화

# SQL

# 정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
# ============================================================
for i, doc in enumerate(docs[:5]):
    print(f"[Chunk {i}]", end="\n\n")
    print(doc.page_content)
    print("===" * 20)
```

<br>

### 사분위수 범위를 사용해서 분할
- 데이터 분포에서 가운데 50%의 범위를 나타냄

```python
with open("/Users/imkdw/study/RAG 비법노트/example/appendix-keywords.txt") as f:
    file = f.read()

from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

text_splitter = SemanticChunker(
    OpenAIEmbeddings(),
    # 분할 기준점 임계값 유형을 사분위수 범위로 설정합니다.
    breakpoint_threshold_type="interquartile",
    # 문장의 유사도가 사분위수 범위의 0.5배 범위를 벗어나면 새로운 청크를 생성
    breakpoint_threshold_amount=0.5,
)

docs = text_splitter.create_documents([file])

# [Chunk 0]

# Semantic Search

# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

# Embedding

# 정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
# ============================================================
# [Chunk 1]

# 예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝

# Token

# 정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

# Tokenizer

# 정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
# ============================================================
for i, doc in enumerate(docs[:5]):
    print(f"[Chunk {i}]", end="\n\n")
    print(doc.page_content)
    print("===" * 20)
```
