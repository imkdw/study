# 대화 지식 그래프 메모리
- `ConversionKGMemory` 는 `지식 그래프(KG)`를 활용하여 정보를 저장하고 불러옴
- 이는 객체 간의 연결고리를 파악해서 저장함

```python
from dotenv import load_dotenv
from langchain_teddynote import logging

load_dotenv()
logging.langsmith("langchain_test")

# ========================================================================

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationKGMemory
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import ConversationChain

llm = ChatOpenAI(temperature=0)

memory = ConversationKGMemory(llm=llm, return_messages=True)
memory.save_context(
    {"input": "이쪽은 노바에 속한 엔버씨 입니다"},
    {"output": "엔버는 누구시죠?"},
)
memory.save_context(
    {"input": "엔버씨는 검은마법사 파티에 참여한 신규 파티원입니다"},
    {"output": "만나서 반갑습니다."},
)

# {
#     "history": [
#         SystemMessage(
#             content="On 엔버씨: 엔버씨 참여한 검은마법사 파티. 엔버씨 신규 파티원.",
#             additional_kwargs={},
#             response_metadata={},
#         )
#     ]
# }
print(memory.load_memory_variables({"input": "엔버씨는 누구시죠?"}))


template = """The following is a friendly conversation between a human and an AI. 
The AI is talkative and provides lots of specific details from its context. 
If the AI does not know the answer to a question, it truthfully says it does not know. 
The AI ONLY uses information contained in the "Relevant Information" section and does not hallucinate.

Relevant Information:

{history}

Conversation:
Human: {input}
AI:"""
prompt = PromptTemplate(input_variables=["history", "input"], template=template)

conversation_with_kg = ConversationChain(
    llm=llm, prompt=prompt, memory=ConversationKGMemory(llm=llm)
)

conversation_with_kg.predict(
    input="My name is Dongwoo. Anber is a coworker of mine, and she's a new person of our black magition raid party."
)

# {
#     "history": "On Anber: Anber is a coworker of Dongwoo. Anber is a new person of black magician raid party."
# }
print(conversation_with_kg.memory.load_memory_variables({"input": "who is Anber?"}))
```