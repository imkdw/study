# 대표적인 매개변수
### temperature
- 얼마나 창의적으로 답변할지 지저아는 값
- 0 ~ 2사이의 값을 지정하고 높을수록 출력이 더 무작위해짐
- 문서 기반과 같이 정확한 답변을 요구할 떄는 값을 0이나 0에 가깝게 설정하는게 좋음

<br>

### max_tokens
- 최대 토큰 수
- 지정하지 않으면 모델에서 허용된 최대 토큰 수까지 허용됨
- 최대 생성할 토큰의 개수를 제한할 때 사용함

<br>

### model_name
- 적용 가능한 모델을 선택해서 사용함
- gpt3.5-turbo, gpt-4 등 사용함

<br>

# Langsmith 설정
```python
from langchain_teddynote import logging

logging.langsmith("langchain_test")
```

<br>

# invoke() 함수로 응답 출력하기
```python
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

# 객체 생성
llm = ChatOpenAI(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    model_name="gpt-4.1-nano",  # 모델명
).bind(logprobs=True)

# 질의내용
question = "대한민국의 수도는 어디인가요?"

# 답변
response = llm.invoke(question)
# 질의
# {
#   "content": "대한민국의 수도는 서울입니다.",
#   "response_metadata": {
#     "token_usage": {
#       "completion_tokens": 8, // 답변에 사용된 토큰 수
#       "prompt_tokens": 16, // 프롬프트 입력 토큰 수
#       "total_tokens": 24, // 총 사용한 토큰 수
#     }
#   }
# }


# 답변만
# 대한민국의 수도는 서울입니다.
print(response.content)

# 메타데이터만
# {
#   "completion_tokens": 8,
#   "prompt_tokens": 16, 
#   "total_tokens": 24,
#   "completion_tokens_details": {
#     "accepted_prediction_tokens": 0,
#     "audio_tokens": 0,
#     "reasoning_tokens": 0,
#     "rejected_prediction_tokens": 0
#   },
#   "prompt_tokens_details": {
#     "audio_tokens": 0,
#     "cached_tokens": 0
#   }
# }
print(response.response_metadata["token_usage"])
```

<br>

# logprob 활성화 하기
- 로그 프로버빌리티란 주어진 GPT 모델의 토큰 확률 로그값, 모델이 토큰을 예측할 확률을 나타냄
- 0에 가까울수록 확률이 높고, 낮을수록 음수로 나타남
```python
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

# 객체 생성
llm = ChatOpenAI(
    temperature=0.1,
    model_name="gpt-4.1-nano",
).bind(logprobs=True)

# 질의내용
question = "대한민국의 수도는 어디인가요?"

# 답변
response = llm.invoke(question)

# {
#   "token_usage": {
#     "completion_tokens": 8,
#     "prompt_tokens": 16,
#     "total_tokens": 24,
#     "completion_tokens_details": {
#       "accepted_prediction_tokens": 0,
#       "audio_tokens": 0,
#       "reasoning_tokens": 0,
#       "rejected_prediction_tokens": 0
#     },
#     "prompt_tokens_details": {
#       "audio_tokens": 0,
#       "cached_tokens": 0
#     }
#   },
#   "model_name": "gpt-4.1-nano-2025-04-14",
#   "system_fingerprint": "fp_38343a2f8f",
#   "id": "chatcmpl-Bz09CCB1RzDqxoUYqwbEEcoE84zDp",
#   "finish_reason": "stop",
#   "logprobs": {
#     "content": [
#       {
#         "token": "대한",
#         "bytes": [235, 140, 128, 237, 149, 156],
#         "logprob": -2.236549335066229e-05,
#         "top_logprobs": []
#       },
#       {
#         "token": "민국",
#         "bytes": [235, 175, 188, 234, 181, 173],
#         "logprob": -2.816093228830141e-06,
#         "top_logprobs": []
#       },
#       {
#         "token": "의",
#         "bytes": [236, 157, 152],
#         "logprob": -9.372294698550832e-06,
#         "top_logprobs": []
#       },
#       {
#         "token": " 수도",
#         "bytes": [32, 236, 136, 152, 235, 143, 132],
#         "logprob": -0.0002787359117064625,
#         "top_logprobs": []
#       },
#       {
#         "token": "는",
#         "bytes": [235, 138, 148],
#         "logprob": -4.7755875129951164e-05,
#         "top_logprobs": []
#       },
#       {
#         "token": " 서울",
#         "bytes": [32, 236, 132, 156, 236, 154, 184],
#         "logprob": -0.000194361709873192,
#         "top_logprobs": []
#       },
#       {
#         "token": "입니다",
#         "bytes": [236, 158, 133, 235, 139, 136, 235, 139, 164],
#         "logprob": -0.006821697112172842,
#         "top_logprobs": []
#       },
#       {
#         "token": ".",
#         "bytes": [46],
#         "logprob": -1.843177233240567e-05,
#         "top_logprobs": []
#       }
#     ],
#     "refusal": null
#   }
# }
print(response.response_metadata)
```

<br>

# 스트리밍하기
```python
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_teddynote.messages import stream_response

load_dotenv()

# 객체 생성
llm = ChatOpenAI(
    temperature=0.1,
    model_name="gpt-4.1-nano",
)

# 질의내용
question = "대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!"

# 답변
answer = llm.stream(question)

# 반복문을 통한 스트리밍 출력(실시간)
for token in answer:
    print(token.content, end="", flush=True)

# stream_response 함수를 통한 스트리밍 출력(실시간)
answer2 = stream_response(answer, return_output=True)
print(answer2)
```