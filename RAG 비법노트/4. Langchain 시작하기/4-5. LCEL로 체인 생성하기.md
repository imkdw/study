# 체인과 LCEL
- 프롬프트를 LLM으로 전달하도록 연결하는 파이프라인을 `체인`이라고 부름
- LCEL이란 여러 구성 요소를 하나의 체인으로 엮어주는 표현인데 각 요소 사이는 파이프연산자(`|`)로 연결함

<br>

# 예제
```python
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_teddynote.messages import stream_response
from langchain_core.prompts import PromptTemplate


load_dotenv()


prompt = PromptTemplate.from_template("{topic} 에 대해 {how} 설명해주세요.")

model = ChatOpenAI(model="gpt-4.1-nano", temperature=0.1)

chain = prompt | model

input = {"topic": "인공지능 모델의 학습 원리", "how": "5살짜리도 이해하기 쉽게"}

answer = chain.stream(input)

# 물론이야! 인공지능은 마치 아주 똑똑한 로봇 친구라고 생각하면 돼. 이 친구는 많은 그림이나 이야기를 보고 배우면서 점점 더 똑똑해지는 거야.
# 예를 들어, 네가 그림책을 많이 보면 그림이 뭔지 알게 되는 것처럼, 인공지능도 많은 데이터를 보고 배우면서 어떤 것이 무엇인지 알게 되는 거야.
# 그래서 나중에는 네가 물어보면 답도 해줄 수 있는 거지!%
stream_response(answer)
```
