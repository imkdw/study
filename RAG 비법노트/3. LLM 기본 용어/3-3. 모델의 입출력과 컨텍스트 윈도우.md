# 컨텍스트 윈도우
- 모델이 한 번에 처리하고 이해할 수 있는 전체 텍스트의 문맥 범위
- LLM 모델이 긴 텍스트를 처리하는 동안 특정 순간에 모델이 주의를 기울이고 있는 텍스트의 부분을 가르킴

<br>

# 컨텍스트 길이
- LLM이 한 번에 처리할 수 있는 입력과 출력의 총합 토큰 수그를 뜻함
- 모델마다 제한이 존재하므로 데이터 입력 / 출력시 제약이 있음
- GPT 모델의 경우 일반적으로 수천 개의 토큰을 한 번에 처리할 수 있음

<br>

# max_tokens 파라미터
- 모델이 생성할 수 있는 최대 출력 토큰 수를 지정하는 파라미터
- 이는 모델이 응답으로 생성할 수 있는 최대 텍스트 길이를 결정함
- GPT-4 모델에서 컨텍스트 길이가 128k인 경우 입/출력을 합쳐서 128k개 까지 처리가 가능하다는 뜻임
- 하지만 모델이 한 번에 내놓을 수 있는 답변은 4096개의 토큰으로 제한되서 출력이 길어질수록 입력이 제한됨
- 보고서같은 매우 긴 텍스트는 한번에 작성이 불가능하고 `계속 생성하기` 기능을 사용해서 답변을 이어가야함

<br>

# 토큰의 비용차이
- 입력 토큰보다 출력 토큰의 비용이 약 3배 더 비쌈