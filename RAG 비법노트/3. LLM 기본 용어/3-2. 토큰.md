# 토큰과 토큰화
- `토큰`은 자연어 처리에서 텍스트를 처리하기 위해 나눈 작은 단위임
- `토큰화`는 텍스트를 토큰으로 나누는 과정을 뜻함
- 토큰은 단어나 문장을 더 작은 단위로 쪼개는 개념임

<br>

# 토큰화 방식
### 문자 기반 토큰화
- 텍스트를 개별 문자로 쪼개서 각 문자를 하나의 토큰으로 취급하는 방식
- `hello`라는 단어를 토큰화하면 `h, e, l, l, o`로 나뉨

<br>

### 단어 기반 토큰화
- 텍스트를 단어 단위로 나누느 방식임
- `Hello, World!`라는 단어를 토큰화하면 `["Hello"," ", "World", "!"]`로 나뉨
- 문자 기반으로 했을땐 12개가 나오지만 단어 기반 토큰화 방식에선 4개가 나옴

<br>

### 토큰화가 끼치는 영향
- LLM은 먼저 나올 토큰을 생성하면 그 다음 나올 토큰을 확률적으로 계산해서 생성하는 로직을 가지고있음
- 문자 기반 토큰화는 토큰수가 많기때문에 틀린 토큰을 생성할 가능성이 높음
- 문자 기반 토큰화는 예측을 더 많이 해야하므로 비용상 비효율적이며 틀릴 확률도 높음
- 하지만 단어 기반 토큰화도 자주 사용하지는 않는데 이유는 모든 단어를 다 만들어서 `단어 사전`을 만들어야되는데 워낙 경우의수가 많음

<br>

### 서브워드 기반 토큰화
- 서브워드는 단어보다 작은 단위로 접두사, 접미사, 어근 등에 해당됨
- 대표적인 방법인 `바이트 페어 인코딩(BPE)`은 단어를 처음 문자 단위로 분해하고 자주 등장하는 문자 쌍을 결헙해서 더 큰 서브워드로 만듬
- BPE의 예시
  - `unhappiness` -> `un`, `happniess`
  - 더 작은 단위로 쪼재고 자주 등장하는 단위를 빈도에 따라 묶음
    - u, n, h, a 등 문자로 나누고 -> un, nh, ha 등으로 묶어서 빈도수를 계산함
    - 자주 함께 등장하는 단어 쌍을 하나의 토큰으로 결합하는데 빈도가 높은 `un`이 하나의 토큰, `happniess`는 빈도가 높아서 하나의 토큰으로 처리

<br>

### 임베딩 백터
- 텍스트를 처리할 때 단어나 문장을 그대로 사용하지 않고 분절해서 토큰 단위로 나누고 숫자로 변환하는 과정을 거침
- 이런 과정의 결과물을 `임베딩 벡터`라고 부르는데 이는 토큰 단위로 이루어짐

<br>

# 토큰 사용량 계산하기
- 토큰은 사용량에 따라서 비용을 정하는 기준이 됨
- 일반적으로는 한글이 영어보다 토큰을 더 많이 소모하게됨
- 하지만 요즘 최신 모델은 최적화가 많이 이루어져서 한글이나 영어나 거의 비슷해짐
