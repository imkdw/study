# OpenAIEmbeddings
- `OpenAIEmbeddings`는 많은 사람들이 활용하는 대표적인 임베딩 알고리즘
- 다국어 처리 성능이 준수해서 다양한 언어에 무난하게 적용이 가능함

<br>

# 텍스트 임베딩
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

text = "임베딩 테스트를 하기 위한 샘플 문장입니다."

query_result = embeddings.embed_query(text)

# OpenAI 임베딩 모델은 1536차원의 벡터를 생성함
print(len(query_result))

# [
#     -0.00776276458054781,
#     -0.002635254291817546,
#     0.020537279546260834,
#     -0.012518479488790035,
#     -0.036999788135290146,
#     -0.018380047753453255,
#     -0.007250694558024406,
#     0.015405682846903801,
#     0.019567614421248436,
#     0.015645375475287437,
#     // ...
# ]
print(query_result)
```

<br>

# 문서 임베딩
- 문서 임베딩은 문서를 구성하는 단락을 벡터로 변환하는 것임

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

text = "임베딩 테스트를 하기 위한 샘플 문장입니다."

# 텍스트를 임베딩해서 문서 임베딩 생성
doc_result = embeddings.embed_documents([text, text, text, text])

# 4개의 문장이 임베딩되어 있음
print(len(doc_result))

# 각각 문서는 1536차원의 벡터가 생성됨
print(len(doc_result[0]))
```

<br>

# 차원 지정하기
- 만약 벡터 데이터베이스가 1024차원만 지원하면 기본 차원인 1536차원을 1024차원으로 변경해야함
- `dimensions` 값을 사용해서 차원수 변경이 가능함

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small", dimensions=1024)

text = "임베딩 테스트를 하기 위한 샘플 문장입니다."

query_result = embeddings.embed_query(text)

# 차원을 1024 차원으로 지정해서 1024 차원으로 임베딩됨
print(len(query_result))
```

<br>

# 임베딩 간 유사도 계산하기
```python
from langchain_openai import OpenAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity


# 두개의 벡터간 각도를 기준으로 유사도를 측정함
# 변환값은 -1 ~ 1 사이이며 1에 가까울수록 두 벡터는 유사함
def similarity(a, b):
    return cosine_similarity([a], [b])[0][0]


embeddings_1024 = OpenAIEmbeddings(model="text-embedding-3-small", dimensions=1024)

sentence1 = "안녕하세요? 반갑습니다."
sentence2 = "안녕하세요? 반갑습니다!"
sentence3 = "안녕하세요? 만나서 반가워요."
sentence4 = "Hi, nice to meet you."
sentence5 = "I like to eat apples."


sentences = [sentence1, sentence2, sentence3, sentence4, sentence5]
embedded_sentences = embeddings_1024.embed_documents(sentences)

# [유사도 0.9644] 안녕하세요? 반갑습니다.          <=====>         안녕하세요? 반갑습니다!
# [유사도 0.8376] 안녕하세요? 반갑습니다.          <=====>         안녕하세요? 만나서 반가워요.
# [유사도 0.5042] 안녕하세요? 반갑습니다.          <=====>         Hi, nice to meet you.
# [유사도 0.1362] 안녕하세요? 반갑습니다.          <=====>         I like to eat apples.
# [유사도 0.8142] 안녕하세요? 반갑습니다!          <=====>         안녕하세요? 만나서 반가워요.
# [유사도 0.4790] 안녕하세요? 반갑습니다!          <=====>         Hi, nice to meet you.
# [유사도 0.1318] 안녕하세요? 반갑습니다!          <=====>         I like to eat apples.
# [유사도 0.5128] 안녕하세요? 만나서 반가워요.     <=====>         Hi, nice to meet you.
# [유사도 0.1409] 안녕하세요? 만나서 반가워요.     <=====>         I like to eat apples.
# [유사도 0.2249] Hi, nice to meet you.    <=====>         I like to eat apples.
for i, sentence in enumerate(embedded_sentences):
    for j, other_sentence in enumerate(embedded_sentences):
        if i < j:
            print(
                f"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \t <=====> \t {sentences[j]}"
            )
```