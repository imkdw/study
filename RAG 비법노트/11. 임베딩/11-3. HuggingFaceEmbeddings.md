# HuggingFaceEmbeddings
- `HuggingFaceEndpointEmbeddings`는 모델을 다운로드 받아서 사용하는 대신 허깅페이스에서 제공하는 Inference API를 통해 원격으로 임베딩을 요청하고 결과를 받을 수 있는 방식임
- 하지만 모든 모델이 Inference API를 지원하지는 않으므로 테스트해보고 안되면 모델을 다운로드 받아서 쓰면됨

```python
import os
import time
import warnings
from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings

warnings.filterwarnings("ignore")

os.environ["HF_HOME"] = "./cache/"

texts = [
    "안녕, 만나서 반가워.",
    "LangChain simplifies the process of building applications with large language models",
    "랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. ",
    "LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.",
    "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.",
]
model_name = "intfloat/multilingual-e5-large-instruct"


hf_embeddings = HuggingFaceEndpointEmbeddings(
    model=model_name,
    task="feature-extraction",
    huggingfacehub_api_token=os.environ["HUGGINGFACEHUB_API_TOKEN"],
)

start_time = time.time()
embedded_documents = hf_embeddings.embed_documents(texts)
embedding_time = time.time() - start_time

# 문서 임베딩 수행 시간: 5.60초
print(f"문서 임베딩 수행 시간: {embedding_time:.2f}초")

# [HuggingFace Endpoint Embedding]
# Model:          intfloat/multilingual-e5-large-instruct
# Dimension:      1024
print("[HuggingFace Endpoint Embedding]")
print(f"Model: \t\t{model_name}")
print(f"Dimension: \t{len(embedded_documents[0])}")

embedded_query = hf_embeddings.embed_query("LangChain 에 대해서 알려주세요.")

# 1024 차원의 벡터가 생성됨
print(len(embedded_query))
```

<br>

# 임베딩된 질문과 문서 간 유사도 계산하기
- 임베딩 벡터의 `내적`을 통해 유사도를 구해서 쿼리, 문서간 관련성 파악이 가능함
- `내적` : 두 벡터 크기와 방향이 얼마나 일치하는지를 숫자로 나타냄
- 숫자가 클수록 두 벡터가 더 비슷한 방향을 가지므로 질문과 문서가 더 유사하다고 판단함
```python
import os
import warnings
from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings
import numpy as np


warnings.filterwarnings("ignore")

os.environ["HF_HOME"] = "./cache/"

texts = [
    "안녕, 만나서 반가워.",
    "LangChain simplifies the process of building applications with large language models",
    "랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. ",
    "LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.",
    "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.",
]
model_name = "intfloat/multilingual-e5-large-instruct"


hf_embeddings = HuggingFaceEndpointEmbeddings(
    model=model_name,
    task="feature-extraction",
    huggingfacehub_api_token=os.environ["HUGGINGFACEHUB_API_TOKEN"],
)

embedded_documents = hf_embeddings.embed_documents(texts)

question = "LangChain 에 대해서 알려주세요."
embedded_query = hf_embeddings.embed_query(question)

# 내림차순으로 정렬해서 가장 유사한 문서의 인덱스 반환
sorted_idx = (np.array(embedded_query) @ np.array(embedded_documents).T).argsort()[::-1]


# [Query] LangChain 에 대해서 알려주세요.
# ====================================
# [0] LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.

# [1] LangChain simplifies the process of building applications with large language models

# [2] 랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다.

# [3] 안녕, 만나서 반가워.

# [4] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.
print(f"[Query] {question}\n====================================")
for i, idx in enumerate(sorted_idx):
    print(f"[{i}] {texts[idx]}")
    print()
```

<br>

# HuggingFaceEmbeddings
- 로컬에 자신의 모델을 다운로드 받아서 사용하는 방식

```python
from langchain_huggingface.embeddings import HuggingFaceEmbeddings

model_name = "intfloat/multilingual-e5-large-instruct"

texts = [
    "안녕, 만나서 반가워.",
    "LangChain simplifies the process of building applications with large language models",
    "랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. ",
    "LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.",
    "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.",
]

hf_embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs={"device": "mps"},  # cuda, cpu
    encode_kwargs={"normalize_embeddings": True},
)

embedded_documents = hf_embeddings.embed_documents(texts)

# Model:          intfloat/multilingual-e5-large-instruct
# Dimension:      1024
print(f"Model: \t\t{model_name}")
print(f"Dimension: \t{len(embedded_documents[0])}")
```

<br>

# BGE-M3 임베딩
- 기타 다른 원하는 모델로도 임베딩이 가능함
```python

from dotenv import load_dotenv
from langchain_teddynote import logging

load_dotenv()
logging.langsmith("langchain_test")

# ========================================================================

from langchain_huggingface import HuggingFaceEmbeddings
import numpy as np

texts = [
    "안녕, 만나서 반가워.",
    "LangChain simplifies the process of building applications with large language models",
    "랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. ",
    "LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.",
    "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.",
]

model_name = "BAAI/bge-m3"
model_kwargs = {"device": "mps"}
encode_kwargs = {"normalize_embeddings": True}
hf_embeddings = HuggingFaceEmbeddings(
    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
)

embedded_documents = hf_embeddings.embed_documents(texts)

# Model: safetensorBAAI/bge-m3
# Dimension:      1024
print(f"Model: \t\t{model_name}")
print(f"Dimension: \t{len(embedded_documents[0])}")


embedded_query = hf_embeddings.embed_query("LangChain 에 대해서 알려주세요.")
embedded_documents = hf_embeddings.embed_documents(texts)

sorted_idx = (np.array(embedded_query) @ np.array(embedded_documents).T).argsort()[::-1]

# Query] LangChain 에 대해서 알려주세요.██▉                                                                                                                                              | 241M/2.27G [00:05<00:35, 56.7MB/s]
# ====================================
# [0] LangChain simplifies the process of building applications with large language models

# [1] LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.

# [2] 랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다.

# [3] 안녕, 만나서 반가워.

# [4] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.
print("[Query] LangChain 에 대해서 알려주세요.\n====================================")
for i, idx in enumerate(sorted_idx):
    print(f"[{i}] {texts[idx]}")
    print()
```

<br>

# FlagEmbedding
- bge-m3 모델은 `FlagEmbedding` 이라는 패키지를 제공하는데 여기에는 세가지 접근법이 존재함

<br>

### Dense Vector
- 밀집 벡터를 생성해서 텍스트를 표현. 일반적으로 임베딩 방식은 밀집 벡터를 뜻함
- 다국어 및 다중 작업 지원에 강점이 있고 벡터의 모든 값이 연속적인 데이터를 포함해서 문맥적으로 유사한 문서를 찾는 의미 기반 검색에 적합함

<br>

### Sparse Embedding(Lexical Weight 활용)
- 특정 단어나 구문의 어휘적 중요도를 반영해서 희소 벡터를 생성
- 단어의 중요도를 계산한 후 해당 가중치를 기반으로 벡터를 생성
- 생성된 벡터의 대부분 값이 0이지만 특정 단어나 구문과 정확히 매칭이 가능한 높은 정밀도를 제공함
- 계산 속도가 빠르고 단어의 중요도를 직접 반영이 가능해서 키워드 기반 검색이나 정확한 단어 매칭이 중요한 도메인에서 활용됨

<br>

### Multi-Vector(ColBERT 기반)
- 문서와 질문을 각각의 벡터로 표현하고 토큰 간 유사도를 계산해서 문맥을 고려한 세밀한 매칭을 수행
- 문맥적 중요성 반영이 가능하고 긴 문서에서도 효과적으로 작동함

<br>

### 밀집 벡터 방식
```python
from FlagEmbedding import BGEM3FlagModel

texts = [
    "안녕, 만나서 반가워.",
    "LangChain simplifies the process of building applications with large language models",
    "랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. ",
    "LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.",
    "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.",
]

model_name = "BAAI/bge-m3"

# fp16을 활성화하면 성능은 낮아지지만 계산 속도가 빨라짐
bge_embeddings = BGEM3FlagModel(model_name, use_fp16=True)

bge_embedded = bge_embeddings.encode(
    texts,
    batch_size=12,
    max_length=8192,
)["dense_vecs"]

# (5, 1024), 5개의 항목을 1024 차원의 벡터로 변환
print(bge_embedded.shape)
```

<br>

### 희소 벡터 방식
```python
from FlagEmbedding import BGEM3FlagModel

texts = [
    "안녕, 만나서 반가워.",
    "LangChain simplifies the process of building applications with large language models",
    "랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. ",
    "LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.",
    "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.",
]


bge_flagmodel = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)
bge_encoded = bge_flagmodel.encode(texts, return_dense=True)

# (5, 1024)
print(bge_encoded["dense_vecs"].shape)
```

<br>

### 매칭 점수 계산
```python
from FlagEmbedding import BGEM3FlagModel

texts = [
    "안녕, 만나서 반가워.",
    "LangChain simplifies the process of building applications with large language models",
    "랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. ",
    "LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.",
    "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.",
]


bge_flagmodel = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)
bge_encoded = bge_flagmodel.encode(texts, return_dense=True)

print(bge_encoded)
print(bge_encoded["lexical_weights"])

# lexical_weights는 각 텍스트의 어휘적 중요도를 표현함
# compute_lexical_matching_score는 두 텍스트 간 어휘적 메칭 점수를 계산해줌
lexical_scores1 = bge_flagmodel.compute_lexical_matching_score(
    bge_encoded["lexical_weights"][0], bge_encoded["lexical_weights"][0]
)
lexical_scores2 = bge_flagmodel.compute_lexical_matching_score(
    bge_encoded["lexical_weights"][0], bge_encoded["lexical_weights"][1]
)

# 0 <-> 0
print(lexical_scores1)
# 0 <-> 1
print(lexical_scores2)
```