# ChatPromptTemplate
- 챗봇처럼 상호작용하는 시스템에서는 `ChatPromptTemplate`을 사용하면 프롬프트 템플릿보다 더 자연스럽고 좋은 답변을 얻을 수 있음
- 이는 튜플로 구성하며 3가지 구성요소가 존재함

<br>

# ChatPromptTemplate의 구성요소
### System
- 시스템 설정 메세지로 대화 전체에 적용되는 설정임
- AI의 역할이나 페르소나를 지정함
  - ex) 당신은 메이플스토리 매니아입니다

<br>

### Human
- 사용자가 입력하는 메세지
- 해당 메세지를 기반으로 AI는 댑변을 생성하는데 여러개의 메세지도 포함할 수 있어서 대화의 흐름을 더욱 자연스럽게 구성이 가능함

<br>

### AI
- AI의 응답 메세지를 의미하고 이전 대화 내용을 기반으로 생성된 답변은 포함
- 맥락을 유지하므로 추가적인 분석과 조언도 가능함
- 이러한 내용을 대화에 포함하면 연속성이 유지되어서 사용자가 더욱 자연스럽게 AI와 상호작용이 가능함

<br>

# 예제

```python
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_core.prompts import load_prompt
from pathlib import Path
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

llm = ChatOpenAI(model="gpt-4o-mini")

chat_prompt = ChatPromptTemplate.from_template("{country}의 수도는 어디인가요?")

# input_variables=['country'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?'), additional_kwargs={})]
print(chat_prompt)

# Human: 대한민국의 수도는 어디인가요?
print(chat_prompt.format(country="대한민국"))


# 튜플 형태로 구성됨
chat_template = ChatPromptTemplate.from_messages(
    [
        ("system", "당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 {name} 입니다."),
        ("human", "반가워요!"),
        ("ai", "안녕하세요! 무엇을 도와드릴까요?"),
        ("human", "{user_input}"),
    ]
)

messages = chat_template.format_messages(
    name="엔버", user_input="당신의 이름은 무엇입니까?"
)

# [
#     SystemMessage(
#         content="당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 엔버 입니다.",
#         additional_kwargs={},
#         response_metadata={},
#     ),
#     HumanMessage(content="반가워요!", additional_kwargs={}, response_metadata={}),
#     AIMessage(
#         content="안녕하세요! 무엇을 도와드릴까요?",
#         additional_kwargs={},
#         response_metadata={},
#     ),
#     HumanMessage(
#         content="당신의 이름은 무엇입니까?", additional_kwargs={}, response_metadata={}
#     ),
# ]
print(messages)

# 제 이름은 엔버입니다! 어떻게 도와드릴까요?
print(llm.invoke(messages).content)
```