# 벡터 스토어 기반 리트리버
- 벡터 스토어에 구현된 유사도 검색이나 MMR 같은 검색 메소드를 사용해서 벡터 스토어 내부의 텍스트를 검색함
- `as_retriever()` 메소드를 통해서 벡터 스토어에 정속된 리트리버를 얻을 수 있음

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader

loader = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/13. 임시/검은 마법사_보스 몬스터_공격 패턴 - 나무위키.txt"
)
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
split_docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()

db = FAISS.from_documents(split_docs, embeddings)

retriever = db.as_retriever()

# 관련 문서를 검색
docs = retriever.invoke("검은마법사의 체력은 몇인가요?")

for doc in docs:
    print(doc.page_content)
    print("=========================================================")
```

<br>

### 주요 파라미터
- search_type : 검색 유형으로 기본값은 오직 유사도만 보는 `similarity`. mmr, similarity_score_threshold 등 다양한 검색 유형을 지원함
- search_kwargs : 검색 유형에 따라 다른 파라미터를 전달할 수 있음
  - k : 반환할 문서 수
  - score_threshold : similarity_score_threshold 검색 유형에서 최소 유사도 임계값 지정
  - fetch_k : 초기 MMR 알고리즘에 전달할 문서 수, 이 만큼 먼저 MMR 알고리즘에 따라 N개의 문서를 검색하고 그중에서 다양성을 고려해서 k개의 문서를 걸러냄
  - lambda_mult : MMR 결과의 다양성을 조절, 0은 다양성 고려 X, 1은 다양성만 보고 검색
  - filter : 메타데이터를 기준으로 필터링하기

<br>

# MMR 알고리즘
- 문서의 관련성과 더불어 이미 선택된 문서들과의 차별성(다양성)도 동시에 고려해서 문서의 중복을 피함
```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader

loader = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/13. 임시/검은 마법사_보스 몬스터_공격 패턴 - 나무위키.txt"
)
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
split_docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()

db = FAISS.from_documents(split_docs, embeddings)


retriever = db.as_retriever(
    # MMR 알고리즘 사용
    search_type="mmr",
    search_kwargs={
        # 검색개수는 2개로 지정
        "k": 2,
        # 기본적으로 10개의 문서를 전달
        "fetch_k": 10,
        # 다양성 조절
        "lambda_mult": 0.6,
    },
)

docs = retriever.invoke("창조의 아이온은 무엇인가요?")

for doc in docs:
    print(doc.page_content)
    print("=========================================================")
```


<br>

# 검색을 동적으로 조정
- 위 예제에서는 리트리버를 가져올 때 설정을 진행함
- invoke() 메소드에도 동적으로 설정 조정이 가능함

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import ConfigurableField


loader = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/13. 임시/검은 마법사_보스 몬스터_공격 패턴 - 나무위키.txt"
)
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
split_docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()

db = FAISS.from_documents(split_docs, embeddings)


retriever = db.as_retriever(search_kwargs={"k": 1}).configurable_fields(
    search_type=ConfigurableField(
        id="search_type",
        name="Search Type",
        description="The search type to use",
    ),
    search_kwargs=ConfigurableField(
        id="search_kwargs",
        name="Search Kwargs",
        description="The search kwargs to use",
    ),
)

# 검색 설정을 지정. Faiss 검색에서 k=3로 설정하여 가장 유사한 문서 3개를 반환
config = {"configurable": {"search_kwargs": {"k": 3}}}

docs = retriever.invoke("창조의 아이온은 무엇인가요?")

for doc in docs:
    print(doc.page_content)
    print("=========================================================")
```

<br>

# Upstage 처럼 질문 및 저장용 임베딩 모델이 분리된 경우
- Upstage의 경우 `질문용 query 임베딩 모델`, `문서 저장용 passage 임베딩 모델` 2개로 나누어짐
- 이런경우 리트리버를 가져와서 invoke() 메소드를 호출하려면 passage 임베딩 모델을 사용함

```python
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_upstage import UpstageEmbeddings

loader = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/13. 임시/검은 마법사_보스 몬스터_공격 패턴 - 나무위키.txt"
)

documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
split_docs = text_splitter.split_documents(documents)

doc_embedder = UpstageEmbeddings(model="solar-embedding-1-large-passage")
query_embedder = UpstageEmbeddings(model="solar-embedding-1-large-query")

db = FAISS.from_documents(split_docs, doc_embedder)

query_vector = query_embedder.embed_query(
    "하드모드 1페이즈는 몇초안에 클리어하는게 좋아?"
)
print(db.similarity_search_by_vector(query_vector, k=2))
```