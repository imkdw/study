# 교차 인코더 리랭커
- 문서의 관련성을 점수화하고 상위 몇 개의 문서를 선택해서 더욱 정밀한 검색 결과를 반환함

<br>

# 기본적인 리트리버
```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"문서 {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )


documents = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/18. 리랭커로 검색된 문서 순위 조정하기/appendix-keywords.txt"
).load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

texts = text_splitter.split_documents(documents)

embeddingsModel = HuggingFaceEmbeddings(
    model_name="sentence-transformers/msmarco-distilbert-dot-v5"
)

retriever = FAISS.from_documents(texts, embeddingsModel).as_retriever(
    search_kwargs={"k": 10}
)

query = "Word2Vec 에 대해서 알려줄래?"

docs = retriever.invoke(query)

pretty_print_docs(docs)
```

<br>

# 교차 인코더 리랭커로 감싸기
- `ContextualCompressionRetriever`로 감싸서 검색 결과를 재정렬하고 상위 문서만 선택해 출력이 가능함
```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"문서 {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )


documents = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/18. 리랭커로 검색된 문서 순위 조정하기/appendix-keywords.txt"
).load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

texts = text_splitter.split_documents(documents)

embeddingsModel = HuggingFaceEmbeddings(
    model_name="sentence-transformers/msmarco-distilbert-dot-v5"
)

retriever = FAISS.from_documents(texts, embeddingsModel).as_retriever(
    search_kwargs={"k": 10}
)

model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-v2-m3")

# 상위 3개의 문서만 선택
compressor = CrossEncoderReranker(model=model, top_n=3)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke("Word2Vec 에 대해서 알려줄래?")

# 문서 1:

# Crawling

# 정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.
# 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.
# 연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진

# Word2Vec

# 정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
# 예시: Word2Vec 모델에서 "왕"과 "여왕"은 서로 가까운 위치에 벡터로 표현됩니다.
# 연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
# LLM (Large Language Model)
# ----------------------------------------------------------------------------------------------------
# 문서 2:

# Token

# 정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
# 예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다.
# 연관키워드: 토큰화, 자연어 처리, 구문 분석

# Tokenizer

# 정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
# 예시: "I love programming."이라는 문장을 ["I", "love", "programming", "."]으로 분할합니다.
# 연관키워드: 토큰화, 자연어 처리, 구문 분석

# VectorStore

# 정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
# 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.
# 연관키워드: 임베딩, 데이터베이스, 벡터화

# SQL
# ----------------------------------------------------------------------------------------------------
# 문서 3:

# Semantic Search

# 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
# 예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
# 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

# Embedding

# 정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
# 예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
# 연관키워드: 자연어 처리, 벡터화, 딥러닝
pretty_print_docs(compressed_docs)
```