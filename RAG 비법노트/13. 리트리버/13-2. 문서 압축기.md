# 문서 압축기
- 검색 시스템에서는 어떤 질문이 들어올지 사전에 알 수 없음
- 질문에 답변하기 위해 필요한 문서 말고도 이와 무관한 텍스트가 많이 포함되어있기 마련임
- 문서 압축기는 검색된 문서를 즉시 반환하는 대신 주어진 질문과의 관련된 정보를 추려내고 불필요한 문서를 제거해서 압축함

<br>

# 문서만 조회했을때
- 기본값인 k=4 처럼 4개의 문서가 반환됨
- 여기에는 불필요한 문서도 많이 포함되어있음
```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"문서 {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )


loader = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/13. 임시/검은 마법사_보스 몬스터_공격 패턴 - 나무위키.txt"
)

text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
texts = loader.load_and_split(text_splitter)

retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()

docs = retriever.invoke("하드모드 1페이즈는 몇초안에 클리어하는게 좋아?")

# 문서 1:

# 익스트림 모드 기준으로는 1페이즈의 체력 비중이 많이 높아지기에 여유컷이 아닌 이상 1페이즈를 한 번만에 넘길 순 없다. 따라서 하드 때와는 빌드가 달라지게 된다. 하지만 제한시간도 30분으로 줄었기 때문에 하드와 비슷하게 3번의 극딜 안에 넘어가야 하는 것은 똑같다.
# ----------------------------------------------------------------------------------------------------
# 문서 2:

# 하드 모드 기준으로는 웬만하면 바인드 극딜 1번 안에(3~40초 안에) 넘어가야 한다. 그 이상이 걸릴 경우 통곡의 장벽이 계속 올라와 맵은 좁아지기 때문에 아이온과 얄다바오트의 협공을 버티기가 힘들어지면서 공략에 애로사항이 꽃 핀다. 1번 안에 1페이즈를 못 넘기면 30분 이상, 2번 안에 못 넘기면 50분 이상 공략하는 장기전을 각오해야 할 것이고, 3번 안에 못 넘기면 최소컷 스펙에도 못미치므로 이후 진행이 불가능하다.
# ----------------------------------------------------------------------------------------------------
# 문서 3:

# 시간이 지날수록 맵이 점점 좁아지기 때문에 나중에는 아이온과 얄다바오트의 공격을 피할 수 없게 되어 사실상 게임 오버에 이르게 된다. 따라서 1페이즈는 되도록이면 바인드 한 번이나 두 번으로 끝내야 한다.
# · 붉은 번개

# 불길한 붉은 번개가 내리쳐 움직임을 제한한다.
# 맵에 600px[18] 간격으로 예고 이펙트가 6개 나타나고, 2.76초 후 붉은 번개가 떨어진다. 붉은 번개는 12.24초 간 지속된다. 피격 시 99% 데미지를 입는다. 다크 사이트로 회피할 수 있으며, 텔레포트로 넘어갈 수 있다.
# ----------------------------------------------------------------------------------------------------
# 문서 4:

# 1페이즈는 보호막을 깔일이 잘 없고, 2페이즈는 영구 지속이 아닐뿐더러 보호막 갱신 주기도 1분을 넘다보니 화력이 부족해도 크게 체감되지 않는다. 하지만 3, 4페이즈에서는 보호막이 영구적으로 지속되고, 권능이 끝난 후 보호막도 계속해서 갱신되기 때문에 딜량이 부족하면 본체의 피를 깎기 매우 힘들어진다. 특히 4페이즈의 경우 갱신 시간이 30초라 평딜이 약한 직업의 경우에는 아예 극딜때만 딜하는 경우도 있다.
pretty_print_docs(docs)
```

<br>

# 문서 압축해보기
- 압축은 LLM을 통해서 진행되는데 4건 -> 3건으로 줄어들었음
```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import CharacterTextSplitter
from langchain_teddynote.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"문서 {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )


loader = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/13. 임시/검은 마법사_보스 몬스터_공격 패턴 - 나무위키.txt"
)

text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
texts = loader.load_and_split(text_splitter)

retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()

llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")

compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever,
)

compressed_docs = compression_retriever.invoke(
    "하드모드 1페이즈는 몇초안에 클리어하는게 좋아?"
)

# 문서 1:

# 익스트림 모드 기준으로는 1페이즈의 체력 비중이 많이 높아지기에 여유컷이 아닌 이상 1페이즈를 한 번만에 넘길 순 없다. 따라서 하드 때와는 빌드가 달라지게 된다. 하지만 제한시간도 30분으로 줄었기 때문에 하드와 비슷하게 3번의 극딜 안에 넘어가야 하는 것은 똑같다.
# ----------------------------------------------------------------------------------------------------
# 문서 2:

# 하드 모드 기준으로는 웬만하면 바인드 극딜 1번 안에(3~40초 안에) 넘어가야 한다. 1번 안에 1페이즈를 못 넘기면 30분 이상, 2번 안에 못 넘기면 50분 이상 공략하는 장기전을 각오해야 할 것이고, 3번 안에 못 넘기면 최소컷 스펙에도 못미치므로 이후 진행이 불가능하다.
# ----------------------------------------------------------------------------------------------------
# 문서 3:

# · 붉은 번개

# 불길한 붉은 번개가 내리쳐 움직임을 제한한다.
# 맵에 600px[18] 간격으로 예고 이펙트가 6개 나타나고, 2.76초 후 붉은 번개가 떨어진다. 붉은 번개는 12.24초 간 지속된다. 피격 시 99% 데미지를 입는다. 다크 사이트로 회피할 수 있으며, 텔레포트로 넘어갈 수 있다.
pretty_print_docs(compressed_docs)
```

<br>

# 문서 필터링
- LLMChainFilter는 LLM 체인을 통해서 초기에 검색된 문서 중 어떤 문서를 필터링하고 어떤 문서를 반환할지 결정
- 이 필터는 문서 내용을 변경(압축)하지 않고 문서를 선택적으로 반환함

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import CharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain_teddynote.document_compressors import LLMChainFilter


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"문서 {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )


loader = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/13. 임시/검은 마법사_보스 몬스터_공격 패턴 - 나무위키.txt"
)

text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
texts = loader.load_and_split(text_splitter)

retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()

llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")

_filter = LLMChainFilter.from_llm(llm)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=_filter,
    base_retriever=retriever,
)

compressed_docs = compression_retriever.invoke(
    "하드모드 1페이즈는 몇초안에 클리어하는게 좋아?"
)

# 문서 1:

# 익스트림 모드 기준으로는 1페이즈의 체력 비중이 많이 높아지기에 여유컷이 아닌 이상 1페이즈를 한 번만에 넘길 순 없다. 따라서 하드 때와는 빌드가 달라지게 된다. 하지만 제한시간도 30분으로 줄었기 때문에 하드와 비슷하게 3번의 극딜 안에 넘어가야 하는 것은 똑같다.
# ----------------------------------------------------------------------------------------------------
# 문서 2:

# 하드 모드 기준으로는 웬만하면 바인드 극딜 1번 안에(3~40초 안에) 넘어가야 한다. 그 이상이 걸릴 경우 통곡의 장벽이 계속 올라와 맵은 좁아지기 때문에 아이온과 얄다바오트의 협공을 버티기가 힘들어지면서 공략에 애로사항이 꽃 핀다. 1번 안에 1페이즈를 못 넘기면 30분 이상, 2번 안에 못 넘기면 50분 이상 공략하는 장기전을 각오해야 할 것이고, 3번 안에 못 넘기면 최소컷 스펙에도 못미치므로 이후 진행이 불가능하다.
# ----------------------------------------------------------------------------------------------------
# 문서 3:

# 시간이 지날수록 맵이 점점 좁아지기 때문에 나중에는 아이온과 얄다바오트의 공격을 피할 수 없게 되어 사실상 게임 오버에 이르게 된다. 따라서 1페이즈는 되도록이면 바인드 한 번이나 두 번으로 끝내야 한다.
# · 붉은 번개

# 불길한 붉은 번개가 내리쳐 움직임을 제한한다.
# 맵에 600px[18] 간격으로 예고 이펙트가 6개 나타나고, 2.76초 후 붉은 번개가 떨어진다. 붉은 번개는 12.24초 간 지속된다. 피격 시 99% 데미지를 입는다. 다크 사이트로 회피할 수 있으며, 텔레포트로 넘어갈 수 있다.
pretty_print_docs(compressed_docs)
```

<br>

# 필터에 임베딩 적용
- `LLMChainExtractor`, `LLMChainFilter` 2개는 모두 LLM을 사용하므로 비용도 발생하고 느림
- `EmbeddingsFilter`는 임베딩을 기반으로 쿼리와 충분히 유사한 임베딩을 가진 문만 반환이 가능함

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import CharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain_teddynote.document_compressors import LLMChainFilter
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain_openai import OpenAIEmbeddings


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"문서 {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )


loader = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/13. 임시/검은 마법사_보스 몬스터_공격 패턴 - 나무위키.txt"
)

text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
texts = loader.load_and_split(text_splitter)

retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()

embeddings = OpenAIEmbeddings()

# 유사도 임계값 0.86 이상만 필터링
embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=embeddings_filter, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "하드모드 1페이즈는 몇초안에 클리어하는게 좋아?"
)

# 문서 1:

# 익스트림 모드 기준으로는 1페이즈의 체력 비중이 많이 높아지기에 여유컷이 아닌 이상 1페이즈를 한 번만에 넘길 순 없다. 따라서 하드 때와는 빌드가 달라지게 된다. 하지만 제한시간도 30분으로 줄었기 때문에 하드와 비슷하게 3번의 극딜 안에 넘어가야 하는 것은 똑같다.
pretty_print_docs(compressed_docs)
```

<br>

# 파이프라인(압축 + 문서변환) 만들기
- DocumentCompressorPipeline을 사용하면 여러 압축기를 파이프라인에 추가해서 순차적으로 결합이 가능함
- `Splitter`를 통해서 문서를 청크로 쪼개고 `중복 필터`로 검색된 문서 사이 유사도를 계산해서 임게치를 넘는 문서는 중복이라고 판단해서 제외
- `관련성 필터`는 임베딩 필터로 유사도가 임계치 이상인 관련성 높은 문서를 걸러내고 마지막으로 `LLMChainExtractor`를 통해서 문서를 압축함

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import CharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain_teddynote.document_compressors import LLMChainExtractor
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain_openai import OpenAIEmbeddings


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"문서 {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )


loader = TextLoader(
    "/Users/imkdw/study/RAG 비법노트/13. 임시/검은 마법사_보스 몬스터_공격 패턴 - 나무위키.txt"
)

from langchain.retrievers.document_compressors import DocumentCompressorPipeline
from langchain_community.document_transformers import EmbeddingsRedundantFilter
from langchain_text_splitters import CharacterTextSplitter


embeddings = OpenAIEmbeddings()

splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
texts = loader.load_and_split(splitter)

redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)

relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)

llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")

retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()


pipeline_compressor = DocumentCompressorPipeline(
    transformers=[
        splitter,
        redundant_filter,
        relevant_filter,
        LLMChainExtractor.from_llm(llm),
    ]
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=pipeline_compressor,
    base_retriever=retriever,
)

compressed_docs = compression_retriever.invoke(
    "하드모드 1페이즈는 몇초안에 클리어하는게 좋아?"
)

# 문서 1:

# 하드모드 1페이즈는 한 번만에 넘길 순 없다. 하지만 제한시간도 30분으로 줄었기 때문에 하드와 비슷하게 3번의 극딜 안에 넘어가야 하는 것은 똑같다.
pretty_print_docs(compressed_docs)
```