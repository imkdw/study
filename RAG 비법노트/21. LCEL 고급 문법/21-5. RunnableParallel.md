# RunnableParallel
- 작업을 병렬로 실행하는데 여러 개의 체인을 순차적으로 처리하는 것보다 시간을 훨씬 절약할 수 있음
- 시퀀스 안에서 하나의 Runnable 출력을 다음 Runnable 형식에 맞게 조작하는데 유용함

<br>

# 기본적으로 병렬 실행
- 체인 생성 부분에서 리트리버를 사용해서 context 가져오고 Passthrough를 통해 질문을 넘겨주는 부분이 병렬로 실행됨
- 아래 코드는 다 동일한 의미를 가짐
  - `{"context": retriever, "question": RunnablePassthrough()}`
  - `RunnableParallel({"context": retriever, "question": RunnablePassthrough()})`
  - `RunnableParallel(context=retriever, question=RunnablePassthrough())`

```python
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(
    ["Teddy is an AI engineer who loves programming!"], embedding=OpenAIEmbeddings()
)

retriever = vectorstore.as_retriever()

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI(model="gpt-4o-mini")

retrieval_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

retrieval_chain.invoke("What is Teddy's occupation?")
```

<br>

# itemgetter로 항목 추출하기
```python
from operator import itemgetter

from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(
    ["Teddy is an AI engineer who loves programming!"], embedding=OpenAIEmbeddings()
)

retriever = vectorstore.as_retriever()

template = """Answer the question based only on the following context:
{context}

Question: {question}

Answer in the following language: {language}
"""
prompt = ChatPromptTemplate.from_template(template)

chain = (
    {
        "context": itemgetter("question") | retriever,
        "question": itemgetter("question"),
        "language": itemgetter("language"),
    }
    | prompt
    | ChatOpenAI(model="gpt-4o-mini")
    | StrOutputParser()
)

# 테디의 직업은 AI 엔지니어입니다.
print(chain.invoke({"question": "What is Teddy's occupation?", "language": "Korean"}))
```

<br>

# 병렬처리 단계별로 살펴보기
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser


model = ChatOpenAI()

capital_chain = (
    ChatPromptTemplate.from_template("{country} 의 수도는 어디입니까?")
    | model
    | StrOutputParser()
)

area_chain = (
    ChatPromptTemplate.from_template("{country} 의 면적은 얼마입니까?")
    | model
    | StrOutputParser()
)

map_chain = RunnableParallel(capital=capital_chain, area=area_chain)

# {
#     "capital": "대한민국의 수도는 서울입니다.",
#     "area": "대한민국의 면적은 약 100,363km² 입니다.",
# }
print(map_chain.invoke({"country": "대한민국"}))
```