# 필요한 패키지
```
pip install -qU faiss-cpu tiktoken grandalf
```

<br>

# Langsmith 대신 직접 보기
```python
from langchain.prompts import ChatPromptTemplate
from langchain.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(
    ["Teddy is an AI engineer who loves programming!"],
    embedding=OpenAIEmbeddings(),
)

retriever = vectorstore.as_retriever()

template = """Answer the question based only on the following context:
{context}  

Question: {question}"""

prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI(model="gpt-4o-mini")

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

# 체인의 그래프에서 노드들 가져오기
# {'19ae68eaca8a4adebefdc80f4db45065': Node(id='19ae68eaca8a4adebefdc80f4db45065', name='Parallel<context,question>Input', data=<class 'langchain_core.runnables.base.RunnableParallel<context,question>Input'>, metadata=None), '17d5e6cff26f48c8aa851bee4ae04e80': Node(id='17d5e6cff26f48c8aa851bee4ae04e80', name='Parallel<context,question>Output', data=<class 'langchain_core.utils.pydantic.RunnableParallel<context,question>Output'>, metadata=None), 'a507c9864b24469dbea54ba87336bb29': Node(id='a507c9864b24469dbea54ba87336bb29', name='VectorStoreRetriever', data=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10591c690>, search_kwargs={}), metadata=None), '7f6d0868786c47a0aff925d5e5800732': Node(id='7f6d0868786c47a0aff925d5e5800732', name='Passthrough', data=RunnablePassthrough(), metadata=None), 'a0718e5adcfe42e1966f1afcb8f8eafd': Node(id='a0718e5adcfe42e1966f1afcb8f8eafd', name='ChatPromptTemplate', data=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\n{context}  \n\nQuestion: {question}'), additional_kwargs={})]), metadata=None), 'd4eae698951148ddab4d61fc83ffe4d4': Node(id='d4eae698951148ddab4d61fc83ffe4d4', name='ChatOpenAI', data=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x1063b1a50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x107c58f10>, root_client=<openai.OpenAI object at 0x10659f450>, root_async_client=<openai.AsyncOpenAI object at 0x107e2cb10>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), metadata=None), '79eb63c40ef74f40a9692f0de97c19dd': Node(id='79eb63c40ef74f40a9692f0de97c19dd', name='StrOutputParser', data=StrOutputParser(), metadata=None), 'fa6d2e281cab4f29ace3eb973f1b69da': Node(id='fa6d2e281cab4f29ace3eb973f1b69da', name='StrOutputParserOutput', data=<class 'langchain_core.output_parsers.string.StrOutputParserOutput'>, metadata=None)}
print(chain.get_graph().nodes)

# [
#     Edge(
#         source="0a2f3340f5be44719ec71b1d47e9deb0",
#         target="5d8711afa3274e91a203a7264ba5fd56",
#         data=None,
#         conditional=False,
#     ),
#     ...
# ]
print(chain.get_graph().edges)

# 그래프 형태로 표시하기
"""
           +---------------------------------+         
           | Parallel<context,question>Input |         
           +---------------------------------+         
                    **               **                
                 ***                   ***             
               **                         **           
+----------------------+              +-------------+  
| VectorStoreRetriever |              | Passthrough |  
+----------------------+              +-------------+  
                    **               **                
                      ***         ***                  
                         **     **                     
           +----------------------------------+        
           | Parallel<context,question>Output |        
           +----------------------------------+        
                             *                         
                             *                         
                             *                         
                  +--------------------+               
                  | ChatPromptTemplate |               
                  +--------------------+               
                             *                         
                             *                         
                             *                         
                      +------------+                   
                      | ChatOpenAI |                   
                      +------------+                   
                             *                         
                             *                         
                             *                         
                   +-----------------+                 
                   | StrOutputParser |                 
                   +-----------------+                 
                             *                         
                             *                         
                             *                         
                +-----------------------+              
                | StrOutputParserOutput |              
                +-----------------------+              
"""
print(chain.get_graph().print_ascii())

# 체인에서 사용하는 프롬프트 객체의 리스트
# [
#     ChatPromptTemplate(
#         input_variables=["context", "question"],
#         input_types={},
#         partial_variables={},
#         messages=[
#             HumanMessagePromptTemplate(
#                 prompt=PromptTemplate(
#                     input_variables=["context", "question"],
#                     input_types={},
#                     partial_variables={},
#                     template="Answer the question based only on the following context:\n{context}  \n\nQuestion: {question}",
#                 ),
#                 additional_kwargs={},
#             )
#         ],
#     )
# ]
print(chain.get_prompts())
```
