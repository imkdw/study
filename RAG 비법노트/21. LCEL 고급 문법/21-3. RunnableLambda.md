# RunnableLambda
- 유저가 자신만의 함수를 정의해서 해당 함수를 실행할 수 있는 기능
- 사용자 정의 함수가 받을 수 있는 인자는 한개이며 여러개를 받아야한다면 단일 입력을 받고 이를 여러 인수로 풀어내는 래퍼 작성이 필요함

<br>

# 예제
```python
from dotenv import load_dotenv
from langchain_teddynote import logging

load_dotenv()

logging.langsmith("test")

# ==============================================

from operator import itemgetter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI


def length_function(text):
    return len(text)


def _multiple_length_function(text1, text2):
    return len(text1) * len(text2)


def multiple_length_function(_dict):
    return _multiple_length_function(_dict["text1"], _dict["text2"])


prompt = ChatPromptTemplate.from_template("what is {a} + {b}?")
model = ChatOpenAI()

chain1 = prompt | model

chain = (
    {
        # input_1 = bar -> length_function(bar) = 3 -> {"a": 3}이 전달됨
        "a": itemgetter("input_1") | RunnableLambda(length_function),
        # input_1 = bar, input_2 = gah -> {"text1": bar, "text2": gah} -> multiple_length_function({"text1": bar, "text2": gah}) = 3 * 3 = 9 -> {"b": 9}이 전달됨
        "b": {"text1": itemgetter("input_1"), "text2": itemgetter("input_2")}
        | RunnableLambda(multiple_length_function),
    }
    | prompt
    | model
    | StrOutputParser()
)

# 3 + 9 = 12
print(chain.invoke({"input_1": "bar", "input_2": "gah"}))
```

<br>

# RunnableConfig 전달하기
- RunnableLambda는 Runnable을 실행할 때 주는 부가적인 설정과 적보인 RunnableConfig를 선택적으로 전달이 가능함
- RunnableConfig는 실행 중 사용할 태그, 로그 활성화 여부 등 정의가 가능함
```python
import json
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig
from langchain.callbacks import get_openai_callback


# JSON 파싱에 실패하면 LLM을 통해서 파싱을 처리하는 함수
def parse_or_fix(text: str, config: RunnableConfig):
    fixing_chain = (
        ChatPromptTemplate.from_template(
            "Fix the following text:\n\ntext\n{input}\n\nError: {error}"
            " Don't narrate, just respond with the fixed data."
        )
        | ChatOpenAI()
        | StrOutputParser()
    )
    for _ in range(3):
        try:
            return json.loads(text)
        except Exception as e:
            text = fixing_chain.invoke({"input": text, "error": e}, config)
            print(f"config: {config}")
    return "Failed to parse"


with get_openai_callback() as cb:
    output = RunnableLambda(parse_or_fix).invoke(
        input="{foo:: bar}",
        config={"tags": ["my-tag"], "callbacks": [cb]},
    )

    # 수정한결과:
    # {'foo': 'bar'}
    print(f"\n\n수정한결과:\n{output}")
```