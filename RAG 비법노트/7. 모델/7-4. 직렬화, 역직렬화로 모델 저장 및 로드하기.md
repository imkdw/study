# 직렬화, 역직렬화로 모델 저장 및 로드하기
- 직렬화 : 데이터 구조나 객체의 상태를 저장하거나 전송하기 위해 일력의 바이트나 문자열로 변환하는 과정
- 역직렬화 : 직렬화된 데이터를 원래 객체나 데이터 구조의 형태로 복원하는 과정

<br>

# 직렬화 가능 여부 확인하기
```python
from dotenv import load_dotenv
from langchain_teddynote import logging

load_dotenv()
logging.langsmith("langchain_test")

# ========================================================================

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("{fruit}의 맛은 어떤가요?")

# True, ChatOpenAI는 직렬화 가능
print(f"ChatOpenAI: {ChatOpenAI.is_lc_serializable()}")

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# True, ChatOpenAI로 만들어진 llm 인스턴스는 직렬화 가능
print(f"ChatOpenAI: {llm.is_lc_serializable()}")

chain = prompt | llm

# True, 해당 체인도 직렬화 가능
print(chain.is_lc_serializable())
```

<br>

# 체인 직렬화하기
- 체인직렬화는 직렬화 가능한 객체를 딕셔너리나 JSON 문자열로 변환하는 과정
- 딕셔너리 형태로 변환하므로 객체를 쉽게 저장하고 전송하거나 재구성이 가능함
- dumps : 객체를 문자열로 직렬화
- dumpd : 객체를 딕셔너리로 직렬화

<br>

### 체인을 딕셔너리로 변환(dumpd)
```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_teddynote import logging

load_dotenv()
logging.langsmith("langchain_test")

# ========================================================================

from langchain.prompts import PromptTemplate
from langchain_core.load import dumpd

prompt = PromptTemplate.from_template("{fruit}의 맛은 어떤가요?")

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
chain = prompt | llm


dumpd_chain = dumpd(chain)
# {
#     dumpd_chain: {
#         "lc": 1,
#         "type": "constructor",
#         "id": ["langchain", "schema", "runnable", "RunnableSequence"],
#         "kwargs": {
#             "first": {
#                 "lc": 1,
#                 "type": "constructor",
#                 "id": ["langchain", "prompts", "prompt", "PromptTemplate"],
#                 "kwargs": {
#                     "input_variables": ["fruit"],
#                     "template": "{fruit}의 맛은 어떤가요?",
#                     "template_format": "f-string",
#                 },
#                 "name": "PromptTemplate",
#             },
#             "last": {
#                 "lc": 1,
#                 "type": "constructor",
#                 "id": ["langchain", "chat_models", "openai", "ChatOpenAI"],
#                 "kwargs": {
#                     "model_name": "gpt-3.5-turbo",
#                     "temperature": 0.0,
#                     "openai_api_key": {
#                         "lc": 1,
#                         "type": "secret",
#                         "id": ["OPENAI_API_KEY"],
#                     },
#                     "output_version": "v0",
#                 },
#                 "name": "ChatOpenAI",
#             },
#         },
#         "name": "RunnableSequence",
#     }
# }
print(f"dumpd_chain: {dumpd_chain}", end="\n\n")

# type(dumpd_chain): <class 'dict'>
print(f"type(dumpd_chain): {type(dumpd_chain)}", end="\n\n")
```

<br>

### 체인을 문자열로 변환(dumps)
```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_teddynote import logging

load_dotenv()
logging.langsmith("langchain_test")

# ========================================================================

from langchain.prompts import PromptTemplate
from langchain_core.load import dumpd, dumps

prompt = PromptTemplate.from_template("{fruit}의 맛은 어떤가요?")

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
chain = prompt | llm


dumps_chain = dumps(chain)

# {
#     dumps_chain: {
#         "lc": 1,
#         "type": "constructor",
#         "id": ["langchain", "schema", "runnable", "RunnableSequence"],
#         "kwargs": {
#             "first": {
#                 "lc": 1,
#                 "type": "constructor",
#                 "id": ["langchain", "prompts", "prompt", "PromptTemplate"],
#                 "kwargs": {
#                     "input_variables": ["fruit"],
#                     "template": "{fruit}\uc758 \ub9db\uc740 \uc5b4\ub5a4\uac00\uc694?",
#                     "template_format": "f-string",
#                 },
#                 "name": "PromptTemplate",
#             },
#             "last": {
#                 "lc": 1,
#                 "type": "constructor",
#                 "id": ["langchain", "chat_models", "openai", "ChatOpenAI"],
#                 "kwargs": {
#                     "model_name": "gpt-3.5-turbo",
#                     "temperature": 0.0,
#                     "openai_api_key": {
#                         "lc": 1,
#                         "type": "secret",
#                         "id": ["OPENAI_API_KEY"],
#                     },
#                     "output_version": "v0",
#                 },
#                 "name": "ChatOpenAI",
#             },
#         },
#         "name": "RunnableSequence",
#     }
# }
print(f"dumps_chain: {dumps_chain}", end="\n\n")

# type(dumps_chain): <class 'str'>
print(f"type(dumps_chain): {type(dumps_chain)}", end="\n\n")
```

<br>

# Pickle 파일로 직렬화하고 로드하기
- 직렬화된 객체를 저장할 때는 파이썬 문법으로 딕셔너리나 문자열 타입으로 처리해서 저장도 가능하지만 `Pickle` 이라는 파일 형태로 저장하는걸 권장
- 이는 파이썬 객체를 이진형태로 변환하는 포멧으로, 파일이나 메모리에 빠르게 저장이 가능하고 다시 로드해서 복원도 가능함

<br>

### 직렬화된 파일 저장
```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_teddynote import logging

load_dotenv()
logging.langsmith("langchain_test")

# ========================================================================

from langchain.prompts import PromptTemplate
from langchain_core.load import dumpd
import pickle
import json

prompt = PromptTemplate.from_template("{fruit}의 맛은 어떤가요?")

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
chain = prompt | llm


dumpd_chain = dumpd(chain)

# fuit_chain.pkl 파일로 직렬화된 체인을 저장
with open("fruit_chain.pkl", "wb") as f:
    pickle.dump(dumpd_chain, f)

# fuit_chain.json 파일로 직렬화된 체인을 저장
with open("fruit_chain.json", "w") as fp:
    json.dump(dumpd_chain, fp)
```

<br>

### 저장한 모델 불러오기
```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_teddynote import logging

load_dotenv()
logging.langsmith("langchain_test")

# ========================================================================

from langchain.prompts import PromptTemplate
import pickle
from langchain_core.load import load

prompt = PromptTemplate.from_template("{fruit}의 맛은 어떤가요?")

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

with open("fruit_chain.pkl", "rb") as f:
    loaded_chain = pickle.load(f)

    chain_from_file = load(loaded_chain)

    response = chain_from_file.invoke({"fruit": "사과"})

    # 사과는 달콤하고 상큼한 맛이 있습니다. 종류에 따라 살짝 신맛이 느껴질 수도 있고, 과일 특유의 향이 좋습니다. 사과는 많은 사람들이 좋아하는 과일 중 하나로, 다양한 요리나 음료로 활용되기도 합니다. 전반적으로 맛있고 건강에 좋은 과일이라고 할 수 있습니다.
    print(response.content)
```